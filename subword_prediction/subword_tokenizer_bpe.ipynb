{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T19:45:45.568729631Z",
     "start_time": "2026-02-10T19:45:45.482979756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "\n",
    "tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n"
   ],
   "id": "55e3488fe548a93c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T19:45:45.637166450Z",
     "start_time": "2026-02-10T19:45:45.573247025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test\\npre-tokenization!\")"
   ],
   "id": "bcfbc5f1f09fdc82",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Let', (0, 3)),\n",
       " (\"'s\", (3, 5)),\n",
       " ('Ġtest', (5, 10)),\n",
       " ('Ċ', (10, 11)),\n",
       " ('pre', (11, 14)),\n",
       " ('-', (14, 15)),\n",
       " ('tokenization', (15, 27)),\n",
       " ('!', (27, 28))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T19:45:45.780002815Z",
     "start_time": "2026-02-10T19:45:45.704555301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "special_tokens=[\"<|endoftext|>\"]\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=8000,\n",
    "    special_tokens=special_tokens,\n",
    "    show_progress=False\n",
    ")"
   ],
   "id": "324fe4e2bb855663",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T19:45:46.827734076Z",
     "start_time": "2026-02-10T19:45:45.795937537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "files = [\"input.txt\"]\n",
    "tokenizer.train(files, trainer)\n",
    "file_name= \"../bpe_tokenizer.json\"\n"
   ],
   "id": "3ae5328152083e2a",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T19:45:46.943654427Z",
     "start_time": "2026-02-10T19:45:46.888603871Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tokenizers import decoders, processors\n",
    "\n",
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
    "tokenizer.decoder = decoders.ByteLevel()"
   ],
   "id": "75f2d1c5e7f3c0fb",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T19:45:47.035401042Z",
     "start_time": "2026-02-10T19:45:46.950085521Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.save(file_name)",
   "id": "bd39b6da528eb576",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T19:45:47.171908573Z",
     "start_time": "2026-02-10T19:45:47.038134275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load your trained tokenizer\n",
    "tokenizer = Tokenizer.from_file(file_name)\n",
    "\n",
    "text = \"To be, or not to be:\\n\\nThat is \\nthe question.\"\n",
    "\n",
    "encoding = tokenizer.encode(text)\n",
    "print(encoding.tokens)\n",
    "decoding=tokenizer.decode(encoding.ids)\n",
    "print(decoding)"
   ],
   "id": "1f9598fc08c814dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To', 'Ġbe', ',', 'Ġor', 'Ġnot', 'Ġto', 'Ġbe', ':', 'Ċ', 'Ċ', 'That', 'Ġis', 'Ġ', 'Ċ', 'the', 'Ġquestion', '.']\n",
      "To be, or not to be:\n",
      "\n",
      "That is \n",
      "the question.\n"
     ]
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
