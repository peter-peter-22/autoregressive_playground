{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T19:51:04.698554851Z",
     "start_time": "2026-02-25T19:51:04.631174014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "file_name = \"bpe_tokenizer.json\"\n",
    "tokenizer = Tokenizer.from_file(file_name)\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(vocab_size)"
   ],
   "id": "b835c4d4e7fac36e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T19:51:04.754621372Z",
     "start_time": "2026-02-25T19:51:04.702906926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"To be, or not to be:\\n\\nThat is \\nthe question.\"\n",
    "\n",
    "encoding = tokenizer.encode(text)\n",
    "print(encoding.tokens)\n",
    "decoding = tokenizer.decode(encoding.ids)\n",
    "print(decoding)"
   ],
   "id": "e7b2dfd245ed3884",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To', 'Ġbe', ',', 'Ġor', 'Ġnot', 'Ġto', 'Ġbe', ':', 'Ċ', 'Ċ', 'That', 'Ġis', 'Ġ', 'Ċ', 'the', 'Ġquestion', '.']\n",
      "To be, or not to be:\n",
      "\n",
      "That is \n",
      "the question.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T19:51:17.122874704Z",
     "start_time": "2026-02-25T19:51:16.425992743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load a text file (any book / text)\n",
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "encoding = tokenizer.encode(text)\n",
    "print(len(encoding.ids))\n",
    "data = torch.tensor(encoding.ids, dtype=torch.long)"
   ],
   "id": "2955db7d7594f9fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317285\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T19:51:17.158209264Z",
     "start_time": "2026-02-25T19:51:17.125947260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cpu_only = False\n",
    "device = \"cpu\" if cpu_only or not torch.cuda.is_available() else \"cuda\"\n",
    "print(\"device:\", device)"
   ],
   "id": "8bbd08d0ca555ebb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T19:51:17.170984326Z",
     "start_time": "2026-02-25T19:51:17.160910921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ],
   "id": "e6105679376345b1",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T19:51:17.181255562Z",
     "start_time": "2026-02-25T19:51:17.173118056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "block_size = 128  # context length\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data_src = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data_src) - block_size - 1, (batch_size,))\n",
    "\n",
    "    # Input tokens\n",
    "    x = torch.stack([data_src[i:i + block_size] for i in ix])\n",
    "    # Target = next character\n",
    "    y = torch.stack([data_src[i + 1:i + block_size + 1] for i in ix])\n",
    "\n",
    "    return x.to(device), y.to(device)"
   ],
   "id": "b4a0493aae7a9a61",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T19:51:17.210103581Z",
     "start_time": "2026-02-25T19:51:17.182215650Z"
    }
   },
   "cell_type": "code",
   "source": "get_batch(\"train\")",
   "id": "4b891bb62d6437ff",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   5,  106,  717,  ...,   64,  359,  309],\n",
       "         [  64,  136,  305,  ...,    4,   29,  125],\n",
       "         [4418,   64,  208,  ...,   64,  510, 7733],\n",
       "         ...,\n",
       "         [ 315,  101,  622,  ...,  144, 1188,  170],\n",
       "         [ 106,  328,  176,  ...,   64,   20,  265],\n",
       "         [  64,  302, 5910,  ..., 4749,  157, 7233]]),\n",
       " tensor([[ 106,  717,   10,  ...,  359,  309,  102],\n",
       "         [ 136,  305,  117,  ...,   29,  125,  406],\n",
       "         [  64,  208,  114,  ...,  510, 7733,  384],\n",
       "         ...,\n",
       "         [ 101,  622,  157,  ..., 1188,  170,  870],\n",
       "         [ 328,  176,  463,  ...,   20,  265,  499],\n",
       "         [ 302, 5910,  868,  ...,  157, 7233,    9]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T19:51:17.221372123Z",
     "start_time": "2026-02-25T19:51:17.211219441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SubwordEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, block_size):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(block_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        tok = self.token_emb(x)  # (B, T, d_model)\n",
    "        pos = self.pos_emb(torch.arange(T, device=device))  # (T, d_model)\n",
    "        return tok + pos"
   ],
   "id": "64c34a5e104c4cb4",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T19:51:17.235651046Z",
     "start_time": "2026-02-25T19:51:17.222820656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, block_size):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "\n",
    "        self.key = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.query = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.value = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.tril(torch.ones(block_size, block_size)).bool()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Project once\n",
    "        K = self.key(x)  # (B, T, C)\n",
    "        Q = self.query(x)\n",
    "        V = self.value(x)\n",
    "\n",
    "        # Split into heads\n",
    "        K = K.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        Q = Q.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        V = V.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        # Shapes: (B, n_heads, T, d_head)\n",
    "\n",
    "        # Attention scores\n",
    "        att = (Q @ K.transpose(-2, -1)) / (self.d_head ** 0.5)\n",
    "        # (B, n_heads, T, T)\n",
    "\n",
    "        att = att.masked_fill(~self.mask[:T, :T], float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "\n",
    "        # Weighted sum\n",
    "        out = att @ V  # (B, n_heads, T, d_head)\n",
    "\n",
    "        # Recombine heads\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        return self.proj(out)"
   ],
   "id": "5de563287749cb8c",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T19:51:17.247415356Z",
     "start_time": "2026-02-25T19:51:17.237051773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, block_size, head_n):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiHeadSelfAttention(d_model, head_n, block_size)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * d_model, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Attention with residual\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        # Feed-forward with residual\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x"
   ],
   "id": "870b845de359f563",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T19:51:17.259682632Z",
     "start_time": "2026-02-25T19:51:17.248997261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SubwordLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, block_size, head_n):\n",
    "        super().__init__()\n",
    "        self.embed = SubwordEmbedding(vocab_size, d_model, block_size)\n",
    "        self.block = TransformerBlock(d_model, block_size, head_n)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        x = self.embed(x)  # (B, T, d_model)\n",
    "        x = self.block(x)\n",
    "        x = self.ln(x)\n",
    "        logits = self.head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            return logits\n",
    "\n",
    "        B, T, V = logits.shape\n",
    "        probs = logits.view(B * T, V)  # probabilities: B * T, V\n",
    "        ids = targets.view(B * T)  # ids: B * T\n",
    "        loss = F.cross_entropy(\n",
    "            probs,\n",
    "            ids\n",
    "        )\n",
    "        return logits, loss"
   ],
   "id": "9fb7b6d0b5b1845d",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T19:51:17.282969124Z",
     "start_time": "2026-02-25T19:51:17.261038255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = SubwordLM(vocab_size, d_model=128, block_size=block_size, head_n=4).to(device)\n",
    "model_path = \"../subword.pth\""
   ],
   "id": "72eac65a967c988c",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T19:51:17.293957613Z",
     "start_time": "2026-02-25T19:51:17.284399050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train():\n",
    "    best_loss = float(\"inf\")\n",
    "    patience = 3  # number of evaluations to wait\n",
    "    min_delta = 0.05  # minimum improvement\n",
    "    patience_counter = 0\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "    for step in range(50000):\n",
    "        xb, yb = get_batch(\"train\")\n",
    "\n",
    "        logits, loss = model(xb, yb)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 500 == 0:\n",
    "            loss_num = loss.item()\n",
    "            print(f\"step {step}, loss {loss_num:.4f}\")\n",
    "            if best_loss - loss_num > min_delta:\n",
    "                best_loss = loss_num\n",
    "                patience_counter = 0\n",
    "                torch.save(model.state_dict(), \"best.pth    \")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    torch.save(model.state_dict(), model_path)"
   ],
   "id": "35cbf0741167b9f1",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T19:51:51.167425622Z",
     "start_time": "2026-02-25T19:51:17.295032894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if os.path.exists(model_path):\n",
    "    model.load_state_dict(\n",
    "        torch.load(\n",
    "            model_path,\n",
    "            weights_only=True,\n",
    "            map_location=torch.device('cpu') if device == \"cpu\" else None\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    train()"
   ],
   "id": "5ebbd77497d160e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss 9.1873\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 10\u001B[0m\n\u001B[1;32m      2\u001B[0m     model\u001B[38;5;241m.\u001B[39mload_state_dict(\n\u001B[1;32m      3\u001B[0m         torch\u001B[38;5;241m.\u001B[39mload(\n\u001B[1;32m      4\u001B[0m             model_path,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      7\u001B[0m         )\n\u001B[1;32m      8\u001B[0m     )\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 10\u001B[0m     \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[15], line 14\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m()\u001B[0m\n\u001B[1;32m     11\u001B[0m logits, loss \u001B[38;5;241m=\u001B[39m model(xb, yb)\n\u001B[1;32m     13\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 14\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m step \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m500\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/PycharmProjects/PythonProject1/.venv/lib/python3.10/site-packages/torch/_tensor.py:630\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    620\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    621\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    622\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    623\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    628\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    629\u001B[0m     )\n\u001B[0;32m--> 630\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    631\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    632\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/PythonProject1/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:364\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    359\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    361\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    362\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    363\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 364\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    365\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    366\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    367\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    368\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    369\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_tuple\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    370\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    371\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    372\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/PythonProject1/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:865\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    863\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    864\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 865\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    866\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    867\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    868\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    869\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, start, max_new_tokens=200):\n",
    "    model.eval()\n",
    "    idx = torch.tensor([tokenizer.encode(start).ids], device=device, dtype=torch.long)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -block_size:]\n",
    "        logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "\n",
    "    return tokenizer.decode(idx[0].tolist())\n"
   ],
   "id": "728e73fcccf50c20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(generate(model, \"CORIOLANUS:\"))\n",
    "\"\"\"CORIOLANUS:\n",
    "I come, that I do lament the way to land\n",
    "Montague against my power, or move indeed\n",
    "mans to Henry's off, his grace.\n",
    "Most mighty suit, then are humble weeds shall behold\n",
    "Your very root of government;\n",
    "True, depends to walk; that I do refuse,\n",
    "Thou art not a coward-- deeds as you are one of them;\n",
    "So as for how can you distingusers\n",
    "by Place-deic bark: therefore thou Romeo, for his sake,\n",
    "Some haunt I do I thank thee, I have kill'd.\n",
    "\n",
    "JULIET:\n",
    "Go ran a tears, my mother playsion, sir.\n",
    "\n",
    "ROMEO:\n",
    "'Tis now, no dancing?\n",
    "\n",
    "FRIAR LAURENCE:\n",
    "One king! why, my wife is no end from your bed!\n",
    "\n",
    "FRIAR LAURENCE:\n",
    "The heads of unsoting both my lady's lord that;\n",
    "For I have more favour else is a par\"\"\""
   ],
   "id": "3cb0896b6527575a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
