{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T19:58:48.323532463Z",
     "start_time": "2026-02-10T19:58:48.173118038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "file_name = \"../bpe_tokenizer.json\"\n",
    "tokenizer = Tokenizer.from_file(file_name)\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(vocab_size)"
   ],
   "id": "b835c4d4e7fac36e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T19:58:48.521091588Z",
     "start_time": "2026-02-10T19:58:48.360536879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"To be, or not to be:\\n\\nThat is \\nthe question.\"\n",
    "\n",
    "encoding = tokenizer.encode(text)\n",
    "print(encoding.tokens)\n",
    "decoding = tokenizer.decode(encoding.ids)\n",
    "print(decoding)"
   ],
   "id": "e7b2dfd245ed3884",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To', 'Ġbe', ',', 'Ġor', 'Ġnot', 'Ġto', 'Ġbe', ':', 'Ċ', 'Ċ', 'That', 'Ġis', 'Ġ', 'Ċ', 'the', 'Ġquestion', '.']\n",
      "To be, or not to be:\n",
      "\n",
      "That is \n",
      "the question.\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T19:58:49.637194554Z",
     "start_time": "2026-02-10T19:58:48.538551820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load a text file (any book / text)\n",
    "with open(\"../input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "encoding = tokenizer.encode(text)\n",
    "print(len(encoding.ids))\n",
    "data = torch.tensor(encoding.ids, dtype=torch.long)"
   ],
   "id": "2955db7d7594f9fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317285\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T19:58:49.773092584Z",
     "start_time": "2026-02-10T19:58:49.688461846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cpu_only = False\n",
    "device = \"cpu\" if cpu_only or not torch.cuda.is_available() else \"cuda\"\n",
    "print(\"device:\", device)"
   ],
   "id": "8bbd08d0ca555ebb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T19:58:49.796480168Z",
     "start_time": "2026-02-10T19:58:49.779548212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ],
   "id": "e6105679376345b1",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T19:58:49.864195495Z",
     "start_time": "2026-02-10T19:58:49.799762172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "block_size = 128  # context length\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data_src = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data_src) - block_size - 1, (batch_size,))\n",
    "\n",
    "    # Input tokens\n",
    "    x = torch.stack([data_src[i:i + block_size] for i in ix])\n",
    "    # Target = next character\n",
    "    y = torch.stack([data_src[i + 1:i + block_size + 1] for i in ix])\n",
    "\n",
    "    return x.to(device), y.to(device)"
   ],
   "id": "b4a0493aae7a9a61",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T19:58:49.964836400Z",
     "start_time": "2026-02-10T19:58:49.884430940Z"
    }
   },
   "cell_type": "code",
   "source": "get_batch(\"train\")",
   "id": "4b891bb62d6437ff",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 650,    5,  116,  ...,  682,    7,   64],\n",
       "         [  64,  800,    9,  ..., 5038,  110, 2529],\n",
       "         [   9,   64,   20,  ...,  189, 1913,  101],\n",
       "         ...,\n",
       "         [4131,    5,   64,  ...,  432,  309,  152],\n",
       "         [ 147,  284,  410,  ...,  275, 1001,  989],\n",
       "         [   7,   64,   64,  ...,   64,   27, 4627]]),\n",
       " tensor([[   5,  116, 1182,  ...,    7,   64,   64],\n",
       "         [ 800,    9,   64,  ...,  110, 2529,   82],\n",
       "         [  64,   20,    5,  ..., 1913,  101,  360],\n",
       "         ...,\n",
       "         [   5,   64,  208,  ...,  309,  152, 2235],\n",
       "         [ 284,  410,    5,  ..., 1001,  989,    9],\n",
       "         [  64,   64,  668,  ...,   27, 4627,  328]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T19:58:50.002347367Z",
     "start_time": "2026-02-10T19:58:49.975015264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CharEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, block_size):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(block_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        tok = self.token_emb(x)  # (B, T, d_model)\n",
    "        pos = self.pos_emb(torch.arange(T, device=device))  # (T, d_model)\n",
    "        return tok + pos\n"
   ],
   "id": "64c34a5e104c4cb4",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T19:58:50.071009852Z",
     "start_time": "2026-02-10T19:58:50.004929932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, block_size):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "\n",
    "        self.key = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.query = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.value = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.tril(torch.ones(block_size, block_size)).bool()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Project once\n",
    "        K = self.key(x)  # (B, T, C)\n",
    "        Q = self.query(x)\n",
    "        V = self.value(x)\n",
    "\n",
    "        # Split into heads\n",
    "        K = K.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        Q = Q.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        V = V.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        # Shapes: (B, n_heads, T, d_head)\n",
    "\n",
    "        # Attention scores\n",
    "        att = (Q @ K.transpose(-2, -1)) / (self.d_head ** 0.5)\n",
    "        # (B, n_heads, T, T)\n",
    "\n",
    "        att = att.masked_fill(~self.mask[:T, :T], float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "\n",
    "        # Weighted sum\n",
    "        out = att @ V  # (B, n_heads, T, d_head)\n",
    "\n",
    "        # Recombine heads\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        return self.proj(out)"
   ],
   "id": "5de563287749cb8c",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T19:58:50.128769318Z",
     "start_time": "2026-02-10T19:58:50.073228365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, block_size, head_n):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiHeadSelfAttention(d_model, head_n, block_size)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * d_model, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Attention with residual\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        # Feed-forward with residual\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x"
   ],
   "id": "870b845de359f563",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:25:53.500304323Z",
     "start_time": "2026-02-13T21:25:53.334072391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SubwordLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, block_size, head_n):\n",
    "        super().__init__()\n",
    "        self.embed = CharEmbedding(vocab_size, d_model, block_size)\n",
    "        self.block = TransformerBlock(d_model, block_size, head_n)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        x = self.embed(x)  # (B, T, d_model)\n",
    "        x = self.block(x)\n",
    "        x = self.ln(x)\n",
    "        logits = self.head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            return logits\n",
    "\n",
    "        B, T, V = logits.shape\n",
    "        probs = logits.view(B * T, V)  # probabilities: B * T, V\n",
    "        ids = targets.view(B * T)  # ids: B * T\n",
    "        loss = F.cross_entropy(\n",
    "            probs,\n",
    "            ids\n",
    "        )\n",
    "        return logits, loss"
   ],
   "id": "9fb7b6d0b5b1845d",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mSubwordLM\u001B[39;00m(\u001B[43mnn\u001B[49m\u001B[38;5;241m.\u001B[39mModule):\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, vocab_size, d_model, block_size,head_n):\n\u001B[1;32m      3\u001B[0m         \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'nn' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T19:58:50.313097451Z",
     "start_time": "2026-02-10T19:58:50.252679805Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 28,
   "source": [
    "model = SubwordLM(vocab_size, d_model=128, block_size=block_size, head_n=4).to(device)\n",
    "model_path = \"../subword.pth\""
   ],
   "id": "72eac65a967c988c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T19:58:50.482415152Z",
     "start_time": "2026-02-10T19:58:50.336616616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train():\n",
    "    best_loss = float(\"inf\")\n",
    "    patience = 3  # number of evaluations to wait\n",
    "    min_delta = 0.05  # minimum improvement\n",
    "    patience_counter = 0\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "    for step in range(50000):\n",
    "        xb, yb = get_batch(\"train\")\n",
    "\n",
    "        logits, loss = model(xb, yb)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 500 == 0:\n",
    "            loss_num = loss.item()\n",
    "            print(f\"step {step}, loss {loss_num:.4f}\")\n",
    "            if best_loss - loss_num > min_delta:\n",
    "                best_loss = loss_num\n",
    "                patience_counter = 0\n",
    "                torch.save(model.state_dict(), \"best.pth    \")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    torch.save(model.state_dict(), model_path)"
   ],
   "id": "35cbf0741167b9f1",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T20:00:32.618297308Z",
     "start_time": "2026-02-10T20:00:32.570286033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if os.path.exists(model_path):\n",
    "    model.load_state_dict(\n",
    "        torch.load(\n",
    "            model_path,\n",
    "            weights_only=True,\n",
    "            map_location=torch.device('cpu') if device == \"cpu\" else None\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    train()"
   ],
   "id": "5ebbd77497d160e6",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T20:00:32.695066291Z",
     "start_time": "2026-02-10T20:00:32.651251015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, start, max_new_tokens=200):\n",
    "    model.eval()\n",
    "    idx = torch.tensor([tokenizer.encode(start).ids], device=device, dtype=torch.long)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -block_size:]\n",
    "        logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "\n",
    "    return tokenizer.decode(idx[0].tolist())\n"
   ],
   "id": "728e73fcccf50c20",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T20:00:33.771211666Z",
     "start_time": "2026-02-10T20:00:32.708050937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(generate(model, \"CORIOLANUS:\"))\n",
    "\"\"\"CORIOLANUS:\n",
    "No more than the aims, you shall kneel along\n",
    "Than to live by my words. I'll to your lordship thence.\n",
    "\n",
    "HASTINGS:\n",
    "Do you know't that gives me a lord, I beseech you far\n",
    "Than you for your grace's life.\n",
    "\n",
    "Bid you think fit the lady ill word or wonderful froward.\n",
    "\n",
    "HASTINGS:\n",
    "Woe the path, if you were well compound it,\n",
    "Because your brother, if mocking us is mine and kind\n",
    "And like a cunning livery of life.\n",
    "For my prosperity!\n",
    "O coward conscienceful life is mine. What'clock?\n",
    "I spoke my life: I'll not to him and to be:\n",
    "Let me effuse him beard, sir, my wife is the business.\n",
    "He whom my rage lies with him to the brother's life,\n",
    "That some the hand in the use: 'Fe's colder way of heart--\n",
    "Of that's here. I'll tell him what,\"\"\""
   ],
   "id": "3cb0896b6527575a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORIOLANUS:\n",
      "I come, that I do lament the way to land\n",
      "Montague against my power, or move indeed\n",
      "mans to Henry's off, his grace.\n",
      "Most mighty suit, then are humble weeds shall behold\n",
      "Your very root of government;\n",
      "True, depends to walk; that I do refuse,\n",
      "Thou art not a coward-- deeds as you are one of them;\n",
      "So as for how can you distingusers\n",
      "by Place-deic bark: therefore thou Romeo, for his sake,\n",
      "Some haunt I do I thank thee, I have kill'd.\n",
      "\n",
      "JULIET:\n",
      "Go ran a tears, my mother playsion, sir.\n",
      "\n",
      "ROMEO:\n",
      "'Tis now, no dancing?\n",
      "\n",
      "FRIAR LAURENCE:\n",
      "One king! why, my wife is no end from your bed!\n",
      "\n",
      "FRIAR LAURENCE:\n",
      "The heads of unsoting both my lady's lord that;\n",
      "For I have more favour else is a par\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"CORIOLANUS:\\nNo more than the aims, you shall kneel along\\nThan to live by my words. I'll to your lordship thence.\\n\\nHASTINGS:\\nDo you know't that gives me a lord, I beseech you far\\nThan you for your grace's life.\\n\\nBid you think fit the lady ill word or wonderful froward.\\n\\nHASTINGS:\\nWoe the path, if you were well compound it,\\nBecause your brother, if mocking us is mine and kind\\nAnd like a cunning livery of life.\\nFor my prosperity!\\nO coward conscienceful life is mine. What'clock?\\nI spoke my life: I'll not to him and to be:\\nLet me effuse him beard, sir, my wife is the business.\\nHe whom my rage lies with him to the brother's life,\\nThat some the hand in the use: 'Fe's colder way of heart--\\nOf that's here. I'll tell him what,\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
