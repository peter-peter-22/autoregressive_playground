{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T12:14:46.691827336Z",
     "start_time": "2026-02-08T12:14:45.500459480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "# Load a text file (any book / text)\n",
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Character vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Character â†” integer mapping\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "\n",
    "# Encode entire dataset as integers\n",
    "data = torch.tensor([stoi[c] for c in text], dtype=torch.long)"
   ],
   "id": "2955db7d7594f9fa",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T12:14:46.768240529Z",
     "start_time": "2026-02-08T12:14:46.695686765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cpu_only=False\n",
    "device = \"cpu\" if cpu_only else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "torch.backends.cudnn.allow_tf32 = False\n",
    "torch.set_default_dtype(torch.float32)\n",
    "print(\"device:\",device)"
   ],
   "id": "8bbd08d0ca555ebb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T12:14:46.868061691Z",
     "start_time": "2026-02-08T12:14:46.769577599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example usage of view and transpose\n",
    "test=torch.arange(6,device=device)\n",
    "print(test)\n",
    "test = test.view(2,3)\n",
    "print(test)\n",
    "test = test.transpose(0,1)\n",
    "print(test)"
   ],
   "id": "f50c9b937cd1dedb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5], device='cuda:0')\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]], device='cuda:0')\n",
      "tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]], device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T12:14:46.928731106Z",
     "start_time": "2026-02-08T12:14:46.881832366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ],
   "id": "e6105679376345b1",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T12:14:46.978770262Z",
     "start_time": "2026-02-08T12:14:46.930899308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "block_size = 64   # context length\n",
    "batch_size = 32\n",
    "\n",
    "def get_batch(split):\n",
    "    data_src = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data_src) - block_size -1, (batch_size,))\n",
    "\n",
    "    # Input tokens\n",
    "    x = torch.stack([data_src[i:i+block_size] for i in ix])\n",
    "    # Target = next character\n",
    "    y = torch.stack([data_src[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "    return x.to(device), y.to(device)"
   ],
   "id": "b4a0493aae7a9a61",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T12:14:47.045857975Z",
     "start_time": "2026-02-08T12:14:46.980688868Z"
    }
   },
   "cell_type": "code",
   "source": "get_batch(\"train\")",
   "id": "4b891bb62d6437ff",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1, 21,  1,  ..., 43, 52, 56],\n",
       "         [ 1, 39,  0,  ..., 56, 52, 39],\n",
       "         [46, 47, 51,  ...,  1, 57, 54],\n",
       "         ...,\n",
       "         [ 0, 16, 27,  ..., 57,  1, 56],\n",
       "         [58,  1, 31,  ..., 56, 43,  5],\n",
       "         [59, 41, 46,  ..., 58, 46, 43]], device='cuda:0'),\n",
       " tensor([[21,  1, 40,  ..., 52, 56, 63],\n",
       "         [39,  0, 42,  ..., 52, 39, 56],\n",
       "         [47, 51,  8,  ..., 57, 54, 43],\n",
       "         ...,\n",
       "         [16, 27, 30,  ...,  1, 56, 43],\n",
       "         [ 1, 31, 43,  ..., 43,  5, 57],\n",
       "         [41, 46,  1,  ..., 46, 43,  1]], device='cuda:0'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T12:14:47.098822241Z",
     "start_time": "2026-02-08T12:14:47.051232760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CharEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, block_size):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(block_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        tok = self.token_emb(x)              # (B, T, d_model)\n",
    "        pos = self.pos_emb(torch.arange(T, device=device))  # (T, d_model)\n",
    "        return tok + pos\n"
   ],
   "id": "64c34a5e104c4cb4",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T12:14:47.150830449Z",
     "start_time": "2026-02-08T12:14:47.101310186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, block_size):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "\n",
    "        self.key   = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.query = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.value = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.tril(torch.ones(block_size, block_size)).bool()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Project once\n",
    "        K = self.key(x)    # (B, T, C)\n",
    "        Q = self.query(x)\n",
    "        V = self.value(x)\n",
    "\n",
    "        # Split into heads\n",
    "        K = K.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        Q = Q.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        V = V.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        # Shapes: (B, n_heads, T, d_head)\n",
    "\n",
    "        # Attention scores\n",
    "        att = (Q @ K.transpose(-2, -1)) / (self.d_head ** 0.5)\n",
    "        # (B, n_heads, T, T)\n",
    "\n",
    "        att = att.masked_fill(~self.mask[:T, :T], float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "\n",
    "        # Weighted sum\n",
    "        out = att @ V  # (B, n_heads, T, d_head)\n",
    "\n",
    "        # Recombine heads\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        return self.proj(out)"
   ],
   "id": "5de563287749cb8c",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T12:14:47.208934200Z",
     "start_time": "2026-02-08T12:14:47.152727802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, block_size, head_n):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiHeadSelfAttention(d_model, head_n, block_size)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * d_model, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Attention with residual\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        # Feed-forward with residual\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x"
   ],
   "id": "870b845de359f563",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T12:14:47.262429019Z",
     "start_time": "2026-02-08T12:14:47.211569498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CharLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, block_size,head_n):\n",
    "        super().__init__()\n",
    "        self.embed = CharEmbedding(vocab_size, d_model, block_size)\n",
    "        self.block = TransformerBlock(d_model,block_size, head_n)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        x = self.embed(x)        # (B, T, d_model)\n",
    "        x = self.block(x)\n",
    "        x = self.ln(x)\n",
    "        logits = self.head(x)   # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            return logits\n",
    "\n",
    "        B, T, V = logits.shape\n",
    "        probs=logits.view(B*T, V) # probabilities: B * T, V\n",
    "        ids = targets.view(B*T) # ids: B * T\n",
    "        loss = F.cross_entropy(\n",
    "            probs,\n",
    "            ids\n",
    "        )\n",
    "        return logits, loss"
   ],
   "id": "9fb7b6d0b5b1845d",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T12:15:17.673564097Z",
     "start_time": "2026-02-08T12:14:47.264965436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = CharLM(vocab_size, d_model=128, block_size=block_size, head_n=4).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "for step in range(5000):\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 500 == 0:\n",
    "        print(f\"step {step}, loss {loss.item():.4f}\")\n"
   ],
   "id": "5ebbd77497d160e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss 4.3161\n",
      "step 500, loss 2.3503\n",
      "step 1000, loss 2.1962\n",
      "step 1500, loss 2.1066\n",
      "step 2000, loss 2.1802\n",
      "step 2500, loss 2.3002\n",
      "step 3000, loss nan\n",
      "step 3500, loss nan\n",
      "step 4000, loss nan\n",
      "step 4500, loss nan\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling cublasLtMatmul with transpose_mat1 1 transpose_mat2 0 m 65 n 2048 k 128 mat1_ld 128 mat2_ld 128 result_ld 65 abcType 0 computeType 68 scaleType 0",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m step \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[32m5000\u001B[39m):\n\u001B[32m      5\u001B[39m     xb, yb = get_batch(\u001B[33m\"\u001B[39m\u001B[33mtrain\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m     logits, loss = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mxb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43myb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      9\u001B[39m     optimizer.zero_grad()\n\u001B[32m     10\u001B[39m     loss.backward()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject1/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject1/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 13\u001B[39m, in \u001B[36mCharLM.forward\u001B[39m\u001B[34m(self, x, targets)\u001B[39m\n\u001B[32m     11\u001B[39m x = \u001B[38;5;28mself\u001B[39m.block(x)\n\u001B[32m     12\u001B[39m x = \u001B[38;5;28mself\u001B[39m.ln(x)\n\u001B[32m---> \u001B[39m\u001B[32m13\u001B[39m logits = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mhead\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m   \u001B[38;5;66;03m# (B, T, vocab_size)\u001B[39;00m\n\u001B[32m     15\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m targets \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m     16\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m logits\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject1/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject1/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/PythonProject1/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001B[39m, in \u001B[36mLinear.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    124\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m125\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mRuntimeError\u001B[39m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling cublasLtMatmul with transpose_mat1 1 transpose_mat2 0 m 65 n 2048 k 128 mat1_ld 128 mat2_ld 128 result_ld 65 abcType 0 computeType 68 scaleType 0"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, start, max_new_tokens=200):\n",
    "    model.eval()\n",
    "    idx = torch.tensor([[stoi[c] for c in start]], device=device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -block_size:]\n",
    "        logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "\n",
    "    return \"\".join(itos[i.item()] for i in idx[0])\n"
   ],
   "id": "728e73fcccf50c20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(generate(model, \"CORIOLANUS:\"))",
   "id": "3cb0896b6527575a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
