{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Pre process and tokenize all datasets, save the token ids.",
   "id": "4a59e67f04d60450"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-15T07:44:23.011041306Z",
     "start_time": "2026-02-15T07:44:22.998902947Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from datasets.formatting.formatting import LazyBatch\n",
    "from huggingface_hub import login\n",
    "\n",
    "from special_tokens import special_tokens\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "login(hf_token)\n",
    "batch_size = 10_000\n",
    "processes = 4"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T07:41:33.925302546Z",
     "start_time": "2026-02-15T07:41:33.858549538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tokenizers.tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"tokenizer.json\")"
   ],
   "id": "2d1c914c81b69378",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T07:41:36.139479821Z",
     "start_time": "2026-02-15T07:41:36.123100472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_tokens(loaded):\n",
    "    token_ids = next(iter(loaded))[\"tokens\"]\n",
    "    text = tokenizer.decode(token_ids)\n",
    "    print(text)"
   ],
   "id": "bbdc76ace01e6f76",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Dataset: no robots",
   "id": "358bd42416aca9b9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T22:47:56.758701670Z",
     "start_time": "2026-02-14T22:47:52.749431752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ds_test = load_dataset(\"HuggingFaceH4/no_robots\", split=\"test\").select_columns([\"messages\"])\n",
    "ds_train = load_dataset(\"HuggingFaceH4/no_robots\", split=\"train\").select_columns([\"messages\"])"
   ],
   "id": "486d48ee4cee7365",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T22:48:03.433231095Z",
     "start_time": "2026-02-14T22:47:56.766256522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from chat_template import chat_template\n",
    "\n",
    "\n",
    "def tokenize_robots(batch: LazyBatch):\n",
    "    results = [\n",
    "        tokenizer.encode(chat_template(row)).ids\n",
    "        for row in batch[\"messages\"]\n",
    "    ]\n",
    "    return {\"tokens\": results}\n",
    "\n",
    "\n",
    "ds_test.map(\n",
    "    tokenize_robots,\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    num_proc=processes\n",
    ").select_columns(\"tokens\").save_to_disk(\"tokenized_data/robots_test\")\n",
    "\n",
    "ds_train.map(\n",
    "    tokenize_robots,\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    num_proc=processes,\n",
    ").select_columns(\"tokens\").save_to_disk(\"tokenized_data/robots_train\")"
   ],
   "id": "ff09b2b2d9edd170",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "33558fdaebbe487b87965567ebcfb93e"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "128513dfb72a4942abb4a882399c76f1"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/9500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "27ae813a9967485fa7534c530bfb0036"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/9500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a51c71fbc7794dc8ac41617ec16d1538"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T22:49:11.872601831Z",
     "start_time": "2026-02-14T22:49:11.825899810Z"
    }
   },
   "cell_type": "code",
   "source": "test_tokens(load_from_disk(\"tokenized_data/robots_test\"))",
   "id": "1ad498e3b3bf1252",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aster is a chatbot who answers questions with rhymes.\n",
      "where did chocolate originate?\n",
      "chocolate is 4000 years old/mexico is where it was first sold\n",
      "where was milk chocolate invented?\n",
      "switzerland was the first to add milk/to make their chocolate smooth as silk\n",
      "what are some good desserts that use chocolate?\n",
      "pie, tart, cookies, and cake/chocolate is great to bake\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Dataset: wikipedia summary",
   "id": "204976b846ac7bfd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T22:49:17.130090113Z",
     "start_time": "2026-02-14T22:49:11.894897091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "splits = load_dataset(\"jordiclive/wikipedia-summary-dataset\", split=\"train\").train_test_split(\n",
    "    test_size=0.1,\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "ds_test = splits[\"test\"].select_columns([\"summary\"])\n",
    "ds_train = splits[\"train\"].select_columns([\"summary\"])"
   ],
   "id": "1ad7ca6881a27196",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T23:18:25.754321990Z",
     "start_time": "2026-02-14T22:49:17.136999049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_wiki(batch: LazyBatch):\n",
    "    eot = special_tokens[\"end_of_text\"]\n",
    "    results = [\n",
    "        tokenizer.encode(row + \"\\n\" + eot).ids\n",
    "        for row in batch[\"summary\"]\n",
    "    ]\n",
    "    return {\"tokens\": results}\n",
    "\n",
    "\n",
    "ds_test.map(\n",
    "    tokenize_wiki,\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    num_proc=processes\n",
    ").select_columns(\"tokens\").save_to_disk(\"tokenized_data/wiki_test\")\n",
    "\n",
    "ds_train.map(\n",
    "    tokenize_wiki,\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    num_proc=processes\n",
    ").select_columns(\"tokens\").save_to_disk(\"tokenized_data/wiki_train\")"
   ],
   "id": "4a89dd722a5e8202",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/775001 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a2bd583355ba40e8bea6323e8a0aa990"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/775001 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e9bd941dfda34c959c67119fe05f3b04"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/6975006 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4a0d06d7be754ef08b6a79c6e256a149"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/13 shards):   0%|          | 0/6975006 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5e8f91a653a94eb6a02414baa364e675"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T23:18:26.926128017Z",
     "start_time": "2026-02-14T23:18:25.962477460Z"
    }
   },
   "cell_type": "code",
   "source": "test_tokens(load_from_disk(\"tokenized_data/wiki_test\"))",
   "id": "841d9728bc3695eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category:populated places in mcpherson county, nebraska\n",
      "mcpherson\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Dataset: tiny stories",
   "id": "fd11bc77bf1c1bdf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T07:42:10.168899779Z",
     "start_time": "2026-02-15T07:41:53.934086186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ds_test = load_dataset(\"roneneldan/TinyStories\", split=\"validation\").select_columns([\"text\"])\n",
    "ds_train = load_dataset(\"roneneldan/TinyStories\", split=\"train\").select_columns([\"text\"])"
   ],
   "id": "af288c35ebda2064",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T07:44:50.553452520Z",
     "start_time": "2026-02-15T07:44:31.035170353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_stories(batch: LazyBatch):\n",
    "    eot = special_tokens[\"end_of_text\"]\n",
    "    results = [\n",
    "        tokenizer.encode(row + \"\\n\" + eot).ids\n",
    "        for row in batch[\"text\"]\n",
    "    ]\n",
    "    return {\"tokens\": results}\n",
    "\n",
    "\n",
    "ds_test.map(\n",
    "    tokenize_stories,\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    num_proc=processes\n",
    ").select_columns(\"tokens\").save_to_disk(\"tokenized_data/stories_test\")\n",
    "\n",
    "ds_train.map(\n",
    "    tokenize_stories,\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    num_proc=processes\n",
    ").select_columns(\"tokens\").save_to_disk(\"tokenized_data/stories_train\")"
   ],
   "id": "7f5714539b15204d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/21990 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ae56bc6de5ef4edbb9df812b4f97508b"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/8 shards):   0%|          | 0/2119719 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dc000495ca624180aa2f19ccf2271f6d"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T07:44:50.796908077Z",
     "start_time": "2026-02-15T07:44:50.566251866Z"
    }
   },
   "cell_type": "code",
   "source": "test_tokens(load_from_disk(\"tokenized_data/stories_test\"))",
   "id": "37556459f04557e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spot. spot saw the shiny car and said, \"wow, kitty, your car is so bright and clean!\" kitty smiled and replied, \"thank you, spot. i polish it every day.\"\n",
      "\n",
      "after playing with the car, kitty and spot felt thirsty. they found a small pond with clear water. they drank the water and felt very happy. they played together all day and became best friends.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Dataset: tiny textbooks",
   "id": "197e0f793f9a78ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T07:46:18.682449847Z",
     "start_time": "2026-02-15T07:46:14.982831765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ds_test = load_dataset(\"nampdn-ai/tiny-textbooks\", split=\"test\").select_columns([\"text\"])\n",
    "ds_train = load_dataset(\"nampdn-ai/tiny-textbooks\", split=\"train\").select_columns([\"text\"])"
   ],
   "id": "cb8d24aab8e0144a",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T07:48:22.738136245Z",
     "start_time": "2026-02-15T07:47:15.867468312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_textbooks(batch: LazyBatch):\n",
    "    eot = special_tokens[\"end_of_text\"]\n",
    "    results = [\n",
    "        tokenizer.encode(row + \"\\n\" + eot).ids\n",
    "        for row in batch[\"text\"]\n",
    "    ]\n",
    "    return {\"tokens\": results}\n",
    "\n",
    "\n",
    "ds_test.map(\n",
    "    tokenize_textbooks,\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    num_proc=processes\n",
    ").select_columns(\"tokens\").save_to_disk(\"tokenized_data/textbooks_test\")\n",
    "\n",
    "ds_train.map(\n",
    "    tokenize_textbooks,\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    num_proc=processes\n",
    ").select_columns(\"tokens\").save_to_disk(\"tokenized_data/textbooks_train\")"
   ],
   "id": "44ff5c4f946d9bea",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/21000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "684cd1ea95584c3eb2c68747f9db942a"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/399000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ce56a260a75441bc9a6f8e33361503b8"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/399000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "62b1cbadd1a5476b80b27ef1d5dccd4a"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T07:48:22.753580022Z",
     "start_time": "2026-02-15T07:48:22.739389734Z"
    }
   },
   "cell_type": "code",
   "source": "test_tokens(load_from_disk(\"tokenized_data/textbooks_test\"))",
   "id": "5a8d0b156694daa5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "watch this video and more exclusive full episodes of gma shows on. carmen (maria isabel lopez) meets jess (emilio garcia) in a ktv bar. they formed an instant bond which led to love, and so they decided to live together. because of their love, carmen had high hopes that jess will treat kat-kat (mara lopez), her daughter, like his own. jess did everything to win kat-kat's approval which led to the unthinkable - a sin that might change their lives forever.. watch full episodes of ‘karelasyon’ on gmanetwork.com/fullepisodes and youtube.com/gmanetwork. hosted by carla abellana, this episode stars maria isabel lopez, mara lopez, emilio garcia\n",
      "\n"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
