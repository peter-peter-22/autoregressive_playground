{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Pre process and tokenize all datasets, save the token ids.",
   "id": "4a59e67f04d60450"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-15T09:10:04.482986380Z",
     "start_time": "2026-02-15T09:10:04.474640138Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from datasets.formatting.formatting import LazyBatch\n",
    "from huggingface_hub import login\n",
    "\n",
    "from special_tokens import special_tokens\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "login(hf_token)\n",
    "batch_size = 10_000\n",
    "processes = 8"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T09:05:23.139144762Z",
     "start_time": "2026-02-15T09:05:23.089979869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tokenizers.tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"tokenizer.json\")"
   ],
   "id": "2d1c914c81b69378",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T09:05:23.150691105Z",
     "start_time": "2026-02-15T09:05:23.141908370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_tokens(loaded):\n",
    "    token_ids = next(iter(loaded))[\"tokens\"]\n",
    "    text = tokenizer.decode(token_ids)\n",
    "    print(text)"
   ],
   "id": "bbdc76ace01e6f76",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Dataset: no robots",
   "id": "358bd42416aca9b9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T09:05:32.137223311Z",
     "start_time": "2026-02-15T09:05:23.152452237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ds_test = load_dataset(\"HuggingFaceH4/no_robots\", split=\"test\").select_columns([\"messages\"])\n",
    "ds_train = load_dataset(\"HuggingFaceH4/no_robots\", split=\"train\").select_columns([\"messages\"])"
   ],
   "id": "486d48ee4cee7365",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T09:05:34.545570924Z",
     "start_time": "2026-02-15T09:05:32.144488076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from chat_template import chat_template\n",
    "\n",
    "\n",
    "def tokenize_robots(batch: LazyBatch):\n",
    "    results = [\n",
    "        tokenizer.encode(chat_template(row)).ids\n",
    "        for row in batch[\"messages\"]\n",
    "    ]\n",
    "    return {\"tokens\": results}\n",
    "\n",
    "\n",
    "ds_test.map(\n",
    "    tokenize_robots,\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    num_proc=processes\n",
    ").select_columns(\"tokens\").save_to_disk(\"tokenized_data/robots_test\")\n",
    "\n",
    "ds_train.map(\n",
    "    tokenize_robots,\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    num_proc=processes,\n",
    ").select_columns(\"tokens\").save_to_disk(\"tokenized_data/robots_train\")"
   ],
   "id": "ff09b2b2d9edd170",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2a55b4c956f54b7796bf2542d6aa07a0"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c43330cebc44421e9f577a93c30d3d0f"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/9500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2d788773121c439ca54e2a5646bdbc71"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/9500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "242466cd58974e3c964d836a2a789cc9"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T09:05:34.586493244Z",
     "start_time": "2026-02-15T09:05:34.547450159Z"
    }
   },
   "cell_type": "code",
   "source": "test_tokens(load_from_disk(\"tokenized_data/robots_test\"))",
   "id": "1ad498e3b3bf1252",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aster is a chatbot who answers questions with rhymes.\n",
      "Where did chocolate originate?\n",
      "Chocolate is 4000 years old/Mexico is where it was first sold\n",
      "Where was milk chocolate invented?\n",
      "Switzerland was the first to add milk/To make their chocolate smooth as silk\n",
      "What are some good desserts that use chocolate?\n",
      "Pie, tart, cookies, and cake/Chocolate is great to bake\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Dataset: wikipedia summary",
   "id": "204976b846ac7bfd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T09:05:36.959816162Z",
     "start_time": "2026-02-15T09:05:34.600935915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "splits = load_dataset(\"jordiclive/wikipedia-summary-dataset\", split=\"train\").train_test_split(\n",
    "    test_size=0.1,\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "ds_test = splits[\"test\"].select_columns([\"summary\"])\n",
    "ds_train = splits[\"train\"].select_columns([\"summary\"])"
   ],
   "id": "1ad7ca6881a27196",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T09:18:19.876991359Z",
     "start_time": "2026-02-15T09:10:10.661983241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_wiki(batch: LazyBatch):\n",
    "    eot = special_tokens[\"end_of_text\"]\n",
    "    results = [\n",
    "        tokenizer.encode(row + \"\\n\" + eot).ids\n",
    "        for row in batch[\"summary\"]\n",
    "    ]\n",
    "    return {\"tokens\": results}\n",
    "\n",
    "\n",
    "ds_test.map(\n",
    "    tokenize_wiki,\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    num_proc=processes\n",
    ").select_columns(\"tokens\").save_to_disk(\"tokenized_data/wiki_test\")\n",
    "\n",
    "ds_train.map(\n",
    "    tokenize_wiki,\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    num_proc=processes\n",
    ").select_columns(\"tokens\").save_to_disk(\"tokenized_data/wiki_train\")"
   ],
   "id": "4a89dd722a5e8202",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/775001 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f8b41691c4bc4dabbcd00ea35e44607a"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/775001 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "95cea5e9f93e47d992051b654c7a7884"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/6975006 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6ec149a9dda14b7c96b3b68ba82dc2b3"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/13 shards):   0%|          | 0/6975006 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "832eace9f00a4c4fa0423dffbf405628"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T09:18:21.451896089Z",
     "start_time": "2026-02-15T09:18:20.141929985Z"
    }
   },
   "cell_type": "code",
   "source": "test_tokens(load_from_disk(\"tokenized_data/wiki_test\"))",
   "id": "841d9728bc3695eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category:Populated places in McPherson County, Nebraska\n",
      "McPherson\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Dataset: tiny stories",
   "id": "fd11bc77bf1c1bdf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T09:18:26.856609628Z",
     "start_time": "2026-02-15T09:18:21.468631568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ds_test = load_dataset(\"roneneldan/TinyStories\", split=\"validation\").select_columns([\"text\"])\n",
    "ds_train = load_dataset(\"roneneldan/TinyStories\", split=\"train\").select_columns([\"text\"])"
   ],
   "id": "af288c35ebda2064",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T09:22:29.474454650Z",
     "start_time": "2026-02-15T09:18:26.869001089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_stories(batch: LazyBatch):\n",
    "    eot = special_tokens[\"end_of_text\"]\n",
    "    results = [\n",
    "        tokenizer.encode(row + \"\\n\" + eot).ids\n",
    "        for row in batch[\"text\"]\n",
    "    ]\n",
    "    return {\"tokens\": results}\n",
    "\n",
    "\n",
    "ds_test.map(\n",
    "    tokenize_stories,\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    num_proc=processes\n",
    ").select_columns(\"tokens\").save_to_disk(\"tokenized_data/stories_test\")\n",
    "\n",
    "ds_train.map(\n",
    "    tokenize_stories,\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    num_proc=processes\n",
    ").select_columns(\"tokens\").save_to_disk(\"tokenized_data/stories_train\")"
   ],
   "id": "7f5714539b15204d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/21990 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dc5d0ba23a4148c2912b941731375c9f"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/21990 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e0907f5a54204100839c79aa1c60eeb8"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/2119719 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "893b7ec61e4743b8a3af2bb5eb19023e"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/8 shards):   0%|          | 0/2119719 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f43900b39e2b4365bfafb7db67171351"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T09:22:29.635800793Z",
     "start_time": "2026-02-15T09:22:29.494835952Z"
    }
   },
   "cell_type": "code",
   "source": "test_tokens(load_from_disk(\"tokenized_data/stories_test\"))",
   "id": "37556459f04557e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spot. Spot saw the shiny car and said, \"Wow, Kitty, your car is so bright and clean!\" Kitty smiled and replied, \"Thank you, Spot. I polish it every day.\"\n",
      "\n",
      "After playing with the car, Kitty and Spot felt thirsty. They found a small pond with clear water. They drank the water and felt very happy. They played together all day and became best friends.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Dataset: tiny textbooks",
   "id": "197e0f793f9a78ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T09:22:34.214907807Z",
     "start_time": "2026-02-15T09:22:29.639295850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ds_test = load_dataset(\"nampdn-ai/tiny-textbooks\", split=\"test\").select_columns([\"textbook\"])\n",
    "ds_train = load_dataset(\"nampdn-ai/tiny-textbooks\", split=\"train\").select_columns([\"textbook\"])"
   ],
   "id": "cb8d24aab8e0144a",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T09:25:11.430200664Z",
     "start_time": "2026-02-15T09:22:34.217325744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_textbooks(batch: LazyBatch):\n",
    "    eot = special_tokens[\"end_of_text\"]\n",
    "    results = [\n",
    "        tokenizer.encode(row + \"\\n\" + eot).ids\n",
    "        for row in batch[\"textbook\"]\n",
    "    ]\n",
    "    return {\"tokens\": results}\n",
    "\n",
    "\n",
    "ds_test.map(\n",
    "    tokenize_textbooks,\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    num_proc=processes\n",
    ").select_columns(\"tokens\").save_to_disk(\"tokenized_data/textbooks_test\")\n",
    "\n",
    "ds_train.map(\n",
    "    tokenize_textbooks,\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    num_proc=processes\n",
    ").select_columns(\"tokens\").save_to_disk(\"tokenized_data/textbooks_train\")"
   ],
   "id": "44ff5c4f946d9bea",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/21000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "88145e02a86d458984801e634018e6ea"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/21000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "67949e4a61b94024a89a86ec2211bd76"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/399000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c9815c09109241d39b2890acf588513c"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/5 shards):   0%|          | 0/399000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "120ce18e68914d83816f22ac0189cebd"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T09:25:11.502000651Z",
     "start_time": "2026-02-15T09:25:11.437429638Z"
    }
   },
   "cell_type": "code",
   "source": "test_tokens(load_from_disk(\"tokenized_data/textbooks_test\"))",
   "id": "5a8d0b156694daa5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lesson: How to Analyze a Drama Series\n",
      "\n",
      "Introduction:\n",
      "In this lesson, we will learn how to analyze a drama series by breaking down its plot, characters, and themes. We will use \"Karelasyon\" as our example to demonstrate how to apply these analytical tools to a specific work.\n",
      "\n",
      "Section 1: Plot Analysis\n",
      "\n",
      "Plot refers to the events and conflicts that make up a story. In \"Karelasyon,\" the plot revolves around Carmen and Jess's relationship and how it affects Carmen's daughter, Kat-kat. \n",
      "\n",
      "1. What is the main conflict of the story?\n",
      "The main conflict is Carmen's hope that Jess will treat Kat-kat like his own daughter.\n",
      "\n",
      "2. How does the story develop this conflict?\n",
      "The story develops this conflict through Carmen and Jess's interactions with Kat-kat, showing how their relationship affects her.\n",
      "\n",
      "3. What is the resolution of the story?\n",
      "The resolution is not explicitly stated, but it can be inferred that the story ends with Carmen and Jess continuing to live together and care for Kat-kat.\n",
      "\n",
      "Section 2: Character Analysis\n",
      "\n",
      "Character analysis involves examining the traits, motivations, and actions of the characters in a story. In \"Karelasyon,\" the main characters are Carmen, Jess, and Kat-kat.\n",
      "\n",
      "1. What are Carmen's traits?\n",
      "Carmen is loving, caring, and protective of her daughter, Kat-kat.\n",
      "\n",
      "2. What are Jess's traits?\n",
      "Jess is kind, compassionate, and willing to do anything for the woman he loves, Carmen.\n",
      "\n",
      "3. What is Kat-kat's role in the story?\n",
      "Kat-kat is the child caught in the middle of her mother's relationship with Jess. She is used as a way to show the impact of their relationship on others.\n",
      "\n",
      "Section 3: Theme Analysis\n",
      "\n",
      "Theme analysis involves identifying the underlying messages or ideas conveyed by a story. In \"Karelasyon,\" the themes are love, family, and responsibility.\n",
      "\n",
      "1. What is the theme of the story?\n",
      "The theme of the story is the complexity of blending families and the challenges that come with it.\n",
      "\n",
      "2. How does the story convey this theme?\n",
      "The story conveys this theme through the interactions between Carmen, Jess, and Kat-kat, showing the difficulties that arise when two families combine.\n",
      "\n",
      "Conclusion:\n",
      "In this lesson, we learned how to analyze a drama series by examining its plot, characters, and themes. By applying these analytical tools to \"Karelasyon,\" we were able to identify the main conflict, character traits, and underlying themes of the story. This skill can be applied to other works of literature or film to deepen our understanding and appreciation of the art form.\n",
      "\n",
      "Glossary:\n",
      "- Plot: The events and conflicts that make up a story.\n",
      "- Character: A person in a story, often defined by their traits and actions.\n",
      "- Theme: The underlying message or idea conveyed by a story.\n",
      "\n",
      "[Note: This lesson is intended for primary through high school students. The content may need to be simplified or expanded upon depending on the age group.]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
