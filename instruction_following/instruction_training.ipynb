{
 "cells": [
  {
   "cell_type": "code",
   "id": "39f49c5833de57aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T10:31:13.340397123Z",
     "start_time": "2026-03-01T10:31:13.298004643Z"
    }
   },
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import gdown\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import Dataset, load_from_disk\n",
    "from dotenv import load_dotenv\n",
    "from millify import millify\n",
    "\n",
    "from chat_template import chat_template\n",
    "from learning_metrics import get_grad_metrics\n",
    "from learning_metrics import get_weight_metrics\n",
    "from settings import ModelSettings\n",
    "from special_tokens import special_tokens"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "dbe19c1cdb7088f1",
   "metadata": {},
   "source": [
    "Mode settings"
   ]
  },
  {
   "cell_type": "code",
   "id": "67cf94c976797ed4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T10:31:13.396295508Z",
     "start_time": "2026-03-01T10:31:13.363181433Z"
    }
   },
   "source": [
    "minified = False\n",
    "colab = False\n",
    "thunder = False\n",
    "checkpoint: int | None = None\n",
    "compile = False\n",
    "upload = False\n",
    "cpu_only = False\n",
    "remove_pre_training_prefix = True"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "4c7e0486169f9ddf",
   "metadata": {},
   "source": [
    "Paths"
   ]
  },
  {
   "cell_type": "code",
   "id": "aeae42b7d36357bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T10:31:13.430312481Z",
     "start_time": "2026-03-01T10:31:13.398538220Z"
    }
   },
   "source": [
    "load_dotenv()\n",
    "if upload:\n",
    "    if os.getenv(\"MEGA_EMAIL\") is None or os.getenv(\"MEGA_PASSWORD\") is None:\n",
    "        raise Exception(\"Missing env values\")\n",
    "\n",
    "if colab:\n",
    "    data_dir = \"/content/drive/MyDrive/tokenized_data\"\n",
    "    checkpoint_dir = \"/content/drive/MyDrive/instruction_checkpoints\"\n",
    "    pre_training_file = \"not specified\"\n",
    "elif thunder:\n",
    "    data_dir = \"tokenized_data\"\n",
    "    os.makedirs(\"output/instruction_checkpoints\", exist_ok=True)\n",
    "    if not os.path.exists(data_dir):\n",
    "        gdown.download_folder(id=\"15x5BNdwty_4y5ezFoIVS47i1-H2dCjLv\", output=data_dir)\n",
    "    checkpoint_dir = \"output/instruction_checkpoints\"\n",
    "    if not minified:\n",
    "        pre_training_file = \"tokenized_data/weights.pt\"\n",
    "    else:\n",
    "        pre_training_file = \"pre_checkpoints/state/00599.pt\"\n",
    "else:\n",
    "    data_dir = \"tokenized_data\"\n",
    "    checkpoint_dir = \"instruction_checkpoints\"\n",
    "    if not minified:\n",
    "        pre_training_file = \"pre_training/weights.pt\"\n",
    "    else:\n",
    "        pre_training_file = \"pre_checkpoints/state/00599.pt\"\n",
    "info_dir = checkpoint_dir + \"/info\"\n",
    "state_dir = checkpoint_dir + \"/state\"\n",
    "train_ds_name = data_dir + \"/train_chats\"\n",
    "test_ds_name = data_dir + \"/test_chats\""
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "276f1e2e08b01b20",
   "metadata": {},
   "source": [
    "General settings"
   ]
  },
  {
   "cell_type": "code",
   "id": "4d4c17ce894ef249",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T10:31:13.458853630Z",
     "start_time": "2026-03-01T10:31:13.432673357Z"
    }
   },
   "source": [
    "if not minified:\n",
    "    if colab:\n",
    "        # Training data\n",
    "        block_size = ModelSettings.max_context_length\n",
    "        batch_size = 8\n",
    "\n",
    "        # Learning\n",
    "        max_iters = 6_000\n",
    "        learning_rate = 1e-6\n",
    "        eval_iters = 30\n",
    "        eval_interval = 300\n",
    "        grad_clip = 1.0\n",
    "        log_metrics_interval = 30\n",
    "        log_text = 100\n",
    "    elif thunder:\n",
    "        # Training data\n",
    "        block_size = ModelSettings.max_context_length\n",
    "        batch_size = 32\n",
    "\n",
    "        # Learning\n",
    "        max_iters = 6_000\n",
    "        learning_rate = 5e-5\n",
    "        eval_iters = 30\n",
    "        eval_interval = 300\n",
    "        grad_clip = 1.0\n",
    "        log_metrics_interval = 30\n",
    "        log_text = 100\n",
    "    else:\n",
    "        # Training data\n",
    "        block_size = ModelSettings.max_context_length\n",
    "        batch_size = 8\n",
    "\n",
    "        # Learning\n",
    "        max_iters = 20_000\n",
    "        learning_rate = 5e-5\n",
    "        eval_iters = 20\n",
    "        eval_interval = 1000\n",
    "        grad_clip = 1.0\n",
    "        log_metrics_interval = 100\n",
    "        log_text = 100\n",
    "else:\n",
    "    # Training data\n",
    "    block_size = 64\n",
    "    batch_size = 8\n",
    "\n",
    "    # Learning\n",
    "    max_iters = 5000\n",
    "    learning_rate = 1e-4\n",
    "    eval_iters = 10\n",
    "    eval_interval = 20\n",
    "    grad_clip = 1.0\n",
    "    log_metrics_interval = 2\n",
    "    log_text = 50\n",
    "\n",
    "print(\"epochs\", max_iters * batch_size / 20_000)\n",
    "print(\"chats\", max_iters * batch_size)\n",
    "print(\"max tokens\", millify(max_iters * batch_size * block_size))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 8.0\n",
      "chats 160000\n",
      "max tokens 160M\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "82a7bd6e9430b08d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T10:31:13.533678473Z",
     "start_time": "2026-03-01T10:31:13.474968339Z"
    }
   },
   "source": [
    "from tokenizers.tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"tokenizer.json\")\n",
    "pad_id = tokenizer.token_to_id(special_tokens[\"pad\"])"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "71caaf7a7a523d74",
   "metadata": {},
   "source": [
    "Hardware settings"
   ]
  },
  {
   "cell_type": "code",
   "id": "b4068dad6e64b371",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T10:31:13.569196637Z",
     "start_time": "2026-03-01T10:31:13.535617371Z"
    }
   },
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn\n",
    "device = \"cuda\" if torch.cuda.is_available() and not cpu_only else \"cpu\"\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
    "# autocast_enabled = device_type == \"cuda\"\n",
    "autocast_enabled = False\n",
    "print(device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "b2238c5052b49fca",
   "metadata": {},
   "source": [
    "Training data stream"
   ]
  },
  {
   "cell_type": "code",
   "id": "36aa087fd28cc248",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T10:31:13.582905416Z",
     "start_time": "2026-03-01T10:31:13.571764764Z"
    }
   },
   "source": [
    "def infinite_iterator(ds: Dataset):\n",
    "    n = 0\n",
    "    while True:\n",
    "        iterator = iter(ds.shuffle(n))\n",
    "        n += 1\n",
    "        for el in iterator:\n",
    "            yield el"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "ddc05c0fccf1c81b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T10:31:13.600599737Z",
     "start_time": "2026-03-01T10:31:13.583908003Z"
    }
   },
   "source": [
    "ignore_index = -100\n",
    "if not minified:\n",
    "    ds_test = infinite_iterator(load_from_disk(test_ds_name))\n",
    "    ds_train = infinite_iterator(load_from_disk(train_ds_name))\n",
    "else:\n",
    "    ds_test = infinite_iterator(load_from_disk(test_ds_name).take(100))\n",
    "    ds_train = infinite_iterator(load_from_disk(test_ds_name).take(10))\n",
    "\n",
    "\n",
    "def prepare_chat(chat, target_size, pad_element):\n",
    "    token_ids = chat[\"tokens\"]\n",
    "    assistant_mask = chat[\"assistant_mask\"]\n",
    "    length = len(token_ids)\n",
    "    # truncate to target size\n",
    "    if length > target_size:\n",
    "        trim = length - target_size\n",
    "        return token_ids[trim:], assistant_mask[trim:]\n",
    "    # pad to target size\n",
    "    if length < target_size:\n",
    "        padding = target_size - length\n",
    "        return token_ids + [pad_element] * padding, assistant_mask + [False] * padding\n",
    "    # unchanged\n",
    "    return token_ids, assistant_mask\n",
    "\n",
    "\n",
    "def apply_mask(tokens, assistant_mask):\n",
    "    return [\n",
    "        t if assistant_mask[i] else ignore_index\n",
    "        for i, t in enumerate(tokens)\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    iterator = ds_train if split == 'train' else ds_test\n",
    "    batch = [next(iterator) for _ in range(batch_size)]\n",
    "    longest_chat = max([len(row[\"tokens\"]) for row in batch])\n",
    "    target_length = min(longest_chat, block_size + 1)\n",
    "    chats = [prepare_chat(chat, target_length, pad_id) for chat in batch]\n",
    "    x = torch.stack([torch.tensor(tokens[0:target_length - 1], dtype=torch.long) for tokens, mask in chats])\n",
    "    y = torch.stack(\n",
    "        [torch.tensor(apply_mask(tokens[1:target_length], mask[1:target_length]), dtype=torch.long) for tokens, mask in\n",
    "         chats])\n",
    "    if device == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "16cb181844baceb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T10:31:13.708487383Z",
     "start_time": "2026-03-01T10:31:13.602148258Z"
    }
   },
   "source": [
    "test = get_batch(split=\"test\")\n",
    "print(test[0].shape)\n",
    "print(test[1].shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 278])\n",
      "torch.Size([8, 278])\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T10:31:13.744541922Z",
     "start_time": "2026-03-01T10:31:13.710855321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def inspect_dataset(token_ids):\n",
    "    token_ids = list(map(lambda x: x if x != -100 else 0, token_ids))\n",
    "    print(tokenizer.decode(token_ids, skip_special_tokens=True))\n",
    "\n",
    "\n",
    "inspect_dataset(test[0][0].tolist())\n",
    "inspect_dataset(test[1][0].tolist())"
   ],
   "id": "6550c4c46ade3b6c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describe what makes the ocean unique.\n",
      "The ocean is a large and complex ecosystem that covers more than seventy percent of the Earth's surface. It is incredibly unique for several reasons, including:\n",
      "\n",
      "1. Biodiversity: The ocean is home to an astonishing variety of plant and animal life, from microscopic organisms to huge whales, and from coral reefs to deep sea trenches.\n",
      "\n",
      "2. Climate regulation: The ocean plays a critical role in regulating the Earth's climate, acting as both a sink and a heat pump. By absorbing and storing large amounts of carbon dioxide, the ocean helps to reduce the greenhouse effect. It also influences weather patterns by redistributing heat through its currents.\n",
      "\n",
      "3. Geology: The ocean holds some of the Earth's most varied underwater geological formations, including volcanic vents, undersea mountains, and deep ocean trenches.\n",
      "\n",
      "4. Economy: The ocean supports a vast array of human activities, from fishing and shipping to tourism and recreation.\n",
      "\n",
      "5. Unexplored mystery: Despite years of exploration, much of the ocean remains a mystery, with vast areas that have never been explored. As such, it holds the promise of new discoveries, new resources, and new understanding of how the Earth and its complex systems work.\n",
      "\n",
      "The ocean is a large and complex ecosystem that covers more than seventy percent of the Earth's surface. It is incredibly unique for several reasons, including:\n",
      "\n",
      "1. Biodiversity: The ocean is home to an astonishing variety of plant and animal life, from microscopic organisms to huge whales, and from coral reefs to deep sea trenches.\n",
      "\n",
      "2. Climate regulation: The ocean plays a critical role in regulating the Earth's climate, acting as both a sink and a heat pump. By absorbing and storing large amounts of carbon dioxide, the ocean helps to reduce the greenhouse effect. It also influences weather patterns by redistributing heat through its currents.\n",
      "\n",
      "3. Geology: The ocean holds some of the Earth's most varied underwater geological formations, including volcanic vents, undersea mountains, and deep ocean trenches.\n",
      "\n",
      "4. Economy: The ocean supports a vast array of human activities, from fishing and shipping to tourism and recreation.\n",
      "\n",
      "5. Unexplored mystery: Despite years of exploration, much of the ocean remains a mystery, with vast areas that have never been explored. As such, it holds the promise of new discoveries, new resources, and new understanding of how the Earth and its complex systems work.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T10:31:13.759693509Z",
     "start_time": "2026-03-01T10:31:13.748473559Z"
    }
   },
   "cell_type": "code",
   "source": "del test",
   "id": "c46385238c8adb50",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Scaler for FP16",
   "id": "993357886ab615f7"
  },
  {
   "cell_type": "code",
   "id": "756fe3809acc0c3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T10:31:13.775887251Z",
     "start_time": "2026-03-01T10:31:13.764568926Z"
    }
   },
   "source": [
    "scaler = torch.amp.GradScaler(device_type)"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "32532e7c9d6b345",
   "metadata": {},
   "source": [
    "Model settings"
   ]
  },
  {
   "cell_type": "code",
   "id": "d529ab02a58a4ac0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T10:31:14.298955082Z",
     "start_time": "2026-03-01T10:31:13.798953328Z"
    }
   },
   "source": [
    "from model import ChatModel\n",
    "from settings import ModelSettings\n",
    "\n",
    "if not minified:\n",
    "    model = ChatModel(\n",
    "        vocabulary_size=ModelSettings.vocabulary_size,\n",
    "        embedding_size=ModelSettings.embedding_size,\n",
    "        max_context_length=block_size,\n",
    "        ff_size_multiplier=ModelSettings.ff_size_multiplier,\n",
    "        transformer_blocks=ModelSettings.transformer_blocks,\n",
    "        attention_heads=ModelSettings.attention_heads,\n",
    "        dropout=ModelSettings.dropout,\n",
    "        bias=ModelSettings.bias,\n",
    "        device=device,\n",
    "    )\n",
    "else:\n",
    "    model = ChatModel(\n",
    "        vocabulary_size=ModelSettings.vocabulary_size,\n",
    "        embedding_size=64,\n",
    "        max_context_length=block_size,\n",
    "        ff_size_multiplier=2,\n",
    "        transformer_blocks=4,\n",
    "        attention_heads=4,\n",
    "        dropout=0.0,\n",
    "        bias=ModelSettings.bias,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "if compile:\n",
    "    model = torch.compile(model)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using flash attention\n",
      "using flash attention\n",
      "using flash attention\n",
      "using flash attention\n",
      "using flash attention\n",
      "using flash attention\n",
      "using flash attention\n",
      "using flash attention\n",
      "using flash attention\n",
      "using flash attention\n",
      "using flash attention\n",
      "using flash attention\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "ea0b85297608fe87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T10:31:14.353695756Z",
     "start_time": "2026-03-01T10:31:14.315316159Z"
    }
   },
   "source": [
    "def freeze_lower_layers(num_freeze):\n",
    "    # embeddings\n",
    "    model.emb.requires_grad_(False)\n",
    "\n",
    "    # transformer blocks\n",
    "    for i in range(num_freeze):\n",
    "        model.transformer[i].requires_grad_(False)\n",
    "\n",
    "\n",
    "# freeze_lower_layers(2 if minified else 6)\n",
    "\n",
    "print(\"trainable\", len([n for n, p in model.named_parameters() if p.requires_grad]))\n",
    "print(\"un-trainable\", len([n for n, p in model.named_parameters() if not p.requires_grad]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable 148\n",
      "un-trainable 0\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "4c9bf0633d5a4597",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T10:31:14.363348501Z",
     "start_time": "2026-03-01T10:31:14.354676077Z"
    }
   },
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "dc195a20418e5df9",
   "metadata": {},
   "source": [
    "Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "id": "f8dc1093f08e8628",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T10:31:15.644471934Z",
     "start_time": "2026-03-01T10:31:14.365126001Z"
    }
   },
   "source": [
    "from optimizer import get_optim_groups\n",
    "\n",
    "optim_groups = get_optim_groups(model)\n",
    "\n",
    "# apply dynamic learning rate to the optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    optim_groups,\n",
    "    lr=learning_rate,\n",
    "    betas=(0.9, 0.95),\n",
    "    eps=1e-8\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "3dc19bb659d08265",
   "metadata": {},
   "source": [
    "Generate"
   ]
  },
  {
   "cell_type": "code",
   "id": "fa87a85c5d97f75f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T10:31:15.780167302Z",
     "start_time": "2026-03-01T10:31:15.750321432Z"
    }
   },
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, start, max_new_tokens=50):\n",
    "    model.eval()\n",
    "    idx = torch.tensor([tokenizer.encode(start, add_special_tokens=False).ids], device=device, dtype=torch.long)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -ModelSettings.max_context_length:]\n",
    "        logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = nn.functional.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "\n",
    "    model.train()\n",
    "    return tokenizer.decode(idx[0].tolist())"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "24e3028ea893d325",
   "metadata": {},
   "source": [
    "Checkpointer"
   ]
  },
  {
   "cell_type": "code",
   "id": "47f3bb6eba858215",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T10:31:15.795550243Z",
     "start_time": "2026-03-01T10:31:15.783166891Z"
    }
   },
   "source": [
    "os.makedirs(info_dir, exist_ok=True)\n",
    "os.makedirs(state_dir, exist_ok=True)\n",
    "\n",
    "if not minified:\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"I'm trying to create a menu with different kinds of pasta. Help me come up with different types of pasta and what they are best used for.\"\n",
    "    }]\n",
    "else:\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello.\"\n",
    "    }]\n",
    "test_text = chat_template(messages, add_generation_token=True)\n",
    "\n",
    "\n",
    "def save_checkpoint(\n",
    "        step,\n",
    "        model,\n",
    "        optimizer,\n",
    "        scaler,\n",
    "        train_loss,\n",
    "        val_loss,\n",
    "        metric_logs\n",
    "):\n",
    "    state = {\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"scaler\": scaler.state_dict() if scaler else None,\n",
    "    }\n",
    "\n",
    "    info = {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"time\": datetime.now().isoformat(),\n",
    "        \"block_size\": block_size,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"eval_interval\": eval_interval,\n",
    "        \"step\": step,\n",
    "        \"text\": generate(model, test_text, log_text),\n",
    "        \"metrics\": json.dumps(metric_logs)\n",
    "    }\n",
    "\n",
    "    state_path = f\"{state_dir}/{step:05d}.pt\"\n",
    "    info_path = f\"{info_dir}/{step:05d}.pt\"\n",
    "\n",
    "    torch.save(state, state_path)\n",
    "    torch.save(info, info_path)\n",
    "\n",
    "    return state_path, info_path"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "id": "cc2939edff54e33a",
   "metadata": {},
   "source": [
    "Load pre-training"
   ]
  },
  {
   "cell_type": "code",
   "id": "21920696f94dfc9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T10:31:16.184546325Z",
     "start_time": "2026-03-01T10:31:15.797799782Z"
    }
   },
   "source": [
    "if checkpoint is None:\n",
    "    state = torch.load(pre_training_file)\n",
    "\n",
    "    state_dict = state[\"model\"]\n",
    "    if remove_pre_training_prefix:\n",
    "        unwanted_prefix = '_orig_mod.'\n",
    "        for k, v in list(state_dict.items()):\n",
    "            if k.startswith(unwanted_prefix):\n",
    "                state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "\n",
    "    model.load_state_dict(state[\"model\"])\n",
    "    optimizer.load_state_dict(state[\"optimizer\"])\n",
    "    scaler.load_state_dict(state[\"scaler\"])\n",
    "    print(\"Loaded pre-training\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pre-training\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "2ea15b7d65829578",
   "metadata": {},
   "source": [
    "Load training state"
   ]
  },
  {
   "cell_type": "code",
   "id": "1cf313bfd2550641",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T10:31:16.203343069Z",
     "start_time": "2026-03-01T10:31:16.195017287Z"
    }
   },
   "source": [
    "def load_checkpoint(step: int):\n",
    "    state = torch.load(f\"{state_dir}/{step:05d}.pt\")\n",
    "    model.load_state_dict(state[\"model\"])\n",
    "    optimizer.load_state_dict(state[\"optimizer\"])\n",
    "    scaler.load_state_dict(state[\"scaler\"])\n",
    "    print(f\"Loaded checkpoint {step}\")\n",
    "\n",
    "\n",
    "if checkpoint is not None:\n",
    "    load_checkpoint(checkpoint)"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "81f9caa1faa59dcd",
   "metadata": {},
   "source": [
    "Clean up old checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "id": "39f4318ddec2db9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T10:31:16.224616206Z",
     "start_time": "2026-03-01T10:31:16.204722913Z"
    }
   },
   "source": [
    "from checkpoint_cleaner import CheckpointCleaner\n",
    "\n",
    "keep_progress = [0.5, 0.7, 0.8, 0.9]\n",
    "preserve_checkpoints = []\n",
    "i = 0\n",
    "last_keep = 0\n",
    "while True:\n",
    "    i += eval_interval\n",
    "    progress = i / max_iters\n",
    "    target_progress = keep_progress[last_keep]\n",
    "    if progress > target_progress:\n",
    "        state_path = f\"{state_dir}/{i:05d}.pt\"\n",
    "        preserve_checkpoints.append(state_path)\n",
    "        last_keep += 1\n",
    "        if last_keep == len(keep_progress):\n",
    "            break\n",
    "    if i >= max_iters:\n",
    "        break\n",
    "\n",
    "checkpoint_cleaner = CheckpointCleaner(3, preserve_checkpoints)\n",
    "preserve_checkpoints"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['instruction_checkpoints/state/11000.pt',\n",
       " 'instruction_checkpoints/state/15000.pt',\n",
       " 'instruction_checkpoints/state/17000.pt',\n",
       " 'instruction_checkpoints/state/19000.pt']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "id": "6b2775c2e3339206",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "id": "198d433f585502b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T11:30:02.455122956Z",
     "start_time": "2026-03-01T10:31:16.228069165Z"
    }
   },
   "source": [
    "from system_metrics import get_system_metrics\n",
    "\n",
    "metric_logs = []\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "patience = 5\n",
    "min_delta = 0.01\n",
    "patience_counter = 0\n",
    "early_stopping = False\n",
    "\n",
    "for step in range(checkpoint or 0, max_iters):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    if autocast_enabled:\n",
    "        with torch.amp.autocast(dtype=torch.float16, device_type=device_type):\n",
    "            logits, loss = model(xb, yb)\n",
    "    else:\n",
    "        logits, loss = model(xb, yb)\n",
    "\n",
    "    # exit if the loss is invalid\n",
    "    if not torch.isfinite(loss):\n",
    "        raise Exception(\"Non-finite loss detected.\")\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.unscale_(optimizer)\n",
    "\n",
    "    if step % log_metrics_interval == 0:\n",
    "        total_norm, max_grad = get_grad_metrics(model)\n",
    "        max_weight, total_weight_norm = get_weight_metrics(model)\n",
    "        metric_logs.append({\n",
    "            \"gradient\": {\n",
    "                \"total_norm\": total_norm,\n",
    "                \"max_grad\": max_grad,\n",
    "            },\n",
    "            \"weight\": {\n",
    "                \"max_weight\": max_weight,\n",
    "                \"total_weight_norm\": total_weight_norm,\n",
    "            },\n",
    "            \"system\": get_system_metrics(),\n",
    "            \"current_loss\": loss.item()\n",
    "        })\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    if step % eval_interval == 0 or step == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        state_path, info_path = save_checkpoint(\n",
    "            step=step,\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            scaler=scaler,\n",
    "            train_loss=losses[\"train\"],\n",
    "            val_loss=losses[\"val\"],\n",
    "            metric_logs=metric_logs\n",
    "        )\n",
    "        checkpoint_cleaner.step(state_path)\n",
    "        metric_logs = []\n",
    "\n",
    "        if early_stopping:\n",
    "            val_loss = losses['val']\n",
    "            improvement = best_val_loss - val_loss\n",
    "            if improvement >= min_delta:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter == patience:\n",
    "                    print(\"Early stopping\")\n",
    "                    break"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 3.4704, val loss 3.4432\n",
      "step 1000: train loss 3.3694, val loss 3.2448\n",
      "step 2000: train loss 3.2807, val loss 3.3079\n",
      "step 3000: train loss 3.2481, val loss 3.3690\n",
      "Removed checkpoint instruction_checkpoints/state/00000.pt\n",
      "step 4000: train loss 3.2726, val loss 3.1984\n",
      "Removed checkpoint instruction_checkpoints/state/01000.pt\n",
      "step 5000: train loss 2.7912, val loss 3.3496\n",
      "Removed checkpoint instruction_checkpoints/state/02000.pt\n",
      "step 6000: train loss 3.1415, val loss 3.2626\n",
      "Removed checkpoint instruction_checkpoints/state/03000.pt\n",
      "step 7000: train loss 3.1670, val loss 3.3034\n",
      "Removed checkpoint instruction_checkpoints/state/04000.pt\n",
      "step 8000: train loss 2.9395, val loss 3.3061\n",
      "Removed checkpoint instruction_checkpoints/state/05000.pt\n",
      "step 9000: train loss 3.1444, val loss 3.4878\n",
      "Removed checkpoint instruction_checkpoints/state/06000.pt\n",
      "step 10000: train loss 2.8687, val loss 3.3082\n",
      "Removed checkpoint instruction_checkpoints/state/07000.pt\n",
      "step 11000: train loss 2.9780, val loss 3.3156\n",
      "Removed checkpoint instruction_checkpoints/state/08000.pt\n",
      "step 12000: train loss 3.0956, val loss 3.3144\n",
      "Removed checkpoint instruction_checkpoints/state/09000.pt\n",
      "step 13000: train loss 2.8897, val loss 3.3122\n",
      "Removed checkpoint instruction_checkpoints/state/10000.pt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[29], line 47\u001B[0m\n\u001B[1;32m     32\u001B[0m     metric_logs\u001B[38;5;241m.\u001B[39mappend({\n\u001B[1;32m     33\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgradient\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\n\u001B[1;32m     34\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtotal_norm\u001B[39m\u001B[38;5;124m\"\u001B[39m: total_norm,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     42\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcurrent_loss\u001B[39m\u001B[38;5;124m\"\u001B[39m: loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m     43\u001B[0m     })\n\u001B[1;32m     45\u001B[0m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mclip_grad_norm_(model\u001B[38;5;241m.\u001B[39mparameters(), \u001B[38;5;241m1.0\u001B[39m)\n\u001B[0;32m---> 47\u001B[0m \u001B[43mscaler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     48\u001B[0m scaler\u001B[38;5;241m.\u001B[39mupdate()\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m step \u001B[38;5;241m%\u001B[39m eval_interval \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m step \u001B[38;5;241m==\u001B[39m max_iters \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[0;32m~/PycharmProjects/PythonProject1/.venv/lib/python3.11/site-packages/torch/amp/grad_scaler.py:463\u001B[0m, in \u001B[0;36mGradScaler.step\u001B[0;34m(self, optimizer, *args, **kwargs)\u001B[0m\n\u001B[1;32m    457\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39munscale_(optimizer)\n\u001B[1;32m    459\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(optimizer_state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfound_inf_per_device\u001B[39m\u001B[38;5;124m\"\u001B[39m]) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m, (\n\u001B[1;32m    460\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo inf checks were recorded for this optimizer.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    461\u001B[0m )\n\u001B[0;32m--> 463\u001B[0m retval \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_maybe_opt_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer_state\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    465\u001B[0m optimizer_state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstage\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m OptState\u001B[38;5;241m.\u001B[39mSTEPPED\n\u001B[1;32m    467\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m retval\n",
      "File \u001B[0;32m~/PycharmProjects/PythonProject1/.venv/lib/python3.11/site-packages/torch/amp/grad_scaler.py:356\u001B[0m, in \u001B[0;36mGradScaler._maybe_opt_step\u001B[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001B[0m\n\u001B[1;32m    348\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_maybe_opt_step\u001B[39m(\n\u001B[1;32m    349\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    350\u001B[0m     optimizer: torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mOptimizer,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    353\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    354\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Optional[\u001B[38;5;28mfloat\u001B[39m]:\n\u001B[1;32m    355\u001B[0m     retval: Optional[\u001B[38;5;28mfloat\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 356\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28msum\u001B[39m(v\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m optimizer_state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfound_inf_per_device\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues()):\n\u001B[1;32m    357\u001B[0m         retval \u001B[38;5;241m=\u001B[39m optimizer\u001B[38;5;241m.\u001B[39mstep(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    358\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m retval\n",
      "File \u001B[0;32m~/PycharmProjects/PythonProject1/.venv/lib/python3.11/site-packages/torch/amp/grad_scaler.py:356\u001B[0m, in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    348\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_maybe_opt_step\u001B[39m(\n\u001B[1;32m    349\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    350\u001B[0m     optimizer: torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mOptimizer,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    353\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    354\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Optional[\u001B[38;5;28mfloat\u001B[39m]:\n\u001B[1;32m    355\u001B[0m     retval: Optional[\u001B[38;5;28mfloat\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 356\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28msum\u001B[39m(\u001B[43mv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m optimizer_state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfound_inf_per_device\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues()):\n\u001B[1;32m    357\u001B[0m         retval \u001B[38;5;241m=\u001B[39m optimizer\u001B[38;5;241m.\u001B[39mstep(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    358\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m retval\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "id": "1173b07565d315a4",
   "metadata": {},
   "source": [
    "Test the model"
   ]
  },
  {
   "cell_type": "code",
   "id": "3b89a73b965f9742",
   "metadata": {},
   "source": [
    "print(generate(model, test_text, 20 if minified else 200))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a05fccf1bfc380fe",
   "metadata": {},
   "source": [
    "Export results if needed"
   ]
  },
  {
   "cell_type": "code",
   "id": "6ea94c22715a2c02",
   "metadata": {},
   "source": [
    "from zip import zip_directory\n",
    "\n",
    "if upload:\n",
    "    output_path = \"output.zip\"\n",
    "    if os.path.exists(output_path):\n",
    "        os.remove(output_path)\n",
    "    zip_directory(checkpoint_dir, output_path)\n",
    "    # Mega has a large chance to silently forget the file after successful upload both the sync and async clients, the uploads are downloaded manually from the instance\n",
    "    # asyncio.run(mega_upload(output_path))"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonproject1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
