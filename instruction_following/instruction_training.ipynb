{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d0cd15",
   "metadata": {},
   "source": [
    "fix redownloading gdrive data\n",
    "download only from thunder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39f49c5833de57aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T11:26:22.229999103Z",
     "start_time": "2026-02-19T11:26:20.295779730Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from learning_metrics import get_grad_metrics\n",
    "from learning_metrics import get_weight_metrics\n",
    "from settings import ModelSettings\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe19c1cdb7088f1",
   "metadata": {},
   "source": [
    "Mode settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cf94c976797ed4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T11:26:22.275547944Z",
     "start_time": "2026-02-19T11:26:22.255256453Z"
    }
   },
   "outputs": [],
   "source": [
    "minified = False\n",
    "colab = False\n",
    "thunder = True\n",
    "checkpoint: int | None = None\n",
    "compile = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7e0486169f9ddf",
   "metadata": {},
   "source": [
    "Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeae42b7d36357bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T11:26:22.296421997Z",
     "start_time": "2026-02-19T11:26:22.279517845Z"
    }
   },
   "outputs": [],
   "source": [
    "if colab:\n",
    "    data_dir = \"/content/drive/MyDrive\"\n",
    "    checkpoint_dir = \"/content/drive/MyDrive/pre_checkpoints\"\n",
    "elif thunder:\n",
    "    os.makedirs(\"output/pre_checkpoints\", exist_ok=True)\n",
    "    if not os.path.exists(\"tokenized_data/train.bin\"):\n",
    "        gdown.download(id=\"15t3259RbsF772b35aaZGouGwQopFAX96\",output=\"tokenized_data/train.bin\")\n",
    "    if not os.path.exists(\"tokenized_data/test.bin\"):\n",
    "        gdown.download(id=\"1rE_MOBhBPQGUuhYmevNZOFj-LMBWkLFD\",output=\"tokenized_data/test.bin\")\n",
    "    data_dir = \"tokenized_data\"\n",
    "    checkpoint_dir = \"output/pre_checkpoints\"\n",
    "else:\n",
    "    data_dir = \"tokenized_data\"\n",
    "    checkpoint_dir = \"pre_checkpoints\"\n",
    "info_dir = checkpoint_dir + \"/info\"\n",
    "state_dir = checkpoint_dir + \"/state\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276f1e2e08b01b20",
   "metadata": {},
   "source": [
    "General settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4c17ce894ef249",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T11:26:22.323700380Z",
     "start_time": "2026-02-19T11:26:22.300608041Z"
    }
   },
   "outputs": [],
   "source": [
    "if not minified:\n",
    "    # Training data\n",
    "    block_size = ModelSettings.max_context_length\n",
    "    batch_size = 32\n",
    "\n",
    "    # Learning\n",
    "    max_iters = 200_000  # too big, had to exit at 100k\n",
    "    learning_rate = 6e-4\n",
    "    min_lr = 6e-5\n",
    "    lr_decay_steps = max_iters  # should be ~= max_iters per Chinchilla\n",
    "    warmup_steps = 4000\n",
    "    eval_iters = 100\n",
    "    eval_interval = 2000\n",
    "    grad_clip = 1.0\n",
    "    log_metrics_interval = 100\n",
    "    log_text=100\n",
    "else:\n",
    "    # Training data\n",
    "    block_size = 64\n",
    "    batch_size = 8\n",
    "\n",
    "    # Learning\n",
    "    max_iters = 600  \n",
    "    learning_rate = 6e-3\n",
    "    min_lr = 6e-4\n",
    "    lr_decay_steps = max_iters  # should be ~= max_iters per Chinchilla\n",
    "    warmup_steps = 60\n",
    "    eval_iters = 2\n",
    "    eval_interval = 10\n",
    "    grad_clip = 1.0\n",
    "    log_metrics_interval = 2\n",
    "    log_text=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51b16ef728a43388",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T11:26:22.485857139Z",
     "start_time": "2026-02-19T11:26:22.326822683Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_iters * batch_size * block_size / 1_500_000_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71caaf7a7a523d74",
   "metadata": {},
   "source": [
    "Hardware settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4068dad6e64b371",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T16:26:17.295981409Z",
     "start_time": "2026-02-18T16:26:17.240921771Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
    "autocast_enabled = device_type == \"cuda\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2238c5052b49fca",
   "metadata": {},
   "source": [
    "Training data stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea8c6c54e89cf203",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T16:26:17.325069681Z",
     "start_time": "2026-02-18T16:26:17.305103302Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
    "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
    "    if split == 'train':\n",
    "        data = np.memmap(os.path.join(data_dir, 'train.bin' if not minified else \"test.bin\"), dtype=np.uint16, mode='r')\n",
    "    else:\n",
    "        data = np.memmap(os.path.join(data_dir, 'test.bin'), dtype=np.uint16, mode='r')\n",
    "    if not minified:\n",
    "        ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    else:\n",
    "        ix = torch.randint(5000 - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i + block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i + 1:i + 1 + block_size]).astype(np.int64)) for i in ix])\n",
    "    if device == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c9bf0633d5a4597",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T16:26:17.344676189Z",
     "start_time": "2026-02-18T16:26:17.329107199Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8594f48b269e2a2f",
   "metadata": {},
   "source": [
    "Scaler for FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "756fe3809acc0c3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T16:26:17.383780507Z",
     "start_time": "2026-02-18T16:26:17.348888045Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = torch.amp.GradScaler(device_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32532e7c9d6b345",
   "metadata": {},
   "source": [
    "Model settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d529ab02a58a4ac0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T16:26:24.202382100Z",
     "start_time": "2026-02-18T16:26:17.390285030Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using flash attention\n",
      "using flash attention\n",
      "using flash attention\n",
      "using flash attention\n"
     ]
    }
   ],
   "source": [
    "from model import ChatModel\n",
    "from settings import ModelSettings\n",
    "\n",
    "if not minified:\n",
    "    model = ChatModel(\n",
    "        vocabulary_size=ModelSettings.vocabulary_size,\n",
    "        embedding_size=ModelSettings.embedding_size,\n",
    "        max_context_length=block_size,\n",
    "        ff_size_multiplier=ModelSettings.ff_size_multiplier,\n",
    "        transformer_blocks=ModelSettings.transformer_blocks,\n",
    "        attention_heads=ModelSettings.attention_heads,\n",
    "        dropout=0.0,\n",
    "        bias=ModelSettings.bias,\n",
    "        device=device,\n",
    "    )\n",
    "else:\n",
    "    model = ChatModel(\n",
    "        vocabulary_size=ModelSettings.vocabulary_size,\n",
    "        embedding_size=64,\n",
    "        max_context_length=block_size,\n",
    "        ff_size_multiplier=2,\n",
    "        transformer_blocks=4,\n",
    "        attention_heads=4,\n",
    "        dropout=0.0,\n",
    "        bias=ModelSettings.bias,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "if compile:\n",
    "    model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc195a20418e5df9",
   "metadata": {},
   "source": [
    "Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8dc1093f08e8628",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T16:26:24.259032205Z",
     "start_time": "2026-02-18T16:26:24.239224801Z"
    }
   },
   "outputs": [],
   "source": [
    "from optimizer import get_optim_groups\n",
    "\n",
    "optim_groups = get_optim_groups(model)\n",
    "\n",
    "# apply dynamic learning rate to the optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    optim_groups,\n",
    "    lr=3e-4,\n",
    "    betas=(0.9, 0.95),\n",
    "    eps=1e-8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc19bb659d08265",
   "metadata": {},
   "source": [
    "Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa87a85c5d97f75f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T16:26:24.402305875Z",
     "start_time": "2026-02-18T16:26:24.261547278Z"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers.tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"tokenizer.json\")\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(model, start, max_new_tokens=50):\n",
    "    idx = torch.tensor([tokenizer.encode(start,add_special_tokens=False).ids], device=device, dtype=torch.long)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -ModelSettings.max_context_length:]\n",
    "        logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = nn.functional.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "\n",
    "    return tokenizer.decode(idx[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e3028ea893d325",
   "metadata": {},
   "source": [
    "Checkpointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47f3bb6eba858215",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T16:26:24.432061517Z",
     "start_time": "2026-02-18T16:26:24.412437310Z"
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs(info_dir, exist_ok=True)\n",
    "os.makedirs(state_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_checkpoint(\n",
    "        step,\n",
    "        model,\n",
    "        optimizer,\n",
    "        scaler,\n",
    "        train_loss,\n",
    "        val_loss,\n",
    "        learning_rate,\n",
    "        metric_logs\n",
    "):\n",
    "    state = {\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"scaler\": scaler.state_dict() if scaler else None,\n",
    "    }\n",
    "\n",
    "    info = {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"time\": datetime.now().isoformat(),\n",
    "        \"block_size\": block_size,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"eval_interval\": eval_interval,\n",
    "        \"step\": step,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"text\": generate(model, \"Once upon a time\", log_text),\n",
    "        \"metrics\": json.dumps(metric_logs)\n",
    "    }\n",
    "\n",
    "    state_path = f\"{state_dir}/{step:05d}.pt\"\n",
    "    info_path = f\"{info_dir}/{step:05d}.pt\"\n",
    "\n",
    "    torch.save(state, state_path)\n",
    "    torch.save(info, info_path)\n",
    "\n",
    "    return state_path,info_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea15b7d65829578",
   "metadata": {},
   "source": [
    "Load training state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cf313bfd2550641",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T16:26:24.470561612Z",
     "start_time": "2026-02-18T16:26:24.435897374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint 599\n"
     ]
    }
   ],
   "source": [
    "def load_checkpoint(step: int):\n",
    "    state = torch.load(f\"{state_dir}/{step:05d}.pt\")\n",
    "    model.load_state_dict(state[\"model\"])\n",
    "    optimizer.load_state_dict(state[\"optimizer\"])\n",
    "    scaler.load_state_dict(state[\"scaler\"])\n",
    "    print(f\"Loaded checkpoint {step}\")\n",
    "\n",
    "\n",
    "if checkpoint is not None:\n",
    "    load_checkpoint(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704cd5792d35ab75",
   "metadata": {},
   "source": [
    "Learning scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15fc11a7c6b60700",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T16:26:24.491044039Z",
     "start_time": "2026-02-18T16:26:24.474836351Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def get_lr(step):\n",
    "    # 1) linear warmup for warmup_steps steps\n",
    "    if step < warmup_steps:\n",
    "        return learning_rate * (step + 1) / (warmup_steps + 1)\n",
    "    # 2) if it > lr_decay_steps, return min learning rate\n",
    "    if step > lr_decay_steps:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (step - warmup_steps) / (lr_decay_steps - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coefficient = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coefficient ranges 0..1\n",
    "    return min_lr + coefficient * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f9caa1faa59dcd",
   "metadata": {},
   "source": [
    "Clean up old checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39f4318ddec2db9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T16:26:24.514304578Z",
     "start_time": "2026-02-18T16:26:24.494841795Z"
    }
   },
   "outputs": [],
   "source": [
    "from checkpoint_cleaner import CheckpointCleaner\n",
    "\n",
    "checkpoint_cleaner=CheckpointCleaner(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2775c2e3339206",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "198d433f585502b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T16:26:52.634395231Z",
     "start_time": "2026-02-18T16:26:24.517332680Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 599: train loss 0.9119, val loss 0.8102\n"
     ]
    }
   ],
   "source": [
    "from system_metrics import get_system_metrics\n",
    "\n",
    "metric_logs = []\n",
    "\n",
    "for step in range(checkpoint or 0, max_iters):\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    if autocast_enabled:\n",
    "        with torch.amp.autocast(dtype=torch.float16, device_type=device_type):\n",
    "            logits, loss = model(xb, yb)\n",
    "    else:\n",
    "        logits, loss = model(xb, yb)\n",
    "\n",
    "    # exit if the loss is invalid\n",
    "    if not torch.isfinite(loss):\n",
    "        raise Exception(\"Non-finite loss detected.\")\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.unscale_(optimizer)\n",
    "\n",
    "    if step % log_metrics_interval == 0:\n",
    "        total_norm, max_grad = get_grad_metrics(model)\n",
    "        max_weight, total_weight_norm = get_weight_metrics(model)\n",
    "        metric_logs.append({\n",
    "            \"gradient\": {\n",
    "                \"total_norm\": total_norm,\n",
    "                \"max_grad\": max_grad,\n",
    "            },\n",
    "            \"weight\": {\n",
    "                \"max_weight\": max_weight,\n",
    "                \"total_weight_norm\": total_weight_norm,\n",
    "            },\n",
    "            \"system\": get_system_metrics(),\n",
    "            \"current_loss\": loss.item()\n",
    "        })\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    if step % eval_interval == 0 or step == max_iters-1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        state_path,info_path= save_checkpoint(\n",
    "            step=step,\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            scaler=scaler,\n",
    "            train_loss=losses[\"train\"],\n",
    "            val_loss=losses[\"val\"],\n",
    "            learning_rate=lr,\n",
    "            metric_logs=metric_logs\n",
    "        )\n",
    "        checkpoint_cleaner.step(state_path)\n",
    "        metric_logs = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1173b07565d315a4",
   "metadata": {},
   "source": [
    "Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b89a73b965f9742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï¿½Ogetepisl orerts that built. Include pop music, the album, B?\n",
      "R like the early blend across the album, the album, the rainyeda be theuto. release of the controls of t\n"
     ]
    }
   ],
   "source": [
    "start_token_id = get_batch(\"test\")[0][0][0].item()\n",
    "start_text = tokenizer.decode([start_token_id])\n",
    "print(generate(model, start_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonproject1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
