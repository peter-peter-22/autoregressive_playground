{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T16:50:31.225806304Z",
     "start_time": "2026-02-23T16:50:31.064351532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from instruction_following.model import ChatModel\n",
    "\n",
    "model = ChatModel(\n",
    "    vocabulary_size=2000,\n",
    "    embedding_size=64,\n",
    "    max_context_length=200,\n",
    "    ff_size_multiplier=2,\n",
    "    transformer_blocks=4,\n",
    "    attention_heads=4,\n",
    "    dropout=0.0,\n",
    "    bias=True,\n",
    "    device=\"cpu\",\n",
    ")"
   ],
   "id": "e8a0ef2625d75b94",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using flash attention\n",
      "using flash attention\n",
      "using flash attention\n",
      "using flash attention\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T16:50:31.388825244Z",
     "start_time": "2026-02-23T16:50:31.242871334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ],
   "id": "fa676fd34bafa410",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb.token_emb.weight\n",
      "emb.pos_emb.weight\n",
      "transformer.0.attention.0.weight\n",
      "transformer.0.attention.0.bias\n",
      "transformer.0.attention.1.kqv.weight\n",
      "transformer.0.attention.1.kqv.bias\n",
      "transformer.0.attention.1.proj.weight\n",
      "transformer.0.attention.1.proj.bias\n",
      "transformer.0.ff.0.weight\n",
      "transformer.0.ff.0.bias\n",
      "transformer.0.ff.1.net.0.weight\n",
      "transformer.0.ff.1.net.0.bias\n",
      "transformer.0.ff.1.net.2.weight\n",
      "transformer.0.ff.1.net.2.bias\n",
      "transformer.1.attention.0.weight\n",
      "transformer.1.attention.0.bias\n",
      "transformer.1.attention.1.kqv.weight\n",
      "transformer.1.attention.1.kqv.bias\n",
      "transformer.1.attention.1.proj.weight\n",
      "transformer.1.attention.1.proj.bias\n",
      "transformer.1.ff.0.weight\n",
      "transformer.1.ff.0.bias\n",
      "transformer.1.ff.1.net.0.weight\n",
      "transformer.1.ff.1.net.0.bias\n",
      "transformer.1.ff.1.net.2.weight\n",
      "transformer.1.ff.1.net.2.bias\n",
      "transformer.2.attention.0.weight\n",
      "transformer.2.attention.0.bias\n",
      "transformer.2.attention.1.kqv.weight\n",
      "transformer.2.attention.1.kqv.bias\n",
      "transformer.2.attention.1.proj.weight\n",
      "transformer.2.attention.1.proj.bias\n",
      "transformer.2.ff.0.weight\n",
      "transformer.2.ff.0.bias\n",
      "transformer.2.ff.1.net.0.weight\n",
      "transformer.2.ff.1.net.0.bias\n",
      "transformer.2.ff.1.net.2.weight\n",
      "transformer.2.ff.1.net.2.bias\n",
      "transformer.3.attention.0.weight\n",
      "transformer.3.attention.0.bias\n",
      "transformer.3.attention.1.kqv.weight\n",
      "transformer.3.attention.1.kqv.bias\n",
      "transformer.3.attention.1.proj.weight\n",
      "transformer.3.attention.1.proj.bias\n",
      "transformer.3.ff.0.weight\n",
      "transformer.3.ff.0.bias\n",
      "transformer.3.ff.1.net.0.weight\n",
      "transformer.3.ff.1.net.0.bias\n",
      "transformer.3.ff.1.net.2.weight\n",
      "transformer.3.ff.1.net.2.bias\n",
      "ln.weight\n",
      "ln.bias\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-23T16:50:40.864141633Z",
     "start_time": "2026-02-23T16:50:40.554720221Z"
    }
   },
   "source": [
    "def freeze_lower_layers(num_freeze):\n",
    "    # embeddings\n",
    "    model.emb.token_emb.weight.requires_grad = False\n",
    "    # transformer blocks\n",
    "    for i in range(num_freeze):\n",
    "        block=model.transformer[i]\n",
    "\n",
    "    # logit projection ?\n",
    "    pass\n",
    "\n",
    "freeze_lower_layers(2)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerBlock(\n",
      "  (attention): Sequential(\n",
      "    (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): MultiHeadSelfAttention(\n",
      "      (kqv): Linear(in_features=64, out_features=192, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "    )\n",
      "    (2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ff): Sequential(\n",
      "    (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): FeedForward(\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "TransformerBlock(\n",
      "  (attention): Sequential(\n",
      "    (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): MultiHeadSelfAttention(\n",
      "      (kqv): Linear(in_features=64, out_features=192, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "    )\n",
      "    (2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ff): Sequential(\n",
      "    (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): FeedForward(\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "TransformerBlock(\n",
      "  (attention): Sequential(\n",
      "    (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): MultiHeadSelfAttention(\n",
      "      (kqv): Linear(in_features=64, out_features=192, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "    )\n",
      "    (2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ff): Sequential(\n",
      "    (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): FeedForward(\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "TransformerBlock(\n",
      "  (attention): Sequential(\n",
      "    (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): MultiHeadSelfAttention(\n",
      "      (kqv): Linear(in_features=64, out_features=192, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "    )\n",
      "    (2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ff): Sequential(\n",
      "    (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): FeedForward(\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[48], line 10\u001B[0m\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;66;03m# logit projection ?\u001B[39;00m\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m \u001B[43mfreeze_lower_layers\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m6\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[48], line 6\u001B[0m, in \u001B[0;36mfreeze_lower_layers\u001B[0;34m(num_freeze)\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# transformer blocks\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_freeze):\n\u001B[0;32m----> 6\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransformer\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m)\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# logit projection ?\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mpass\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/autoregressive_playground/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py:138\u001B[0m, in \u001B[0;36mSequential.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m    136\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m(OrderedDict(\u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_modules\u001B[38;5;241m.\u001B[39mitems())[idx]))\n\u001B[1;32m    137\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 138\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_item_by_idx\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_modules\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/autoregressive_playground/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py:129\u001B[0m, in \u001B[0;36mSequential._get_item_by_idx\u001B[0;34m(self, iterator, idx)\u001B[0m\n\u001B[1;32m    127\u001B[0m idx \u001B[38;5;241m=\u001B[39m operator\u001B[38;5;241m.\u001B[39mindex(idx)\n\u001B[1;32m    128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;241m-\u001B[39msize \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m idx \u001B[38;5;241m<\u001B[39m size:\n\u001B[0;32m--> 129\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIndexError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindex \u001B[39m\u001B[38;5;132;01m{\u001B[39;00midx\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is out of range\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    130\u001B[0m idx \u001B[38;5;241m%\u001B[39m\u001B[38;5;241m=\u001B[39m size\n\u001B[1;32m    131\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mnext\u001B[39m(islice(iterator, idx, \u001B[38;5;28;01mNone\u001B[39;00m))\n",
      "\u001B[0;31mIndexError\u001B[0m: index 4 is out of range"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T16:50:31.533528602Z",
     "start_time": "2026-02-23T16:50:31.443571755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "print(len(trainable_params))\n",
    "print(trainable_params[0])"
   ],
   "id": "ee7fa83cb3174e24",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "Parameter containing:\n",
      "tensor([[-0.0092,  0.0071,  0.0058,  ...,  0.0197, -0.0118,  0.0243],\n",
      "        [-0.0128, -0.0076,  0.0027,  ...,  0.0079,  0.0266,  0.0207],\n",
      "        [ 0.0146, -0.0019, -0.0017,  ...,  0.0171,  0.0150,  0.0074],\n",
      "        ...,\n",
      "        [ 0.0202,  0.0468,  0.0062,  ..., -0.0055, -0.0167, -0.0068],\n",
      "        [-0.0012, -0.0175,  0.0312,  ..., -0.0589,  0.0183,  0.0069],\n",
      "        [ 0.0173,  0.0115, -0.0099,  ...,  0.0208, -0.0155, -0.0021]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T16:50:31.643928053Z",
     "start_time": "2026-02-23T16:50:31.558631663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "un_trainable_params = [p for p in model.parameters() if not p.requires_grad]\n",
    "print(len(un_trainable_params))\n",
    "print(un_trainable_params[0])"
   ],
   "id": "5b0039dedf2b7760",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Parameter containing:\n",
      "tensor([[-0.0180, -0.0097, -0.0047,  ..., -0.0230,  0.0222, -0.0281],\n",
      "        [-0.0054, -0.0109,  0.0368,  ..., -0.0298,  0.0199, -0.0010],\n",
      "        [-0.0135,  0.0151,  0.0109,  ...,  0.0022,  0.0063,  0.0042],\n",
      "        ...,\n",
      "        [ 0.0291, -0.0151,  0.0160,  ..., -0.0004, -0.0486, -0.0075],\n",
      "        [ 0.0122,  0.0386,  0.0214,  ..., -0.0193,  0.0087,  0.0363],\n",
      "        [ 0.0228, -0.0223, -0.0215,  ...,  0.0157,  0.0246, -0.0068]])\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T16:50:31.719866368Z",
     "start_time": "2026-02-23T16:50:31.646171046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model)"
   ],
   "id": "8bbde0e1fdbcf6ff",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "ChatModel                                          --\n",
       "├─Embedding: 1-1                                   --\n",
       "│    └─Embedding: 2-1                              (128,000)\n",
       "│    └─Embedding: 2-2                              12,800\n",
       "├─Dropout: 1-2                                     --\n",
       "├─Sequential: 1-3                                  --\n",
       "│    └─TransformerBlock: 2-3                       --\n",
       "│    │    └─Sequential: 3-1                        16,768\n",
       "│    │    └─Sequential: 3-2                        16,704\n",
       "│    └─TransformerBlock: 2-4                       --\n",
       "│    │    └─Sequential: 3-3                        16,768\n",
       "│    │    └─Sequential: 3-4                        16,704\n",
       "│    └─TransformerBlock: 2-5                       --\n",
       "│    │    └─Sequential: 3-5                        16,768\n",
       "│    │    └─Sequential: 3-6                        16,704\n",
       "│    └─TransformerBlock: 2-6                       --\n",
       "│    │    └─Sequential: 3-7                        16,768\n",
       "│    │    └─Sequential: 3-8                        16,704\n",
       "├─LayerNorm: 1-4                                   128\n",
       "├─Linear: 1-5                                      (128,000)\n",
       "===========================================================================\n",
       "Total params: 402,816\n",
       "Trainable params: 146,816\n",
       "Non-trainable params: 256,000\n",
       "==========================================================================="
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
