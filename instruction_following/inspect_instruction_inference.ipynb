{
 "cells": [
  {
   "cell_type": "code",
   "id": "809914ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T13:38:25.242689500Z",
     "start_time": "2026-02-23T13:38:24.955415199Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_from_disk\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "from chat_template import chat_template\n",
    "from model import ChatModel\n",
    "from settings import ModelSettings\n",
    "from special_tokens import special_tokens\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"tokenizer.json\")\n",
    "device = \"cpu\"\n",
    "\n",
    "def apply_top_p(logits: torch.Tensor, p: float = 0.9, min_keep: int = 1) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    In-place top-p filtering on logits\n",
    "    Very common pattern in vLLM, transformers, llama.cpp python bindings, etc.\n",
    "    \"\"\"\n",
    "    if p >= 1.0:\n",
    "        return logits\n",
    "\n",
    "    sorted_logits, sorted_idx = logits.sort(descending=True)\n",
    "    sorted_probs = sorted_logits.softmax(dim=-1)\n",
    "    cum_probs = sorted_probs.cumsum(dim=-1)\n",
    "\n",
    "    # Mask tokens after the top-p cutoff\n",
    "    mask = cum_probs > p\n",
    "    # Protect at least min_keep tokens\n",
    "    mask[..., :min_keep] = False\n",
    "\n",
    "    # Scatter back\n",
    "    to_remove = torch.zeros_like(logits, dtype=torch.bool).scatter_(\n",
    "        dim=-1, index=sorted_idx, src=mask\n",
    "    )\n",
    "\n",
    "    logits[to_remove] = -float('inf')\n",
    "    return logits\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(model, start, max_new_tokens=50, temperature=0.7, top_k:int|None=None, argmax=False, stop_tokens=None, top_p=0.9):\n",
    "    idx = torch.tensor(\n",
    "        [[tokenizer.token_to_id(special_tokens[\"bos\"])]+ tokenizer.encode(start, add_special_tokens=False).ids],\n",
    "        device=device,\n",
    "        dtype=torch.long,\n",
    "    )\n",
    "    print(list(map(tokenizer.id_to_token,idx[0].tolist())))\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -ModelSettings.max_context_length:]\n",
    "        logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        if top_k is not None:\n",
    "            top_logits, top_pos = torch.topk(logits, top_k)\n",
    "            logits = torch.where(\n",
    "                logits < top_logits[:, -1],\n",
    "                input=torch.tensor(float(\"-inf\")),\n",
    "                other=logits\n",
    "            )\n",
    "        logits=apply_top_p(logits,top_p)\n",
    "\n",
    "        probs = nn.functional.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "        if argmax:\n",
    "            next_id = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "        else:\n",
    "            next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "\n",
    "        if stop_tokens and next_id.item() in stop_tokens:\n",
    "            print(\"Reached stop token\")\n",
    "            break\n",
    "\n",
    "    return tokenizer.decode(idx[0].tolist())"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "4552ef70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T13:38:26.584132711Z",
     "start_time": "2026-02-23T13:38:25.300389066Z"
    }
   },
   "source": [
    "minified = False\n",
    "\n",
    "if not minified:\n",
    "    model = ChatModel(\n",
    "        vocabulary_size=ModelSettings.vocabulary_size,\n",
    "        embedding_size=ModelSettings.embedding_size,\n",
    "        max_context_length=ModelSettings.max_context_length,\n",
    "        ff_size_multiplier=ModelSettings.ff_size_multiplier,\n",
    "        transformer_blocks=ModelSettings.transformer_blocks,\n",
    "        attention_heads=ModelSettings.attention_heads,\n",
    "        dropout=ModelSettings.dropout,\n",
    "        bias=ModelSettings.bias,\n",
    "        device=device,\n",
    "    )\n",
    "else:\n",
    "    model = ChatModel(\n",
    "        vocabulary_size=ModelSettings.vocabulary_size,\n",
    "        embedding_size=64,\n",
    "        max_context_length=64,\n",
    "        ff_size_multiplier=2,\n",
    "        transformer_blocks=4,\n",
    "        attention_heads=4,\n",
    "        dropout=ModelSettings.dropout,\n",
    "        bias=ModelSettings.bias,\n",
    "        device=device,\n",
    "    )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using flash attention\n",
      "using flash attention\n",
      "using flash attention\n",
      "using flash attention\n",
      "using flash attention\n",
      "using flash attention\n",
      "using flash attention\n",
      "using flash attention\n",
      "using flash attention\n",
      "using flash attention\n",
      "using flash attention\n",
      "using flash attention\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "3d29b4ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T13:38:28.605890138Z",
     "start_time": "2026-02-23T13:38:28.097705889Z"
    }
   },
   "source": [
    "step = 5999\n",
    "state = torch.load(f\"instruction_output_colab/{step:05d}_with_dropout.pt\", map_location=torch.device('cpu'))"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "d06a9442",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T13:38:29.952670498Z",
     "start_time": "2026-02-23T13:38:29.806084568Z"
    }
   },
   "source": [
    "state_dict = state[\"model\"]\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k, v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "3bb2db89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T13:38:30.097953141Z",
     "start_time": "2026-02-23T13:38:29.958871252Z"
    }
   },
   "source": [
    "model.load_state_dict(state[\"model\"])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "c2fb924f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T13:38:30.294574158Z",
     "start_time": "2026-02-23T13:38:30.136867862Z"
    }
   },
   "source": [
    "stop_tokens = tokenizer.encode(special_tokens[\"end_of_turn\"] + special_tokens[\"eos\"], add_special_tokens=False).ids\n",
    "print(stop_tokens)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3]\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T13:38:30.324687929Z",
     "start_time": "2026-02-23T13:38:30.305390691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def chat_completion(user_text):\n",
    "    return generate(\n",
    "        model,\n",
    "        chat_template([\n",
    "            {\"role\": \"user\", \"content\": user_text}\n",
    "        ],add_generation_token=True),\n",
    "        max_new_tokens=200,\n",
    "        stop_tokens=stop_tokens,\n",
    "    )"
   ],
   "id": "176cf1aa7e65673c",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T13:38:30.399769749Z",
     "start_time": "2026-02-23T13:38:30.327921245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_dir = \"tokenized_data\"\n",
    "test_ds_name = data_dir + \"/test_chats\"\n",
    "ds_train = load_from_disk(test_ds_name).take(10).shuffle(0)\n",
    "token_ids=next(iter(ds_train))[\"tokens\"]\n",
    "tokenizer.decode(token_ids)"
   ],
   "id": "958e123b985341a9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Write one paragraph about the Tlachihualtepetl pyramid. Keep it at or below 15 sentences. Include its size and when it was built. Also, include any aspects of worship it had. Include aspects of its more recent history, too. Name at least one specific year. \\nWhen people hear the word \"pyramid,\" they often think of Egypt. However, the world\\'s largest pyramid is in Cholula, Mexico. Tlachihualtepetl, also called the Great Pyramid of Cholula, is 217 feet (66 meters) tall and its base is 1,476 by 1,1476 feet (450 by 450 meters). Its total volume is 157 million cubic feet. Due to its size, it took centuries to build. It was worked on several different times between 300 BC and 700 AD. There was a temple above it to worship Quetzalcoatl - the Aztec god of learning and books. However, the temple was abandoned around the 8th or 9th century AD. Over time, it became covered with vegetation. As a result, it blended in and Spaniards first mistook it for a hill when they conquered Mexico in 1521. They replaced the temple with a church and the pyramid stayed hidden until the 1930s when the earth started to erode. Archeologists have excavated and studied many parts of the pyramid since then, including the altars. It has also become a tourist attraction since then. Thousands of people visit Tlachihualtepetl each year. \\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "92d5c415",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T13:39:11.465471256Z",
     "start_time": "2026-02-23T13:38:30.409192119Z"
    }
   },
   "source": [
    "# test\n",
    "chat_completion(\"Write one paragraph about the Tlachihualtepetl pyramid.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|bos|>', '<|user|>', 'W', 'rite', 'Ġone', 'Ġparagraph', 'Ġabout', 'Ġthe', 'ĠT', 'l', 'ach', 'ih', 'ual', 't', 'ep', 'et', 'l', 'Ġpyramid', '.', '<|endofturn|>', 'Ċ', '<|assistant|>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Write one paragraph about the Tlachihualtepetl pyramid.\\nIf the Tlachihric What You First\\n\\nIt's real that Tlachihualtepret's natural Tlii. It's one of the most popular tlists in the world, but it's a secret one. It's a type of art that, which comes from a small type of colors that can be used by simple colors. It is very interesting to add some of the most popular Tlihualt, which is good breeds, and it's an interesting example to the tlachihualtepetl. Tlachihualtepetl is a type of art that evokes the basics of the world's art, making a feature that is truly unique and unique. It's a unique and unique style that can be used to add a touch to the artist's style. It's also a type of art that can add a touch to a touch of boldness or a touch, so make sure that it's\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T13:39:27.464519521Z",
     "start_time": "2026-02-23T13:39:12.086667703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train\n",
    "chat_completion(\"I'm trying to create a menu with different kinds of pasta. Help me come up with different types of pasta and what they are best used for.\")"
   ],
   "id": "5772c7198d4e2387",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|bos|>', '<|user|>', 'I', \"'m\", 'Ġtrying', 'Ġto', 'Ġcreate', 'Ġa', 'Ġmenu', 'Ġwith', 'Ġdifferent', 'Ġkinds', 'Ġof', 'Ġpasta', '.', 'ĠHelp', 'Ġme', 'Ġcome', 'Ġup', 'Ġwith', 'Ġdifferent', 'Ġtypes', 'Ġof', 'Ġpasta', 'Ġand', 'Ġwhat', 'Ġthey', 'Ġare', 'Ġbest', 'Ġused', 'Ġfor', '.', '<|endofturn|>', 'Ċ', '<|assistant|>']\n",
      "Reached stop token\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm trying to create a menu with different kinds of pasta. Help me come up with different types of pasta and what they are best used for.\\nHere are a list of pasta!\\n\\n1. Cat-fil-fil-fil-fil- sauce\\n2. Potato sauce\\n3. Butter-fil-fil-fil- sauce\\n4. Parmesanagine-fil-fil-fil- sauce\\n5. Cauliflower-fil-fil- sauce\\n6. Parmesan sauce\\n7. Parmesan sauce\\n8. Parmesan sauce\\n9. Parmesan sauce\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T13:40:02.733496796Z",
     "start_time": "2026-02-23T13:39:28.134747731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train\n",
    "chat_completion(\"Write me an acrostic poem about cheese.\")"
   ],
   "id": "e18d57a8385c2142",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|bos|>', '<|user|>', 'W', 'rite', 'Ġme', 'Ġan', 'Ġac', 'ro', 'stic', 'Ġpoem', 'Ġabout', 'Ġcheese', '.', '<|endofturn|>', 'Ċ', '<|assistant|>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Write me an acrostic poem about cheese.\\nFat is this kind of cheese,\\nIt is so much.\\nI love it more than cheese,\\nIt's just so much to make\\nI love it.\\nI love it more than white.\\nI love the cheese.\\nIt is so small,\\nAnd it's so very small,\\nI love it.\\nI love it more than almond milk.\\nIt's so small,\\nI love it just the cheese.\\nI love it more than white.\\nI love it more than white,\\nI love it more than white.\\nAnd it's so small,\\nI love it more than white.\\nI love it more than white.\\nI love it more than white.\\nSo love it more than white.\\nI love it more than white,\\nI love it more than white.\\nI love it with it more than white.\\nI love it more than white.\\nI love it more than white.\\nI love\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T13:40:38.466683864Z",
     "start_time": "2026-02-23T13:40:03.632944821Z"
    }
   },
   "cell_type": "code",
   "source": "chat_completion(\"When did Michael Jackson pass away?\")",
   "id": "a7dcb26d3a167594",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|bos|>', '<|user|>', 'When', 'Ġdid', 'ĠMichael', 'ĠJackson', 'Ġpass', 'Ġaway', '?', '<|endofturn|>', 'Ċ', '<|assistant|>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"When did Michael Jackson pass away?\\n Michael Jackson pass back home from his home at the University of Pennsylvania. He is the only one person on the board for the largest episodes ever. He is a professional and it was a slip-back who can be played by many of the sports. He is a extract from the area. He has a passionate passion for his ability to be and the world's most well-known hobby.\\n\\nWhen Michael Jackson passed away, he saw the state team playing against theantly many other State feelings. He is a curious farmer who is a farmer, and he is a regular farmer. He has a passion for his ability to be in the field, and he is a reference to the team. He is a good fit for his ability to be and the state team's ability to be a hero.\\n\\nWhen Michael Jackson's work with the state team, he is a0Hello. The state has a rich history and a whole number of people who have a great love for the state.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "I completed the available tutorials and built a minified GPT-2 clone.\n",
    "I have issues with the result of the instruction training.\n",
    "The resulting model can somewhat answer chat messages, but it's very inconfident.\n",
    "It often answers something unrelated, or repeats the same words.\n",
    "It can give a coherent answer only around 1 of 3 messages.\n",
    "With dropout, the validation loss converges at ~4.25 and the training loss at ~3.2\n",
    "Without dropout, the validation loss converges at ~5 and the training loss at ~0\n",
    "What causes this? Too small model? Bad training?\n",
    "\n",
    "The model:\n",
    "- context length: 1000\n",
    "- embedding dimensions: 384\n",
    "- attention heads: 12\n",
    "- transformer blocks: 12\n",
    "- FFN width multilier: 4\n",
    "- vocabulary size: 24k\n",
    "- params: ~40M\n",
    "\n",
    "Techniques:\n",
    "- weight tying\n",
    "- custom initialized weights\n",
    "- custom optimizer config with gradient decay groups\n",
    "- gradient clipping\n",
    "- cuda optimizations\n",
    "- temperature: 0.1, top_k: 10\n",
    "- the chat inference stops when reaching EOS or end_of_turn tokens\n",
    "- The chat inference adds the generation token\n",
    "\n",
    "Pre-training:\n",
    "- dropout: 0\n",
    "- training tokens: 3B (too large)\n",
    "- unique tokens: 1.5B\n",
    "- learning rate schedule: peak 6e-4, min 6e-5\n",
    "- batch size: 32\n",
    "- BOS, EOS, end_of_text tokens\n",
    "- The generated text here is pretty confident, but the repeating still appears sometimes\n",
    "\n",
    "Instruction training:\n",
    "- dropout: 0.1\n",
    "- training chats: 5 * 9500\n",
    "- unique chats: 9500\n",
    "- learning rate: fixed, 1e-5\n",
    "- batch size: 8 (smaller, because it was trained on a different GPU)\n",
    "- I checked the training data, loss masking, attention masking, special tokens\n",
    "\n",
    "\n",
    "Examples chats:\n",
    "\n",
    "- User: When did Michael Jackson pass away?\n",
    "- Assistant: Michael Jackson's first walked away from the United States on August 14, 1969. He was the second in the first place to walk away from the American States. He was the second to walk away from the country in the country and was the second to walk away from the country. He was also the second to walk away from the country in the country in the country.\n",
    "\n",
    "- User: Write me an acrostic poem about cheese.\n",
    "- Assistant: Oh, I love,\\nOh, I love,\\nOh, how can I love you?\\nOh, I love you,\\nOh, I love you,\\nOh, how can I love you?\\nOh, I love you, you love you,\\nOh, how can I love you,\\nOh, how I love you,\\nI love you, love you,\\nOh, how love you,\\nOh, how love you, how love you,\\nOh, how love you,\\nOh, how love you,\\nOh, how love you,\\nOh, how love you,\\nOh, how love you, my love,\\nOh, how love you,\\nOh, how love you,\\nOh, how love you,\\nOh, how love you,\\nOh, how love you,\\nOh, how love, I love you,\\nOh, how love you, how love you,\\nOh, how love you,\\nOh\n",
    "\n",
    "- User: I'm trying to create a menu with different kinds of pasta. Help me come up with different types of pasta and what they are best used for.\n",
    "- Assistant: Here are a few pasta that are best used for a pasta:\\n1. Cauliflower. This pasta is great for pasta and can be used to make it a pasta-together recipe.\\n2. Parmesan. This pasta is great for pasta and pasta.\\n3. Parmesan. Parmesan is a pasta-together pasta."
   ],
   "id": "5b1820a314f4c2f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T13:40:39.390798749Z",
     "start_time": "2026-02-23T13:40:39.353460937Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b812dd9b9d73001c",
   "outputs": [],
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonproject1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
