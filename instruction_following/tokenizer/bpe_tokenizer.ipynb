{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T09:58:02.437629112Z",
     "start_time": "2026-02-13T09:58:02.342580895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from typing import Iterator\n",
    "\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "login(hf_token)\n",
    "\n",
    "\n",
    "def tokenizer_lite_dataset():\n",
    "    total_length = 0\n",
    "\n",
    "    # dataset\n",
    "    no_robots_ds = load_dataset(\"HuggingFaceH4/no_robots\", streaming=True, split=\"test\").select_columns(\n",
    "        [\"messages\"])\n",
    "    total_length += no_robots_ds.dataset_size\n",
    "\n",
    "    # iterator\n",
    "    def iterator() -> Iterator[str]:\n",
    "        for row in no_robots_ds:\n",
    "            for msg in row[\"messages\"]:\n",
    "                yield msg[\"content\"]\n",
    "\n",
    "    return iterator, total_length\n",
    "\n",
    "\n",
    "def tokenizer_real_dataset():\n",
    "    total_length = 0\n",
    "\n",
    "    # no robots\n",
    "    no_robots_ds = load_dataset(\"HuggingFaceH4/no_robots\", streaming=True, split=\"test\").select_columns([\"messages\"])\n",
    "    total_length += no_robots_ds.dataset_size\n",
    "\n",
    "    # wiki\n",
    "    wiki_ds = load_dataset(\"rahular/simple-wikipedia\", streaming=True, split=\"train\").select_columns([\"text\"])\n",
    "    reduced_count = int(wiki_ds.dataset_size / 100)\n",
    "    wiki_ds = wiki_ds.take(reduced_count)\n",
    "    total_length += reduced_count\n",
    "\n",
    "    # tiny stories\n",
    "    tiny_stories_ds = load_dataset(\"roneneldan/TinyStories\", streaming=True, split=\"validation\").select_columns(\n",
    "        [\"text\"])\n",
    "    total_length += tiny_stories_ds.dataset_size\n",
    "\n",
    "    # tiny textbooks\n",
    "    tiny_textbooks_ds = load_dataset(\"nampdn-ai/tiny-textbooks\", streaming=True, split=\"test\").select_columns(\n",
    "        [\"textbook\"])\n",
    "    total_length += tiny_textbooks_ds.dataset_size\n",
    "\n",
    "    # iterator\n",
    "    def iterator() -> Iterator[str]:\n",
    "        for row in no_robots_ds:\n",
    "            for msg in row[\"messages\"]:\n",
    "                yield msg[\"content\"]\n",
    "        print(\"no robots completed\")\n",
    "        for row in wiki_ds:\n",
    "            yield row[\"text\"]\n",
    "        print(\"wiki completed\")\n",
    "        for row in tiny_stories_ds:\n",
    "            yield row[\"text\"]\n",
    "        print(\"tiny stories completed\")\n",
    "        for row in tiny_textbooks_ds:\n",
    "            yield row[\"textbook\"]\n",
    "        print(\"tiny textbooks completed\")\n",
    "\n",
    "    return iterator, total_length"
   ],
   "id": "e2bfda37e484cdbb",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T09:58:09.681058438Z",
     "start_time": "2026-02-13T09:58:02.439034936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lite_dataset = False\n",
    "dataset_iter, dataset_length = tokenizer_lite_dataset() if lite_dataset else tokenizer_real_dataset()\n",
    "\n",
    "print(dataset_length)\n",
    "example_message = next(dataset_iter())\n",
    "print(example_message)"
   ],
   "id": "eeb6b55c15ae509f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3798585513\n",
      "Aster is a chatbot who answers questions with rhymes.\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T09:58:09.692576080Z",
     "start_time": "2026-02-13T09:58:09.681956425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "\n",
    "tokenizer = Tokenizer(models.BPE(unk_token=\"<|unk|>\"))"
   ],
   "id": "b945d6a1885d0859",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T09:58:09.713148509Z",
     "start_time": "2026-02-13T09:58:09.694105422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tokenizers.normalizers import NFD, StripAccents, Lowercase, Sequence\n",
    "\n",
    "tokenizer.normalizer = Sequence([\n",
    "    NFD(),\n",
    "    StripAccents(),\n",
    "    Lowercase()\n",
    "])\n",
    "tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\")"
   ],
   "id": "21234873cfa63d4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello how are u?'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T09:58:09.729548553Z",
     "start_time": "2026-02-13T09:58:09.713971853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(example_message)"
   ],
   "id": "93d668078757e946",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Aster', (0, 5)),\n",
       " ('Ġis', (5, 8)),\n",
       " ('Ġa', (8, 10)),\n",
       " ('Ġchatbot', (10, 18)),\n",
       " ('Ġwho', (18, 22)),\n",
       " ('Ġanswers', (22, 30)),\n",
       " ('Ġquestions', (30, 40)),\n",
       " ('Ġwith', (40, 45)),\n",
       " ('Ġrhymes', (45, 52)),\n",
       " ('.', (52, 53))]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T09:58:09.746740457Z",
     "start_time": "2026-02-13T09:58:09.730462298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "special_tokens: dict[str, str] = {\n",
    "    \"end_of_text\": \"<|endoftext|>\",\n",
    "    \"end_of_turn\": \"<|endofturn|>\",\n",
    "    \"bos\": \"<|bos|>\",\n",
    "    \"eos\": \"<|eos|>\",\n",
    "    \"user\": \"<|user|>\",\n",
    "    \"assistant\": \"<|assistant|>\",\n",
    "    \"system\": \"<|system|>\",\n",
    "    \"pad\": \"<|pad|>\",\n",
    "    \"unk\": \"<|unk|>\"\n",
    "}\n",
    "\n",
    "special_token_list = list(special_tokens.values())\n",
    "tokenizer.add_tokens(special_token_list)\n",
    "tokenizer.get_vocab()"
   ],
   "id": "e672bf14aedce181",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<|assistant|>': 5,\n",
       " '<|bos|>': 2,\n",
       " '<|eos|>': 3,\n",
       " '<|user|>': 4,\n",
       " '<|endofturn|>': 1,\n",
       " '<|system|>': 6,\n",
       " '<|unk|>': 8,\n",
       " '<|endoftext|>': 0,\n",
       " '<|pad|>': 7}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T09:58:09.757062270Z",
     "start_time": "2026-02-13T09:58:09.747388687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=8000,\n",
    "    special_tokens=special_token_list,\n",
    "    show_progress=True,\n",
    "    min_frequency=2,\n",
    ")"
   ],
   "id": "bb54d1e6a69bb983",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T09:59:32.933631410Z",
     "start_time": "2026-02-13T09:58:09.757781525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer.train_from_iterator(dataset_iter(), trainer, length=dataset_length)\n",
    "file_name = \"./tokenizer.json\""
   ],
   "id": "3ae5328152083e2a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no robots completed\n",
      "wiki completed\n",
      "tiny stories completed\n",
      "tiny textbooks completed\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T09:59:32.944767436Z",
     "start_time": "2026-02-13T09:59:32.934416697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tokenizers import decoders, processors\n",
    "\n",
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
    "tokenizer.decoder = decoders.ByteLevel()"
   ],
   "id": "75f2d1c5e7f3c0fb",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T09:59:32.955801633Z",
     "start_time": "2026-02-13T09:59:32.945941030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"<|bos|> $A <|eos|>\",  # adds BOS before & EOS after each sequence\n",
    "    pair=\"<|bos|> $A <|eos|> <|bos|> $B <|eos|>\",  # for pairs (less common)\n",
    "    special_tokens=[\n",
    "        (\"<|bos|>\", tokenizer.token_to_id(\"<|bos|>\")),\n",
    "        (\"<|eos|>\", tokenizer.token_to_id(\"<|eos|>\")),\n",
    "    ],\n",
    ")"
   ],
   "id": "7478a5f2bcc078ca",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T09:59:32.970111672Z",
     "start_time": "2026-02-13T09:59:32.956580009Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.save(file_name)",
   "id": "bd39b6da528eb576",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T09:59:33.001465686Z",
     "start_time": "2026-02-13T09:59:32.971245503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"<|user|>\" in tokenizer.get_vocab())\n",
    "print(\"<|\" in tokenizer.get_vocab())"
   ],
   "id": "3d84110ddae5b69",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T09:59:33.032010338Z",
     "start_time": "2026-02-13T09:59:33.002193354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = Tokenizer.from_file(file_name)\n",
    "\n",
    "encoding = tokenizer.encode(example_message)\n",
    "print(encoding.tokens)\n",
    "decoding = tokenizer.decode(encoding.ids)\n",
    "print(decoding)"
   ],
   "id": "1f9598fc08c814dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|bos|>', 'aster', 'Ġis', 'Ġa', 'Ġch', 'at', 'b', 'ot', 'Ġwho', 'Ġanswers', 'Ġquestions', 'Ġwith', 'Ġrhy', 'mes', '.', '<|eos|>']\n",
      "aster is a chatbot who answers questions with rhymes.\n"
     ]
    }
   ],
   "execution_count": 70
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
