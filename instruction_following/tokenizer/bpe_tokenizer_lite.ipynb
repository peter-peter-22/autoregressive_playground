{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This is tokenizer training uses a smaller dataset to make local experimenting easier.",
   "id": "a37424816825670e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T12:36:48.063629447Z",
     "start_time": "2026-02-12T12:36:42.258626079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "total_length=0\n",
    "\n",
    "no_robots = load_dataset(\"HuggingFaceH4/no_robots\", streaming=True)\n",
    "no_robots_ds=no_robots[\"test\"].select_columns([\"messages\"])\n",
    "total_length+=no_robots_ds.dataset_size\n",
    "\n",
    "print(total_length)\n",
    "example_row=next(iter(no_robots_ds))\n",
    "print(example_row)"
   ],
   "id": "eeb6b55c15ae509f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17384327\n",
      "{'messages': [{'content': 'Aster is a chatbot who answers questions with rhymes.', 'role': 'system'}, {'content': 'Where did chocolate originate?', 'role': 'user'}, {'content': 'Chocolate is 4000 years old/Mexico is where it was first sold', 'role': 'assistant'}, {'content': 'Where was milk chocolate invented?', 'role': 'user'}, {'content': 'Switzerland was the first to add milk/To make their chocolate smooth as silk', 'role': 'assistant'}, {'content': 'What are some good desserts that use chocolate?', 'role': 'user'}, {'content': 'Pie, tart, cookies, and cake/Chocolate is great to bake', 'role': 'assistant'}]}\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T12:36:48.192937634Z",
     "start_time": "2026-02-12T12:36:48.151092442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "\n",
    "tokenizer = Tokenizer(models.BPE(unk_token=\"<|unk|>\"))"
   ],
   "id": "d5f1df6bae00e41f",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T12:36:48.248155588Z",
     "start_time": "2026-02-12T12:36:48.201024455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "special_tokens = [\n",
    "    \"<|endoftext|>\",       # end of text (EOT) — very common in GPT-style models\n",
    "    \"<|eot_id|>\",          # alternative name sometimes used for end of turn / end of sequence\n",
    "    \"<|bos|>\",             # beginning of sequence (BOS)\n",
    "    \"<|eos|>\",             # end of sequence (EOS) — sometimes distinct from EOT\n",
    "    \"<|user|>\",            # user marker\n",
    "    \"<|assistant|>\",       # assistant marker\n",
    "    \"<|system|>\",          # system prompt\n",
    "    \"<|pad|>\",             # padding token (PAD)\n",
    "    \"<|unk|>\",             # usually good to have\n",
    "]"
   ],
   "id": "f5e71901b86e6d0",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T12:36:48.615538288Z",
     "start_time": "2026-02-12T12:36:48.255927936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def chat_template(messages:list[dict[str,str]]):\n",
    "    result=\"\"\n",
    "    for message in messages:\n",
    "        role=message[\"role\"]\n",
    "        role_token:str\n",
    "        match role:\n",
    "            case \"user\":\n",
    "                role_token=\"<|user|>\"\n",
    "            case \"system\":\n",
    "                role_token=\"<|system|>\"\n",
    "            case \"assistant\":\n",
    "                role_token=\"<|assistant|>\"\n",
    "            case _:\n",
    "                raise ValueError(f\"Invalid message role {role}\")\n",
    "        result += role_token+message[\"content\"]+\"<|eot_id|>\\n\"\n",
    "    return result\n",
    "\n",
    "messages_text=chat_template(example_row[\"messages\"])\n",
    "print(messages_text)"
   ],
   "id": "533c4019a85b21dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>Aster is a chatbot who answers questions with rhymes.<|eot_id|>\n",
      "<|user|>Where did chocolate originate?<|eot_id|>\n",
      "<|assistant|>Chocolate is 4000 years old/Mexico is where it was first sold<|eot_id|>\n",
      "<|user|>Where was milk chocolate invented?<|eot_id|>\n",
      "<|assistant|>Switzerland was the first to add milk/To make their chocolate smooth as silk<|eot_id|>\n",
      "<|user|>What are some good desserts that use chocolate?<|eot_id|>\n",
      "<|assistant|>Pie, tart, cookies, and cake/Chocolate is great to bake<|eot_id|>\n",
      "\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T12:40:40.893784737Z",
     "start_time": "2026-02-12T12:40:40.569750057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer.add_tokens(special_tokens)\n",
    "tokenizer.add_special_tokens(special_tokens)"
   ],
   "id": "41defb8b0c3d08be",
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Unk token `<|unk|>` not found in the vocabulary",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[44], line 8\u001B[0m\n\u001B[1;32m      3\u001B[0m tokenizer\u001B[38;5;241m.\u001B[39madd_tokens([\n\u001B[1;32m      4\u001B[0m     AddedToken(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<|unk|>\u001B[39m\u001B[38;5;124m\"\u001B[39m, single_word\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, lstrip\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, rstrip\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m),\n\u001B[1;32m      5\u001B[0m     AddedToken(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<|user|>\u001B[39m\u001B[38;5;124m\"\u001B[39m, single_word\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, lstrip\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, rstrip\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m),\n\u001B[1;32m      6\u001B[0m ])\n\u001B[1;32m      7\u001B[0m tokenizer\u001B[38;5;241m.\u001B[39mpre_tokenizer \u001B[38;5;241m=\u001B[39m pre_tokenizers\u001B[38;5;241m.\u001B[39mByteLevel(add_prefix_space\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m----> 8\u001B[0m \u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessages_text\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mException\u001B[0m: Unk token `<|unk|>` not found in the vocabulary"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T12:41:27.306522319Z",
     "start_time": "2026-02-12T12:41:26.777603170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(messages_text)"
   ],
   "id": "93d668078757e946",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<|', (0, 2)),\n",
       " ('system', (2, 8)),\n",
       " ('|>', (8, 10)),\n",
       " ('Aster', (10, 15)),\n",
       " ('Ġis', (15, 18)),\n",
       " ('Ġa', (18, 20)),\n",
       " ('Ġchatbot', (20, 28)),\n",
       " ('Ġwho', (28, 32)),\n",
       " ('Ġanswers', (32, 40)),\n",
       " ('Ġquestions', (40, 50)),\n",
       " ('Ġwith', (50, 55)),\n",
       " ('Ġrhymes', (55, 62)),\n",
       " ('.<|', (62, 65)),\n",
       " ('eot', (65, 68)),\n",
       " ('_', (68, 69)),\n",
       " ('id', (69, 71)),\n",
       " ('|>', (71, 73)),\n",
       " ('Ċ', (73, 74)),\n",
       " ('<|', (74, 76)),\n",
       " ('user', (76, 80)),\n",
       " ('|>', (80, 82)),\n",
       " ('Where', (82, 87)),\n",
       " ('Ġdid', (87, 91)),\n",
       " ('Ġchocolate', (91, 101)),\n",
       " ('Ġoriginate', (101, 111)),\n",
       " ('?<|', (111, 114)),\n",
       " ('eot', (114, 117)),\n",
       " ('_', (117, 118)),\n",
       " ('id', (118, 120)),\n",
       " ('|>', (120, 122)),\n",
       " ('Ċ', (122, 123)),\n",
       " ('<|', (123, 125)),\n",
       " ('assistant', (125, 134)),\n",
       " ('|>', (134, 136)),\n",
       " ('Chocolate', (136, 145)),\n",
       " ('Ġis', (145, 148)),\n",
       " ('Ġ4000', (148, 153)),\n",
       " ('Ġyears', (153, 159)),\n",
       " ('Ġold', (159, 163)),\n",
       " ('/', (163, 164)),\n",
       " ('Mexico', (164, 170)),\n",
       " ('Ġis', (170, 173)),\n",
       " ('Ġwhere', (173, 179)),\n",
       " ('Ġit', (179, 182)),\n",
       " ('Ġwas', (182, 186)),\n",
       " ('Ġfirst', (186, 192)),\n",
       " ('Ġsold', (192, 197)),\n",
       " ('<|', (197, 199)),\n",
       " ('eot', (199, 202)),\n",
       " ('_', (202, 203)),\n",
       " ('id', (203, 205)),\n",
       " ('|>', (205, 207)),\n",
       " ('Ċ', (207, 208)),\n",
       " ('<|', (208, 210)),\n",
       " ('user', (210, 214)),\n",
       " ('|>', (214, 216)),\n",
       " ('Where', (216, 221)),\n",
       " ('Ġwas', (221, 225)),\n",
       " ('Ġmilk', (225, 230)),\n",
       " ('Ġchocolate', (230, 240)),\n",
       " ('Ġinvented', (240, 249)),\n",
       " ('?<|', (249, 252)),\n",
       " ('eot', (252, 255)),\n",
       " ('_', (255, 256)),\n",
       " ('id', (256, 258)),\n",
       " ('|>', (258, 260)),\n",
       " ('Ċ', (260, 261)),\n",
       " ('<|', (261, 263)),\n",
       " ('assistant', (263, 272)),\n",
       " ('|>', (272, 274)),\n",
       " ('Switzerland', (274, 285)),\n",
       " ('Ġwas', (285, 289)),\n",
       " ('Ġthe', (289, 293)),\n",
       " ('Ġfirst', (293, 299)),\n",
       " ('Ġto', (299, 302)),\n",
       " ('Ġadd', (302, 306)),\n",
       " ('Ġmilk', (306, 311)),\n",
       " ('/', (311, 312)),\n",
       " ('To', (312, 314)),\n",
       " ('Ġmake', (314, 319)),\n",
       " ('Ġtheir', (319, 325)),\n",
       " ('Ġchocolate', (325, 335)),\n",
       " ('Ġsmooth', (335, 342)),\n",
       " ('Ġas', (342, 345)),\n",
       " ('Ġsilk', (345, 350)),\n",
       " ('<|', (350, 352)),\n",
       " ('eot', (352, 355)),\n",
       " ('_', (355, 356)),\n",
       " ('id', (356, 358)),\n",
       " ('|>', (358, 360)),\n",
       " ('Ċ', (360, 361)),\n",
       " ('<|', (361, 363)),\n",
       " ('user', (363, 367)),\n",
       " ('|>', (367, 369)),\n",
       " ('What', (369, 373)),\n",
       " ('Ġare', (373, 377)),\n",
       " ('Ġsome', (377, 382)),\n",
       " ('Ġgood', (382, 387)),\n",
       " ('Ġdesserts', (387, 396)),\n",
       " ('Ġthat', (396, 401)),\n",
       " ('Ġuse', (401, 405)),\n",
       " ('Ġchocolate', (405, 415)),\n",
       " ('?<|', (415, 418)),\n",
       " ('eot', (418, 421)),\n",
       " ('_', (421, 422)),\n",
       " ('id', (422, 424)),\n",
       " ('|>', (424, 426)),\n",
       " ('Ċ', (426, 427)),\n",
       " ('<|', (427, 429)),\n",
       " ('assistant', (429, 438)),\n",
       " ('|>', (438, 440)),\n",
       " ('Pie', (440, 443)),\n",
       " (',', (443, 444)),\n",
       " ('Ġtart', (444, 449)),\n",
       " (',', (449, 450)),\n",
       " ('Ġcookies', (450, 458)),\n",
       " (',', (458, 459)),\n",
       " ('Ġand', (459, 463)),\n",
       " ('Ġcake', (463, 468)),\n",
       " ('/', (468, 469)),\n",
       " ('Chocolate', (469, 478)),\n",
       " ('Ġis', (478, 481)),\n",
       " ('Ġgreat', (481, 487)),\n",
       " ('Ġto', (487, 490)),\n",
       " ('Ġbake', (490, 495)),\n",
       " ('<|', (495, 497)),\n",
       " ('eot', (497, 500)),\n",
       " ('_', (500, 501)),\n",
       " ('id', (501, 503)),\n",
       " ('|>', (503, 505)),\n",
       " ('Ċ', (505, 506))]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=8000,\n",
    "    special_tokens=special_tokens,\n",
    "    show_progress=True,\n",
    "    min_frequency=2,\n",
    ")"
   ],
   "id": "bb54d1e6a69bb983",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import Iterator\n",
    "\n",
    "def plain_text_iterator() -> Iterator[str]:\n",
    "    \"\"\"\n",
    "    Streams all text messages from the dataset.\n",
    "    \"\"\"\n",
    "    for row in no_robots_ds:\n",
    "        for msg in row[\"messages\"]:\n",
    "            yield msg[\"content\"]\n",
    "\n",
    "texts=plain_text_iterator()\n",
    "print(next(texts))"
   ],
   "id": "34496481210c0b38",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenizer.train_from_iterator(texts, trainer,length=total_length)\n",
    "file_name= \"./tokenizer_lite.json\""
   ],
   "id": "3ae5328152083e2a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tokenizers import decoders, processors\n",
    "\n",
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
    "tokenizer.decoder = decoders.ByteLevel()"
   ],
   "id": "75f2d1c5e7f3c0fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"<|bos|> $A <|eos|>\",          # adds BOS before & EOS after each sequence\n",
    "    pair=\"<|bos|> $A <|eos|> <|bos|> $B <|eos|>\",  # for pairs (less common)\n",
    "    special_tokens=[\n",
    "        (\"<|bos|>\", tokenizer.token_to_id(\"<|bos|>\")),\n",
    "        (\"<|eos|>\", tokenizer.token_to_id(\"<|eos|>\")),\n",
    "    ],\n",
    ")"
   ],
   "id": "7478a5f2bcc078ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tokenizer.save(file_name)",
   "id": "bd39b6da528eb576",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenizer = Tokenizer.from_file(file_name)\n",
    "\n",
    "text = \"To be, or not to be:\\n\\nThat is \\nthe question.\"\n",
    "\n",
    "encoding = tokenizer.encode(text)\n",
    "print(encoding.tokens)\n",
    "decoding=tokenizer.decode(encoding.ids)\n",
    "print(decoding)"
   ],
   "id": "1f9598fc08c814dd",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
