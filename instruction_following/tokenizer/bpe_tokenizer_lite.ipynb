{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This is tokenizer training uses a smaller dataset to make local experimenting easier.",
   "id": "a37424816825670e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T20:09:15.955053219Z",
     "start_time": "2026-02-12T20:09:08.236018140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "total_length=0\n",
    "\n",
    "no_robots = load_dataset(\"HuggingFaceH4/no_robots\", streaming=True)\n",
    "no_robots_ds=no_robots[\"test\"].select_columns([\"messages\"])\n",
    "total_length+=no_robots_ds.dataset_size\n",
    "\n",
    "print(total_length)\n",
    "example_row=next(iter(no_robots_ds))\n",
    "print(example_row)"
   ],
   "id": "eeb6b55c15ae509f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Downloading readme: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d2bae52d88c84114a7fc31a41e73c875"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17384327\n",
      "{'messages': [{'content': 'Aster is a chatbot who answers questions with rhymes.', 'role': 'system'}, {'content': 'Where did chocolate originate?', 'role': 'user'}, {'content': 'Chocolate is 4000 years old/Mexico is where it was first sold', 'role': 'assistant'}, {'content': 'Where was milk chocolate invented?', 'role': 'user'}, {'content': 'Switzerland was the first to add milk/To make their chocolate smooth as silk', 'role': 'assistant'}, {'content': 'What are some good desserts that use chocolate?', 'role': 'user'}, {'content': 'Pie, tart, cookies, and cake/Chocolate is great to bake', 'role': 'assistant'}]}\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T20:09:15.974583452Z",
     "start_time": "2026-02-12T20:09:15.964632385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "special_tokens = [\n",
    "    \"<|endoftext|>\",       # end of text (EOT) — very common in GPT-style models\n",
    "    \"<|eot_id|>\",          # alternative name sometimes used for end of turn / end of sequence\n",
    "    \"<|bos|>\",             # beginning of sequence (BOS)\n",
    "    \"<|eos|>\",             # end of sequence (EOS) — sometimes distinct from EOT\n",
    "    \"<|user|>\",            # user marker\n",
    "    \"<|assistant|>\",       # assistant marker\n",
    "    \"<|system|>\",          # system prompt\n",
    "    \"<|pad|>\",             # padding token (PAD)\n",
    "    \"<|unk|>\",             # usually good to have\n",
    "]"
   ],
   "id": "f5e71901b86e6d0",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T20:09:16.000739534Z",
     "start_time": "2026-02-12T20:09:15.977768721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def chat_template(messages:list[dict[str,str]]):\n",
    "    result=\"\"\n",
    "    for message in messages:\n",
    "        role=message[\"role\"]\n",
    "        role_token:str\n",
    "        match role:\n",
    "            case \"user\":\n",
    "                role_token=\"<|user|>\"\n",
    "            case \"system\":\n",
    "                role_token=\"<|system|>\"\n",
    "            case \"assistant\":\n",
    "                role_token=\"<|assistant|>\"\n",
    "            case _:\n",
    "                raise ValueError(f\"Invalid message role {role}\")\n",
    "        result += role_token+message[\"content\"]+\"<|eot_id|>\\n\"\n",
    "    return result\n",
    "\n",
    "messages_text=chat_template(example_row[\"messages\"])\n",
    "print(messages_text)"
   ],
   "id": "533c4019a85b21dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>Aster is a chatbot who answers questions with rhymes.<|eot_id|>\n",
      "<|user|>Where did chocolate originate?<|eot_id|>\n",
      "<|assistant|>Chocolate is 4000 years old/Mexico is where it was first sold<|eot_id|>\n",
      "<|user|>Where was milk chocolate invented?<|eot_id|>\n",
      "<|assistant|>Switzerland was the first to add milk/To make their chocolate smooth as silk<|eot_id|>\n",
      "<|user|>What are some good desserts that use chocolate?<|eot_id|>\n",
      "<|assistant|>Pie, tart, cookies, and cake/Chocolate is great to bake<|eot_id|>\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T20:18:07.636703904Z",
     "start_time": "2026-02-12T20:18:07.627447425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "\n",
    "tokenizer = Tokenizer(models.BPE(unk_token=\"<|unk|>\"))"
   ],
   "id": "b945d6a1885d0859",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T20:18:08.709471621Z",
     "start_time": "2026-02-12T20:18:08.672979273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tokenizers.normalizers import NFD, StripAccents, Lowercase, Sequence\n",
    "\n",
    "tokenizer.normalizer = Sequence([\n",
    "        NFD(),\n",
    "        StripAccents(),\n",
    "        Lowercase()\n",
    "    ])\n",
    "tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\")"
   ],
   "id": "21234873cfa63d4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello how are u?'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T20:18:09.854110881Z",
     "start_time": "2026-02-12T20:18:09.817548988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(messages_text)"
   ],
   "id": "93d668078757e946",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<|', (0, 2)),\n",
       " ('system', (2, 8)),\n",
       " ('|>', (8, 10)),\n",
       " ('Aster', (10, 15)),\n",
       " ('Ġis', (15, 18)),\n",
       " ('Ġa', (18, 20)),\n",
       " ('Ġchatbot', (20, 28)),\n",
       " ('Ġwho', (28, 32)),\n",
       " ('Ġanswers', (32, 40)),\n",
       " ('Ġquestions', (40, 50)),\n",
       " ('Ġwith', (50, 55)),\n",
       " ('Ġrhymes', (55, 62)),\n",
       " ('.<|', (62, 65)),\n",
       " ('eot', (65, 68)),\n",
       " ('_', (68, 69)),\n",
       " ('id', (69, 71)),\n",
       " ('|>', (71, 73)),\n",
       " ('Ċ', (73, 74)),\n",
       " ('<|', (74, 76)),\n",
       " ('user', (76, 80)),\n",
       " ('|>', (80, 82)),\n",
       " ('Where', (82, 87)),\n",
       " ('Ġdid', (87, 91)),\n",
       " ('Ġchocolate', (91, 101)),\n",
       " ('Ġoriginate', (101, 111)),\n",
       " ('?<|', (111, 114)),\n",
       " ('eot', (114, 117)),\n",
       " ('_', (117, 118)),\n",
       " ('id', (118, 120)),\n",
       " ('|>', (120, 122)),\n",
       " ('Ċ', (122, 123)),\n",
       " ('<|', (123, 125)),\n",
       " ('assistant', (125, 134)),\n",
       " ('|>', (134, 136)),\n",
       " ('Chocolate', (136, 145)),\n",
       " ('Ġis', (145, 148)),\n",
       " ('Ġ4000', (148, 153)),\n",
       " ('Ġyears', (153, 159)),\n",
       " ('Ġold', (159, 163)),\n",
       " ('/', (163, 164)),\n",
       " ('Mexico', (164, 170)),\n",
       " ('Ġis', (170, 173)),\n",
       " ('Ġwhere', (173, 179)),\n",
       " ('Ġit', (179, 182)),\n",
       " ('Ġwas', (182, 186)),\n",
       " ('Ġfirst', (186, 192)),\n",
       " ('Ġsold', (192, 197)),\n",
       " ('<|', (197, 199)),\n",
       " ('eot', (199, 202)),\n",
       " ('_', (202, 203)),\n",
       " ('id', (203, 205)),\n",
       " ('|>', (205, 207)),\n",
       " ('Ċ', (207, 208)),\n",
       " ('<|', (208, 210)),\n",
       " ('user', (210, 214)),\n",
       " ('|>', (214, 216)),\n",
       " ('Where', (216, 221)),\n",
       " ('Ġwas', (221, 225)),\n",
       " ('Ġmilk', (225, 230)),\n",
       " ('Ġchocolate', (230, 240)),\n",
       " ('Ġinvented', (240, 249)),\n",
       " ('?<|', (249, 252)),\n",
       " ('eot', (252, 255)),\n",
       " ('_', (255, 256)),\n",
       " ('id', (256, 258)),\n",
       " ('|>', (258, 260)),\n",
       " ('Ċ', (260, 261)),\n",
       " ('<|', (261, 263)),\n",
       " ('assistant', (263, 272)),\n",
       " ('|>', (272, 274)),\n",
       " ('Switzerland', (274, 285)),\n",
       " ('Ġwas', (285, 289)),\n",
       " ('Ġthe', (289, 293)),\n",
       " ('Ġfirst', (293, 299)),\n",
       " ('Ġto', (299, 302)),\n",
       " ('Ġadd', (302, 306)),\n",
       " ('Ġmilk', (306, 311)),\n",
       " ('/', (311, 312)),\n",
       " ('To', (312, 314)),\n",
       " ('Ġmake', (314, 319)),\n",
       " ('Ġtheir', (319, 325)),\n",
       " ('Ġchocolate', (325, 335)),\n",
       " ('Ġsmooth', (335, 342)),\n",
       " ('Ġas', (342, 345)),\n",
       " ('Ġsilk', (345, 350)),\n",
       " ('<|', (350, 352)),\n",
       " ('eot', (352, 355)),\n",
       " ('_', (355, 356)),\n",
       " ('id', (356, 358)),\n",
       " ('|>', (358, 360)),\n",
       " ('Ċ', (360, 361)),\n",
       " ('<|', (361, 363)),\n",
       " ('user', (363, 367)),\n",
       " ('|>', (367, 369)),\n",
       " ('What', (369, 373)),\n",
       " ('Ġare', (373, 377)),\n",
       " ('Ġsome', (377, 382)),\n",
       " ('Ġgood', (382, 387)),\n",
       " ('Ġdesserts', (387, 396)),\n",
       " ('Ġthat', (396, 401)),\n",
       " ('Ġuse', (401, 405)),\n",
       " ('Ġchocolate', (405, 415)),\n",
       " ('?<|', (415, 418)),\n",
       " ('eot', (418, 421)),\n",
       " ('_', (421, 422)),\n",
       " ('id', (422, 424)),\n",
       " ('|>', (424, 426)),\n",
       " ('Ċ', (426, 427)),\n",
       " ('<|', (427, 429)),\n",
       " ('assistant', (429, 438)),\n",
       " ('|>', (438, 440)),\n",
       " ('Pie', (440, 443)),\n",
       " (',', (443, 444)),\n",
       " ('Ġtart', (444, 449)),\n",
       " (',', (449, 450)),\n",
       " ('Ġcookies', (450, 458)),\n",
       " (',', (458, 459)),\n",
       " ('Ġand', (459, 463)),\n",
       " ('Ġcake', (463, 468)),\n",
       " ('/', (468, 469)),\n",
       " ('Chocolate', (469, 478)),\n",
       " ('Ġis', (478, 481)),\n",
       " ('Ġgreat', (481, 487)),\n",
       " ('Ġto', (487, 490)),\n",
       " ('Ġbake', (490, 495)),\n",
       " ('<|', (495, 497)),\n",
       " ('eot', (497, 500)),\n",
       " ('_', (500, 501)),\n",
       " ('id', (501, 503)),\n",
       " ('|>', (503, 505)),\n",
       " ('Ċ', (505, 506))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T20:19:40.150348980Z",
     "start_time": "2026-02-12T20:19:40.119235237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer.add_tokens(special_tokens)\n",
    "tokenizer.get_vocab()"
   ],
   "id": "e672bf14aedce181",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<|eos|>': 3,\n",
       " '<|unk|>': 8,\n",
       " '<|assistant|>': 5,\n",
       " '<|system|>': 6,\n",
       " '<|eot_id|>': 1,\n",
       " '<|endoftext|>': 0,\n",
       " '<|user|>': 4,\n",
       " '<|bos|>': 2,\n",
       " '<|pad|>': 7}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T20:19:48.042062309Z",
     "start_time": "2026-02-12T20:19:48.032754406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=8000,\n",
    "    special_tokens=special_tokens,\n",
    "    show_progress=True,\n",
    "    min_frequency=2,\n",
    ")"
   ],
   "id": "bb54d1e6a69bb983",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T20:19:54.467211161Z",
     "start_time": "2026-02-12T20:19:53.355762156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Iterator\n",
    "\n",
    "def plain_text_iterator() -> Iterator[str]:\n",
    "    \"\"\"\n",
    "    Streams all text messages from the dataset.\n",
    "    \"\"\"\n",
    "    for row in no_robots_ds:\n",
    "        for msg in row[\"messages\"]:\n",
    "            yield msg[\"content\"]\n",
    "\n",
    "texts=plain_text_iterator()\n",
    "print(next(texts))"
   ],
   "id": "34496481210c0b38",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aster is a chatbot who answers questions with rhymes.\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T20:19:57.872667524Z",
     "start_time": "2026-02-12T20:19:57.593739799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer.train_from_iterator(texts, trainer,length=total_length)\n",
    "file_name= \"./tokenizer_lite.json\""
   ],
   "id": "3ae5328152083e2a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T20:19:59.669938890Z",
     "start_time": "2026-02-12T20:19:59.638689750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tokenizers import decoders, processors\n",
    "\n",
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
    "tokenizer.decoder = decoders.ByteLevel()"
   ],
   "id": "75f2d1c5e7f3c0fb",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T20:20:00.794757068Z",
     "start_time": "2026-02-12T20:20:00.778995707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"<|bos|> $A <|eos|>\",          # adds BOS before & EOS after each sequence\n",
    "    pair=\"<|bos|> $A <|eos|> <|bos|> $B <|eos|>\",  # for pairs (less common)\n",
    "    special_tokens=[\n",
    "        (\"<|bos|>\", tokenizer.token_to_id(\"<|bos|>\")),\n",
    "        (\"<|eos|>\", tokenizer.token_to_id(\"<|eos|>\")),\n",
    "    ],\n",
    ")"
   ],
   "id": "7478a5f2bcc078ca",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T20:20:02.098409226Z",
     "start_time": "2026-02-12T20:20:02.086923737Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.save(file_name)",
   "id": "bd39b6da528eb576",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T20:20:03.437676658Z",
     "start_time": "2026-02-12T20:20:03.398636587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = Tokenizer.from_file(file_name)\n",
    "\n",
    "text = \"To be, or not to be:\\n\\nThat is \\nthe question.\"\n",
    "\n",
    "encoding = tokenizer.encode(text)\n",
    "print(encoding.tokens)\n",
    "decoding=tokenizer.decode(encoding.ids)\n",
    "print(decoding)"
   ],
   "id": "1f9598fc08c814dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|bos|>', 'to', 'Ġbe', ',', 'Ġor', 'Ġnot', 'Ġto', 'Ġbe', ':', 'Ċ', 'Ċ', 'that', 'Ġis', 'Ġ', 'Ċ', 'the', 'Ġquestion', '.', '<|eos|>']\n",
      "to be, or not to be:\n",
      "\n",
      "that is \n",
      "the question.\n"
     ]
    }
   ],
   "execution_count": 29
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
