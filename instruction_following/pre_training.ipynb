{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T09:45:19.379700503Z",
     "start_time": "2026-02-18T09:45:19.291994223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from settings import ModelSettings"
   ],
   "id": "39f49c5833de57aa",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Mode settings\n",
   "id": "dbe19c1cdb7088f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T09:45:19.403230234Z",
     "start_time": "2026-02-18T09:45:19.382475537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "minified = True\n",
    "colab = False\n",
    "checkpoint: int | None = None\n",
    "compile = True"
   ],
   "id": "67cf94c976797ed4",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Paths",
   "id": "4c7e0486169f9ddf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T09:45:19.474817851Z",
     "start_time": "2026-02-18T09:45:19.424384541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if colab:\n",
    "    data_dir = \"/content/drive/MyDrive\"\n",
    "    checkpoint_dir = \"/content/drive/MyDrive/pre_checkpoints\"\n",
    "else:\n",
    "    data_dir = \"tokenized_data\"\n",
    "    checkpoint_dir = \"pre_checkpoints\"\n",
    "info_dir=checkpoint_dir+\"/info\"\n",
    "state_dir=checkpoint_dir+\"/state\""
   ],
   "id": "aeae42b7d36357bb",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "General settings",
   "id": "276f1e2e08b01b20"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T09:45:19.552365954Z",
     "start_time": "2026-02-18T09:45:19.512081903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if not minified:\n",
    "    # Training data\n",
    "    block_size = ModelSettings.max_context_length\n",
    "    batch_size = 32\n",
    "\n",
    "    # Learning\n",
    "    max_iters = 600000  # total number of training iterations\n",
    "    learning_rate = 6e-4\n",
    "    min_lr = 6e-5\n",
    "    lr_decay_steps = max_iters  # should be ~= max_iters per Chinchilla\n",
    "    warmup_steps = 2000\n",
    "    eval_iters = 200\n",
    "    eval_interval = 2000\n",
    "    grad_clip = 1.0\n",
    "else:\n",
    "    # Training data\n",
    "    block_size = 64\n",
    "    batch_size = 8\n",
    "\n",
    "    # Learning\n",
    "    max_iters = 600  # total number of training iterations\n",
    "    learning_rate = 6e-3\n",
    "    min_lr = 6e-4\n",
    "    lr_decay_steps = max_iters  # should be ~= max_iters per Chinchilla\n",
    "    warmup_steps = 60\n",
    "    eval_iters = 2\n",
    "    eval_interval = 10\n",
    "    grad_clip = 1.0"
   ],
   "id": "4d4c17ce894ef249",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Hardware settings",
   "id": "71caaf7a7a523d74"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T09:45:19.644693529Z",
     "start_time": "2026-02-18T09:45:19.553730232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
    "autocast_enabled = device_type == \"cuda\"\n",
    "print(device)"
   ],
   "id": "b4068dad6e64b371",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Training data stream",
   "id": "b2238c5052b49fca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T09:45:19.846861988Z",
     "start_time": "2026-02-18T09:45:19.722815750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_batch(split):\n",
    "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
    "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
    "    if split == 'train':\n",
    "        data = np.memmap(os.path.join(data_dir, 'train.bin' if not minified else \"test.bin\"), dtype=np.uint16, mode='r')\n",
    "    else:\n",
    "        data = np.memmap(os.path.join(data_dir, 'test.bin'), dtype=np.uint16, mode='r')\n",
    "    if not minified:\n",
    "        ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    else:\n",
    "        ix = torch.randint(5000 - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i + block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i + 1:i + 1 + block_size]).astype(np.int64)) for i in ix])\n",
    "    if device == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ],
   "id": "ea8c6c54e89cf203",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T09:45:19.872418750Z",
     "start_time": "2026-02-18T09:45:19.850263807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ],
   "id": "4c9bf0633d5a4597",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Scaler for FP16",
   "id": "8594f48b269e2a2f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T09:45:19.895021624Z",
     "start_time": "2026-02-18T09:45:19.874744953Z"
    }
   },
   "cell_type": "code",
   "source": "scaler = torch.amp.GradScaler(device_type)",
   "id": "756fe3809acc0c3c",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Model settings",
   "id": "32532e7c9d6b345"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T09:45:20.012049612Z",
     "start_time": "2026-02-18T09:45:19.897537274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from model import ChatModel\n",
    "from settings import ModelSettings\n",
    "\n",
    "if not minified:\n",
    "    model = ChatModel(\n",
    "        vocabulary_size=ModelSettings.vocabulary_size,\n",
    "        embedding_size=ModelSettings.embedding_size,\n",
    "        max_context_length=block_size,\n",
    "        ff_size_multiplier=ModelSettings.ff_size_multiplier,\n",
    "        transformer_blocks=ModelSettings.transformer_blocks,\n",
    "        attention_heads=ModelSettings.attention_heads,\n",
    "        dropout=0.0,\n",
    "        bias=ModelSettings.bias,\n",
    "        device=device,\n",
    "    )\n",
    "else:\n",
    "    model = ChatModel(\n",
    "        vocabulary_size=ModelSettings.vocabulary_size,\n",
    "        embedding_size=64,\n",
    "        max_context_length=block_size,\n",
    "        ff_size_multiplier=2,\n",
    "        transformer_blocks=4,\n",
    "        attention_heads=4,\n",
    "        dropout=0.0,\n",
    "        bias=ModelSettings.bias,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "if compile:\n",
    "    model = torch.compile(model)"
   ],
   "id": "d529ab02a58a4ac0",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Optimizer",
   "id": "dc195a20418e5df9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T09:45:20.030506463Z",
     "start_time": "2026-02-18T09:45:20.015316865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from optimizer import get_optim_groups\n",
    "\n",
    "optim_groups = get_optim_groups(model)\n",
    "\n",
    "# apply dynamic learning rate to the optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    optim_groups,\n",
    "    lr=3e-4,\n",
    "    betas=(0.9, 0.95),\n",
    "    eps=1e-8\n",
    ")"
   ],
   "id": "f8dc1093f08e8628",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Checkpointer",
   "id": "24e3028ea893d325"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T09:45:20.054186328Z",
     "start_time": "2026-02-18T09:45:20.033825410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "os.makedirs(info_dir, exist_ok=True)\n",
    "os.makedirs(state_dir, exist_ok=True)\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def save_checkpoint(\n",
    "        step,\n",
    "        model,\n",
    "        optimizer,\n",
    "        scaler,\n",
    "        train_loss,\n",
    "        val_loss,\n",
    "        learning_rate\n",
    "):\n",
    "    state = {\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"scaler\": scaler.state_dict() if scaler else None,\n",
    "    }\n",
    "\n",
    "    info = {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"time\": datetime.now().isoformat(),\n",
    "        \"block_size\": block_size,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"eval_interval\": eval_interval,\n",
    "        \"step\": step,\n",
    "        \"learning_rate\": learning_rate\n",
    "    }\n",
    "\n",
    "    torch.save(state, f\"{state_dir}/{step:05d}.pt\")\n",
    "    torch.save(info,  f\"{info_dir}/{step:05d}.pt\")"
   ],
   "id": "47f3bb6eba858215",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load training state",
   "id": "2ea15b7d65829578"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T09:45:20.113656498Z",
     "start_time": "2026-02-18T09:45:20.056375180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_checkpoint(step: int):\n",
    "    state = torch.load(f\"{state_dir}/{step:05d}.pt\")\n",
    "    model.load_state_dict(state[\"model\"])\n",
    "    optimizer.load_state_dict(state[\"optimizer\"])\n",
    "    scaler.load_state_dict(state[\"scaler\"])\n",
    "    print(f\"Loaded checkpoint {step}\")\n",
    "\n",
    "\n",
    "if checkpoint is not None:\n",
    "    load_checkpoint(checkpoint)"
   ],
   "id": "1cf313bfd2550641",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Learning scheduler",
   "id": "704cd5792d35ab75"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T09:45:20.212134622Z",
     "start_time": "2026-02-18T09:45:20.121139566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def get_lr(step):\n",
    "    # 1) linear warmup for warmup_steps steps\n",
    "    if step < warmup_steps:\n",
    "        return learning_rate * (step + 1) / (warmup_steps + 1)\n",
    "    # 2) if it > lr_decay_steps, return min learning rate\n",
    "    if step > lr_decay_steps:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (step - warmup_steps) / (lr_decay_steps - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coefficient = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coefficient ranges 0..1\n",
    "    return min_lr + coefficient * (learning_rate - min_lr)"
   ],
   "id": "15fc11a7c6b60700",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Training loop",
   "id": "6b2775c2e3339206"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T09:48:52.550388732Z",
     "start_time": "2026-02-18T09:45:20.285619932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for step in range(checkpoint or 0, max_iters):\n",
    "\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    if autocast_enabled:\n",
    "        with torch.amp.autocast(dtype=torch.float16, device_type=device_type):\n",
    "            logits, loss = model(xb, yb)\n",
    "    else:\n",
    "        logits, loss = model(xb, yb)\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.unscale_(optimizer)\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        save_checkpoint(\n",
    "            step=step,\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            scaler=scaler,\n",
    "            train_loss=losses[\"train\"],\n",
    "            val_loss=losses[\"val\"],\n",
    "            learning_rate=lr\n",
    "        )"
   ],
   "id": "198d433f585502b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 10.0898, val loss 10.0815\n",
      "step 10: train loss 9.5862, val loss 9.6375\n",
      "step 20: train loss 8.2541, val loss 8.2854\n",
      "step 30: train loss 6.7647, val loss 6.7371\n",
      "step 40: train loss 6.4303, val loss 6.4374\n",
      "step 50: train loss 6.3996, val loss 6.3657\n",
      "step 60: train loss 6.3121, val loss 6.4734\n",
      "step 70: train loss 6.1462, val loss 6.2023\n",
      "step 80: train loss 6.1210, val loss 6.1800\n",
      "step 90: train loss 5.9747, val loss 6.0395\n",
      "step 100: train loss 5.7022, val loss 5.7250\n",
      "step 110: train loss 5.7560, val loss 5.9357\n",
      "step 120: train loss 5.5485, val loss 5.6725\n",
      "step 130: train loss 5.4132, val loss 5.2164\n",
      "step 140: train loss 5.1278, val loss 5.1207\n",
      "step 150: train loss 4.8249, val loss 5.0904\n",
      "step 160: train loss 4.6409, val loss 4.8761\n",
      "step 170: train loss 4.2857, val loss 4.5573\n",
      "step 180: train loss 4.2839, val loss 4.5829\n",
      "step 190: train loss 4.0262, val loss 4.2305\n",
      "step 200: train loss 3.8650, val loss 3.8589\n",
      "step 210: train loss 3.5512, val loss 4.0321\n",
      "step 220: train loss 3.5291, val loss 3.7808\n",
      "step 230: train loss 3.2491, val loss 3.2805\n",
      "step 240: train loss 2.8841, val loss 3.1689\n",
      "step 250: train loss 2.8353, val loss 3.0092\n",
      "step 260: train loss 2.9947, val loss 2.7086\n",
      "step 270: train loss 2.6297, val loss 2.5734\n",
      "step 280: train loss 2.7930, val loss 2.2124\n",
      "step 290: train loss 2.5553, val loss 2.3964\n",
      "step 300: train loss 2.2031, val loss 2.5576\n",
      "step 310: train loss 2.0691, val loss 2.1670\n",
      "step 320: train loss 2.1900, val loss 2.2561\n",
      "step 330: train loss 1.8150, val loss 2.1025\n",
      "step 340: train loss 1.7368, val loss 1.7458\n",
      "step 350: train loss 1.6460, val loss 1.5604\n",
      "step 360: train loss 1.4682, val loss 1.6840\n",
      "step 370: train loss 1.3920, val loss 1.6097\n",
      "step 380: train loss 1.1916, val loss 1.3370\n",
      "step 390: train loss 1.3313, val loss 1.4180\n",
      "step 400: train loss 1.2491, val loss 1.1739\n",
      "step 410: train loss 1.1810, val loss 1.1986\n",
      "step 420: train loss 0.9413, val loss 0.9413\n",
      "step 430: train loss 1.1664, val loss 0.8400\n",
      "step 440: train loss 0.8891, val loss 0.9370\n",
      "step 450: train loss 0.9000, val loss 0.9789\n",
      "step 460: train loss 0.7519, val loss 0.8859\n",
      "step 470: train loss 0.6793, val loss 0.7107\n",
      "step 480: train loss 0.7605, val loss 0.7859\n",
      "step 490: train loss 0.6727, val loss 0.6019\n",
      "step 500: train loss 0.5807, val loss 0.6883\n",
      "step 510: train loss 0.6870, val loss 0.5455\n",
      "step 520: train loss 0.5407, val loss 0.5076\n",
      "step 530: train loss 0.5692, val loss 0.6295\n",
      "step 540: train loss 0.4966, val loss 0.5608\n",
      "step 550: train loss 0.6095, val loss 0.5895\n",
      "step 560: train loss 0.5339, val loss 0.5982\n",
      "step 570: train loss 0.5818, val loss 0.4864\n",
      "step 580: train loss 0.4735, val loss 0.4738\n",
      "step 590: train loss 0.4286, val loss 0.4888\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Test the model",
   "id": "92ad2907fd3edaed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T09:49:16.059058826Z",
     "start_time": "2026-02-18T09:48:52.653664853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tokenizers.tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"tokenizer.json\")\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(model, start, max_new_tokens=50):\n",
    "    model.eval()\n",
    "    idx = torch.tensor([tokenizer.encode(start).ids], device=device, dtype=torch.long)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -ModelSettings.max_context_length:]\n",
    "        logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = nn.functional.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "\n",
    "    return tokenizer.decode(idx[0].tolist())\n",
    "\n",
    "\n",
    "start_token_id = get_batch(\"test\")[0][0][0].item()\n",
    "start_text = tokenizer.decode([start_token_id])\n",
    "print(generate(model, start_text))"
   ],
   "id": "3b89a73b965f9742",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",Write several rh 1 in arts centre in the impressive roughlyican Centre is 15 called the City and 700 AD. In century AD. In temple above it has Al size, as silk\n",
      "What are 5 debts is one\n"
     ]
    }
   ],
   "execution_count": 43
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
