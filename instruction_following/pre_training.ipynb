{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T11:18:19.598833197Z",
     "start_time": "2026-02-19T11:18:17.668899319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from learning_metrics import get_grad_metrics\n",
    "from learning_metrics import get_weight_metrics\n",
    "from settings import ModelSettings"
   ],
   "id": "39f49c5833de57aa",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Mode settings\n",
   "id": "dbe19c1cdb7088f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T11:18:19.638158394Z",
     "start_time": "2026-02-19T11:18:19.620919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "minified = False\n",
    "colab = False\n",
    "checkpoint: int | None = None\n",
    "compile = True"
   ],
   "id": "67cf94c976797ed4",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Paths",
   "id": "4c7e0486169f9ddf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T11:18:19.657326216Z",
     "start_time": "2026-02-19T11:18:19.642149471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if colab:\n",
    "    data_dir = \"/content/drive/MyDrive\"\n",
    "    checkpoint_dir = \"/content/drive/MyDrive/pre_checkpoints\"\n",
    "else:\n",
    "    data_dir = \"tokenized_data\"\n",
    "    checkpoint_dir = \"pre_checkpoints\"\n",
    "info_dir = checkpoint_dir + \"/info\"\n",
    "state_dir = checkpoint_dir + \"/state\""
   ],
   "id": "aeae42b7d36357bb",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "General settings",
   "id": "276f1e2e08b01b20"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T11:18:35.970172424Z",
     "start_time": "2026-02-19T11:18:35.922707299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if not minified:\n",
    "    # Training data\n",
    "    block_size = ModelSettings.max_context_length\n",
    "    batch_size = 32\n",
    "\n",
    "    # Learning\n",
    "    max_iters = 200_000  # total number of training iterations\n",
    "    learning_rate = 6e-4\n",
    "    min_lr = 6e-5\n",
    "    lr_decay_steps = max_iters  # should be ~= max_iters per Chinchilla\n",
    "    warmup_steps = 2000\n",
    "    eval_iters = 100\n",
    "    eval_interval = 2000\n",
    "    grad_clip = 1.0\n",
    "    log_metrics_interval = 100\n",
    "    log_text=100\n",
    "else:\n",
    "    # Training data\n",
    "    block_size = 64\n",
    "    batch_size = 8\n",
    "\n",
    "    # Learning\n",
    "    max_iters = 600  # total number of training iterations\n",
    "    learning_rate = 6e-3\n",
    "    min_lr = 6e-4\n",
    "    lr_decay_steps = max_iters  # should be ~= max_iters per Chinchilla\n",
    "    warmup_steps = 60\n",
    "    eval_iters = 2\n",
    "    eval_interval = 10\n",
    "    grad_clip = 1.0\n",
    "    log_metrics_interval = 2\n",
    "    log_text=50"
   ],
   "id": "4d4c17ce894ef249",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T11:18:38.179200106Z",
     "start_time": "2026-02-19T11:18:38.065366881Z"
    }
   },
   "cell_type": "code",
   "source": "max_iters * batch_size * block_size / 1_500_000_000",
   "id": "51b16ef728a43388",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1845333333333334"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Hardware settings",
   "id": "71caaf7a7a523d74"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T16:26:17.295981409Z",
     "start_time": "2026-02-18T16:26:17.240921771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
    "autocast_enabled = device_type == \"cuda\"\n",
    "print(device)"
   ],
   "id": "b4068dad6e64b371",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Training data stream",
   "id": "b2238c5052b49fca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T16:26:17.325069681Z",
     "start_time": "2026-02-18T16:26:17.305103302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_batch(split):\n",
    "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
    "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
    "    if split == 'train':\n",
    "        data = np.memmap(os.path.join(data_dir, 'train.bin' if not minified else \"test.bin\"), dtype=np.uint16, mode='r')\n",
    "    else:\n",
    "        data = np.memmap(os.path.join(data_dir, 'test.bin'), dtype=np.uint16, mode='r')\n",
    "    if not minified:\n",
    "        ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    else:\n",
    "        ix = torch.randint(5000 - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i + block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i + 1:i + 1 + block_size]).astype(np.int64)) for i in ix])\n",
    "    if device == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ],
   "id": "ea8c6c54e89cf203",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T16:26:17.344676189Z",
     "start_time": "2026-02-18T16:26:17.329107199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ],
   "id": "4c9bf0633d5a4597",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Scaler for FP16",
   "id": "8594f48b269e2a2f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T16:26:17.383780507Z",
     "start_time": "2026-02-18T16:26:17.348888045Z"
    }
   },
   "cell_type": "code",
   "source": "scaler = torch.amp.GradScaler(device_type)",
   "id": "756fe3809acc0c3c",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Model settings",
   "id": "32532e7c9d6b345"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T16:26:24.202382100Z",
     "start_time": "2026-02-18T16:26:17.390285030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from model import ChatModel\n",
    "from settings import ModelSettings\n",
    "\n",
    "if not minified:\n",
    "    model = ChatModel(\n",
    "        vocabulary_size=ModelSettings.vocabulary_size,\n",
    "        embedding_size=ModelSettings.embedding_size,\n",
    "        max_context_length=block_size,\n",
    "        ff_size_multiplier=ModelSettings.ff_size_multiplier,\n",
    "        transformer_blocks=ModelSettings.transformer_blocks,\n",
    "        attention_heads=ModelSettings.attention_heads,\n",
    "        dropout=0.0,\n",
    "        bias=ModelSettings.bias,\n",
    "        device=device,\n",
    "    )\n",
    "else:\n",
    "    model = ChatModel(\n",
    "        vocabulary_size=ModelSettings.vocabulary_size,\n",
    "        embedding_size=64,\n",
    "        max_context_length=block_size,\n",
    "        ff_size_multiplier=2,\n",
    "        transformer_blocks=4,\n",
    "        attention_heads=4,\n",
    "        dropout=0.0,\n",
    "        bias=ModelSettings.bias,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "if compile:\n",
    "    model = torch.compile(model)"
   ],
   "id": "d529ab02a58a4ac0",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Optimizer",
   "id": "dc195a20418e5df9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T16:26:24.259032205Z",
     "start_time": "2026-02-18T16:26:24.239224801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from optimizer import get_optim_groups\n",
    "\n",
    "optim_groups = get_optim_groups(model)\n",
    "\n",
    "# apply dynamic learning rate to the optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    optim_groups,\n",
    "    lr=3e-4,\n",
    "    betas=(0.9, 0.95),\n",
    "    eps=1e-8\n",
    ")"
   ],
   "id": "f8dc1093f08e8628",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Generate",
   "id": "3dc19bb659d08265"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T16:26:24.402305875Z",
     "start_time": "2026-02-18T16:26:24.261547278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tokenizers.tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"tokenizer.json\")\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(model, start, max_new_tokens=50):\n",
    "    idx = torch.tensor([tokenizer.encode(start).ids], device=device, dtype=torch.long)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -ModelSettings.max_context_length:]\n",
    "        logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = nn.functional.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "\n",
    "    return tokenizer.decode(idx[0].tolist())"
   ],
   "id": "fa87a85c5d97f75f",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Checkpointer",
   "id": "24e3028ea893d325"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T16:26:24.432061517Z",
     "start_time": "2026-02-18T16:26:24.412437310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "os.makedirs(info_dir, exist_ok=True)\n",
    "os.makedirs(state_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_checkpoint(\n",
    "        step,\n",
    "        model,\n",
    "        optimizer,\n",
    "        scaler,\n",
    "        train_loss,\n",
    "        val_loss,\n",
    "        learning_rate,\n",
    "        metric_logs\n",
    "):\n",
    "    state = {\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"scaler\": scaler.state_dict() if scaler else None,\n",
    "    }\n",
    "\n",
    "    info = {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"time\": datetime.now().isoformat(),\n",
    "        \"block_size\": block_size,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"eval_interval\": eval_interval,\n",
    "        \"step\": step,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"text\": generate(model, \"Write\", log_text),\n",
    "        \"metrics\": json.dumps(metric_logs)\n",
    "    }\n",
    "\n",
    "    state_path = f\"{state_dir}/{step:05d}.pt\"\n",
    "    info_path = f\"{info_dir}/{step:05d}.pt\"\n",
    "\n",
    "    torch.save(state, state_path)\n",
    "    torch.save(info, info_path)\n",
    "\n",
    "    return state_path,info_path"
   ],
   "id": "47f3bb6eba858215",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load training state",
   "id": "2ea15b7d65829578"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T16:26:24.470561612Z",
     "start_time": "2026-02-18T16:26:24.435897374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_checkpoint(step: int):\n",
    "    state = torch.load(f\"{state_dir}/{step:05d}.pt\")\n",
    "    model.load_state_dict(state[\"model\"])\n",
    "    optimizer.load_state_dict(state[\"optimizer\"])\n",
    "    scaler.load_state_dict(state[\"scaler\"])\n",
    "    print(f\"Loaded checkpoint {step}\")\n",
    "\n",
    "\n",
    "if checkpoint is not None:\n",
    "    load_checkpoint(checkpoint)"
   ],
   "id": "1cf313bfd2550641",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Learning scheduler",
   "id": "704cd5792d35ab75"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T16:26:24.491044039Z",
     "start_time": "2026-02-18T16:26:24.474836351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def get_lr(step):\n",
    "    # 1) linear warmup for warmup_steps steps\n",
    "    if step < warmup_steps:\n",
    "        return learning_rate * (step + 1) / (warmup_steps + 1)\n",
    "    # 2) if it > lr_decay_steps, return min learning rate\n",
    "    if step > lr_decay_steps:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (step - warmup_steps) / (lr_decay_steps - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coefficient = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coefficient ranges 0..1\n",
    "    return min_lr + coefficient * (learning_rate - min_lr)"
   ],
   "id": "15fc11a7c6b60700",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Clean up old checkpoints",
   "id": "81f9caa1faa59dcd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T16:26:24.514304578Z",
     "start_time": "2026-02-18T16:26:24.494841795Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from checkpoint_cleaner import CheckpointCleaner\n",
    "\n",
    "checkpoint_cleaner=CheckpointCleaner(3)"
   ],
   "id": "39f4318ddec2db9b",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Training loop",
   "id": "6b2775c2e3339206"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T16:26:52.634395231Z",
     "start_time": "2026-02-18T16:26:24.517332680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from system_metrics import get_system_metrics\n",
    "\n",
    "metric_logs = []\n",
    "\n",
    "for step in range(checkpoint or 0, max_iters):\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    if autocast_enabled:\n",
    "        with torch.amp.autocast(dtype=torch.float16, device_type=device_type):\n",
    "            logits, loss = model(xb, yb)\n",
    "    else:\n",
    "        logits, loss = model(xb, yb)\n",
    "\n",
    "    # exit if the loss is invalid\n",
    "    if not torch.isfinite(loss):\n",
    "        raise Exception(\"Non-finite loss detected.\")\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.unscale_(optimizer)\n",
    "\n",
    "    if step % log_metrics_interval == 0:\n",
    "        total_norm, max_grad = get_grad_metrics(model)\n",
    "        max_weight, total_weight_norm = get_weight_metrics(model)\n",
    "        metric_logs.append({\n",
    "            \"gradient\": {\n",
    "                \"total_norm\": total_norm,\n",
    "                \"max_grad\": max_grad,\n",
    "            },\n",
    "            \"weight\": {\n",
    "                \"max_weight\": max_weight,\n",
    "                \"total_weight_norm\": total_weight_norm,\n",
    "            },\n",
    "            \"system\": get_system_metrics(),\n",
    "            \"current_loss\": loss.item()\n",
    "        })\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        state_path,info_path= save_checkpoint(\n",
    "            step=step,\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            scaler=scaler,\n",
    "            train_loss=losses[\"train\"],\n",
    "            val_loss=losses[\"val\"],\n",
    "            learning_rate=lr,\n",
    "            metric_logs=metric_logs\n",
    "        )\n",
    "        checkpoint_cleaner.step(state_path)\n",
    "        metric_logs = []\n"
   ],
   "id": "198d433f585502b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU usage: 55.0%\n",
      "RAM usage: 0.52 GB\n",
      "step 0: train loss 10.0899, val loss 10.0847\n",
      "CPU usage: 43.9%\n",
      "RAM usage: 0.56 GB\n",
      "CPU usage: 73.4%\n",
      "RAM usage: 0.56 GB\n",
      "CPU usage: 96.9%\n",
      "RAM usage: 0.56 GB\n",
      "CPU usage: 88.8%\n",
      "RAM usage: 0.56 GB\n",
      "CPU usage: 90.8%\n",
      "RAM usage: 0.56 GB\n",
      "step 10: train loss 9.6420, val loss 9.6387\n",
      "CPU usage: 89.3%\n",
      "RAM usage: 0.56 GB\n",
      "CPU usage: 87.6%\n",
      "RAM usage: 0.56 GB\n",
      "CPU usage: 81.4%\n",
      "RAM usage: 0.56 GB\n",
      "CPU usage: 75.9%\n",
      "RAM usage: 0.56 GB\n",
      "CPU usage: 92.3%\n",
      "RAM usage: 0.56 GB\n",
      "step 20: train loss 8.3401, val loss 8.2792\n",
      "CPU usage: 79.9%\n",
      "RAM usage: 0.56 GB\n",
      "CPU usage: 90.1%\n",
      "RAM usage: 0.56 GB\n",
      "CPU usage: 85.2%\n",
      "RAM usage: 0.56 GB\n",
      "CPU usage: 81.1%\n",
      "RAM usage: 0.56 GB\n",
      "CPU usage: 89.1%\n",
      "RAM usage: 0.56 GB\n",
      "step 30: train loss 6.7502, val loss 6.9467\n",
      "CPU usage: 78.5%\n",
      "RAM usage: 0.56 GB\n",
      "CPU usage: 92.5%\n",
      "RAM usage: 0.56 GB\n",
      "CPU usage: 89.5%\n",
      "RAM usage: 0.56 GB\n",
      "CPU usage: 94.1%\n",
      "RAM usage: 0.56 GB\n",
      "CPU usage: 88.6%\n",
      "RAM usage: 0.56 GB\n",
      "step 40: train loss 6.6174, val loss 6.6792\n",
      "CPU usage: 82.7%\n",
      "RAM usage: 0.56 GB\n",
      "CPU usage: 83.1%\n",
      "RAM usage: 0.56 GB\n",
      "CPU usage: 88.1%\n",
      "RAM usage: 0.56 GB\n",
      "CPU usage: 83.9%\n",
      "RAM usage: 0.56 GB\n",
      "CPU usage: 87.1%\n",
      "RAM usage: 0.56 GB\n",
      "step 50: train loss 6.3866, val loss 6.3340\n",
      "Removed checkpoint pre_checkpoints/state/00000.pt\n",
      "CPU usage: 73.9%\n",
      "RAM usage: 0.56 GB\n",
      "CPU usage: 82.4%\n",
      "RAM usage: 0.56 GB\n",
      "CPU usage: 79.0%\n",
      "RAM usage: 0.56 GB\n",
      "CPU usage: 83.5%\n",
      "RAM usage: 0.56 GB\n",
      "CPU usage: 92.3%\n",
      "RAM usage: 0.56 GB\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 49\u001B[0m\n\u001B[1;32m     46\u001B[0m scaler\u001B[38;5;241m.\u001B[39mupdate()\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m step \u001B[38;5;241m%\u001B[39m eval_interval \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m---> 49\u001B[0m     losses \u001B[38;5;241m=\u001B[39m \u001B[43mestimate_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstep \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: train loss \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlosses[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, val loss \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlosses[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     51\u001B[0m     state_path,info_path\u001B[38;5;241m=\u001B[39m save_checkpoint(\n\u001B[1;32m     52\u001B[0m         step\u001B[38;5;241m=\u001B[39mstep,\n\u001B[1;32m     53\u001B[0m         model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     59\u001B[0m         metric_logs\u001B[38;5;241m=\u001B[39mmetric_logs\n\u001B[1;32m     60\u001B[0m     )\n",
      "File \u001B[0;32m~/PycharmProjects/autoregressive_playground/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:124\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    120\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    121\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    122\u001B[0m     \u001B[38;5;66;03m# pyrefly: ignore [bad-context-manager]\u001B[39;00m\n\u001B[1;32m    123\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 124\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[7], line 9\u001B[0m, in \u001B[0;36mestimate_loss\u001B[0;34m()\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(eval_iters):\n\u001B[1;32m      8\u001B[0m     X, Y \u001B[38;5;241m=\u001B[39m get_batch(split)\n\u001B[0;32m----> 9\u001B[0m     logits, loss \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m     losses[k] \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m     11\u001B[0m out[split] \u001B[38;5;241m=\u001B[39m losses\u001B[38;5;241m.\u001B[39mmean()\n",
      "File \u001B[0;32m~/PycharmProjects/autoregressive_playground/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:465\u001B[0m, in \u001B[0;36mOptimizedModule.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    455\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    456\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing `torch.compile(module)` when there are global hooks on \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    457\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodules (e.g., from `register_module_forward_hook`); this will\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    462\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[1;32m    463\u001B[0m     )\n\u001B[1;32m    464\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _set_in_optimized_module():\n\u001B[0;32m--> 465\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/autoregressive_playground/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1776\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1774\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1775\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1776\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/autoregressive_playground/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1787\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1782\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1783\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1784\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1785\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1786\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1787\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1789\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1790\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/PycharmProjects/autoregressive_playground/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:953\u001B[0m, in \u001B[0;36m_TorchDynamoContext.__call__.<locals>.compile_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    950\u001B[0m _maybe_set_eval_frame(_callback_from_stance(callback))\n\u001B[1;32m    952\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 953\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    954\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m Unsupported \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    955\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m config\u001B[38;5;241m.\u001B[39mverbose:\n",
      "File \u001B[0;32m~/PycharmProjects/autoregressive_playground/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1776\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1774\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1775\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1776\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/autoregressive_playground/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1787\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1782\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1783\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1784\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1785\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1786\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1787\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1789\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1790\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/PycharmProjects/autoregressive_playground/instruction_following/model.py:205\u001B[0m, in \u001B[0;36mChatModel.forward\u001B[0;34m(self, x, targets)\u001B[0m\n\u001B[1;32m    201\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39memb\u001B[38;5;241m.\u001B[39mtoken_emb\u001B[38;5;241m.\u001B[39mweight \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhead\u001B[38;5;241m.\u001B[39mweight\n\u001B[1;32m    203\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply(_init_weights)\n\u001B[0;32m--> 205\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, targets\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    206\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39memb(x)\n\u001B[1;32m    207\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(x)\n",
      "File \u001B[0;32m~/PycharmProjects/autoregressive_playground/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1181\u001B[0m, in \u001B[0;36mDisableContext.__call__.<locals>._fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m   1171\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m fx_traceback\u001B[38;5;241m.\u001B[39mannotate(\n\u001B[1;32m   1172\u001B[0m             {\n\u001B[1;32m   1173\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_torchdynamo_disable\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1178\u001B[0m             }\n\u001B[1;32m   1179\u001B[0m         ):\n\u001B[1;32m   1180\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m-> 1181\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1182\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1183\u001B[0m     set_eval_frame(\u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "File \u001B[0;32m~/PycharmProjects/autoregressive_playground/.venv/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1148\u001B[0m, in \u001B[0;36maot_module_simplified.<locals>.forward\u001B[0;34m(*runtime_args)\u001B[0m\n\u001B[1;32m   1146\u001B[0m full_args\u001B[38;5;241m.\u001B[39mextend(params_buffers_flat)\n\u001B[1;32m   1147\u001B[0m full_args\u001B[38;5;241m.\u001B[39mextend(runtime_args)\n\u001B[0;32m-> 1148\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcompiled_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfull_args\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/autoregressive_playground/.venv/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:1962\u001B[0m, in \u001B[0;36mSerializableCompiledFunction.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1961\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m-> 1962\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompiled_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/autoregressive_playground/.venv/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:357\u001B[0m, in \u001B[0;36m_create_runtime_wrapper.<locals>.runtime_wrapper\u001B[0;34m(args)\u001B[0m\n\u001B[1;32m    355\u001B[0m         torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_set_grad_enabled(\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    356\u001B[0m     record_runtime_wrapper_prologue_exit(cm)\n\u001B[0;32m--> 357\u001B[0m     all_outs \u001B[38;5;241m=\u001B[39m \u001B[43mcall_func_at_runtime_with_args\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    358\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcompiled_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdisable_amp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisable_amp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msteal_args\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[1;32m    359\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    360\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    361\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m grad_enabled:\n",
      "File \u001B[0;32m~/PycharmProjects/autoregressive_playground/.venv/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py:134\u001B[0m, in \u001B[0;36mcall_func_at_runtime_with_args\u001B[0;34m(f, args, steal_args, disable_amp)\u001B[0m\n\u001B[1;32m    132\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[1;32m    133\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(f, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_boxed_call\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m--> 134\u001B[0m         out \u001B[38;5;241m=\u001B[39m normalize_as_list(\u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    135\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    136\u001B[0m         \u001B[38;5;66;03m# TODO: Please remove soon\u001B[39;00m\n\u001B[1;32m    137\u001B[0m         \u001B[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001B[39;00m\n\u001B[1;32m    138\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    139\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYour compiler for AOTAutograd is returning a function that doesn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt take boxed arguments. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    140\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    141\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    142\u001B[0m             stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[1;32m    143\u001B[0m         )\n",
      "File \u001B[0;32m~/PycharmProjects/autoregressive_playground/.venv/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:531\u001B[0m, in \u001B[0;36mFunctionalizedRngRuntimeWrapper.post_compile.<locals>.wrapper\u001B[0;34m(runtime_args)\u001B[0m\n\u001B[1;32m    524\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_functionalized_rng_runtime_epilogue(\n\u001B[1;32m    525\u001B[0m         runtime_metadata,\n\u001B[1;32m    526\u001B[0m         out,\n\u001B[1;32m    527\u001B[0m         \u001B[38;5;66;03m# TODO: this won't be right for the backward when we convert the call_compiled_backward to use the wrapper\u001B[39;00m\n\u001B[1;32m    528\u001B[0m         runtime_metadata\u001B[38;5;241m.\u001B[39mnum_forward_returns,\n\u001B[1;32m    529\u001B[0m     )\n\u001B[1;32m    530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m out\n\u001B[0;32m--> 531\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcompiled_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mruntime_args\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/autoregressive_playground/.venv/lib/python3.10/site-packages/torch/_inductor/output_code.py:638\u001B[0m, in \u001B[0;36mCompiledFxGraph.__call__\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m    636\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcurrent_callable(inputs)\n\u001B[1;32m    637\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 638\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcurrent_callable\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    639\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    640\u001B[0m     get_runtime_metrics_context()\u001B[38;5;241m.\u001B[39mfinish()\n",
      "File \u001B[0;32m/tmp/torchinductor_peter/ra/craeguywz3mlfyaxwryxmeflsu5ivec3a4y3tsqm464iywpe6j3z.py:1548\u001B[0m, in \u001B[0;36mRunner.call\u001B[0;34m(self, args)\u001B[0m\n\u001B[1;32m   1546\u001B[0m buf98 \u001B[38;5;241m=\u001B[39m empty_strided_cpu((\u001B[38;5;241m512\u001B[39m, \u001B[38;5;241m24000\u001B[39m), (\u001B[38;5;241m24000\u001B[39m, \u001B[38;5;241m1\u001B[39m), torch\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[1;32m   1547\u001B[0m \u001B[38;5;66;03m# Topologically Sorted Source Nodes: [input_26, x_8, input_31, x_9, x_10, logits], Original ATen: [aten.view, aten.add, aten.native_layer_norm, aten.t, aten.mm]\u001B[39;00m\n\u001B[0;32m-> 1548\u001B[0m \u001B[43mextern_kernels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreinterpret_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbuf97\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m512\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m64\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m64\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreinterpret_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43marg1_1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m64\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m24000\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m64\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbuf98\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1549\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m arg1_1\n\u001B[1;32m   1550\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m buf97\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Test the model",
   "id": "1173b07565d315a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "start_token_id = get_batch(\"test\")[0][0][0].item()\n",
    "start_text = tokenizer.decode([start_token_id])\n",
    "print(generate(model, start_text))"
   ],
   "id": "3b89a73b965f9742",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
