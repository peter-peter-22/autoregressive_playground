{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39f49c5833de57aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T07:55:49.222224405Z",
     "start_time": "2026-02-23T07:55:46.615569971Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from learning_metrics import get_grad_metrics\n",
    "from learning_metrics import get_weight_metrics\n",
    "from settings import ModelSettings\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe19c1cdb7088f1",
   "metadata": {},
   "source": [
    "Mode settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67cf94c976797ed4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T07:55:49.278710780Z",
     "start_time": "2026-02-23T07:55:49.258991320Z"
    }
   },
   "outputs": [],
   "source": [
    "minified = True\n",
    "colab = False\n",
    "thunder = False\n",
    "checkpoint: int | None = None\n",
    "compile = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7e0486169f9ddf",
   "metadata": {},
   "source": [
    "Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeae42b7d36357bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T07:55:49.302472380Z",
     "start_time": "2026-02-23T07:55:49.283718480Z"
    }
   },
   "outputs": [],
   "source": [
    "if colab:\n",
    "    data_dir = \"/content/drive/MyDrive\"\n",
    "    checkpoint_dir = \"/content/drive/MyDrive/pre_checkpoints\"\n",
    "elif thunder:\n",
    "    os.makedirs(\"output/pre_checkpoints\", exist_ok=True)\n",
    "    if not os.path.exists(\"tokenized_data/train.bin\"):\n",
    "        gdown.download(id=\"15t3259RbsF772b35aaZGouGwQopFAX96\",output=\"tokenized_data/train.bin\")\n",
    "    if not os.path.exists(\"tokenized_data/test.bin\"):\n",
    "        gdown.download(id=\"1rE_MOBhBPQGUuhYmevNZOFj-LMBWkLFD\",output=\"tokenized_data/test.bin\")\n",
    "    data_dir = \"tokenized_data\"\n",
    "    checkpoint_dir = \"output/pre_checkpoints\"\n",
    "else:\n",
    "    data_dir = \"tokenized_data\"\n",
    "    checkpoint_dir = \"pre_checkpoints\"\n",
    "info_dir = checkpoint_dir + \"/info\"\n",
    "state_dir = checkpoint_dir + \"/state\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276f1e2e08b01b20",
   "metadata": {},
   "source": [
    "General settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d4c17ce894ef249",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T07:55:49.335838681Z",
     "start_time": "2026-02-23T07:55:49.307030841Z"
    }
   },
   "outputs": [],
   "source": [
    "if not minified:\n",
    "    # Training data\n",
    "    block_size = ModelSettings.max_context_length\n",
    "    batch_size = 32\n",
    "\n",
    "    # Learning\n",
    "    max_iters = 200_000  # too big, had to exit at 100k\n",
    "    learning_rate = 6e-4\n",
    "    min_lr = 6e-5\n",
    "    lr_decay_steps = max_iters  # should be ~= max_iters per Chinchilla\n",
    "    warmup_steps = 4000\n",
    "    eval_iters = 100\n",
    "    eval_interval = 2000\n",
    "    grad_clip = 1.0\n",
    "    log_metrics_interval = 100\n",
    "    log_text=100\n",
    "else:\n",
    "    # Training data\n",
    "    block_size = 64\n",
    "    batch_size = 8\n",
    "\n",
    "    # Learning\n",
    "    max_iters = 1000\n",
    "    learning_rate = 6e-3\n",
    "    min_lr = 6e-4\n",
    "    lr_decay_steps = max_iters  # should be ~= max_iters per Chinchilla\n",
    "    warmup_steps = 60\n",
    "    eval_iters = 2\n",
    "    eval_interval = 10\n",
    "    grad_clip = 1.0\n",
    "    log_metrics_interval = 2\n",
    "    log_text=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51b16ef728a43388",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T07:55:49.556126977Z",
     "start_time": "2026-02-23T07:55:49.419659373Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00034133333333333335"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_iters * batch_size * block_size / 1_500_000_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71caaf7a7a523d74",
   "metadata": {},
   "source": [
    "Hardware settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4068dad6e64b371",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T07:55:49.638539240Z",
     "start_time": "2026-02-23T07:55:49.559208088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
    "autocast_enabled = device_type == \"cuda\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2238c5052b49fca",
   "metadata": {},
   "source": [
    "Training data stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea8c6c54e89cf203",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T07:55:49.714076786Z",
     "start_time": "2026-02-23T07:55:49.686030612Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
    "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
    "    if split == 'train':\n",
    "        data = np.memmap(os.path.join(data_dir, 'train.bin' if not minified else \"test.bin\"), dtype=np.uint16, mode='r')\n",
    "    else:\n",
    "        data = np.memmap(os.path.join(data_dir, 'test.bin'), dtype=np.uint16, mode='r')\n",
    "    if not minified:\n",
    "        ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    else:\n",
    "        ix = torch.randint(50000 - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i + block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i + 1:i + 1 + block_size]).astype(np.int64)) for i in ix])\n",
    "    if device == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c9bf0633d5a4597",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T07:55:49.749775069Z",
     "start_time": "2026-02-23T07:55:49.717799842Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8594f48b269e2a2f",
   "metadata": {},
   "source": [
    "Scaler for FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "756fe3809acc0c3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T07:55:49.804070975Z",
     "start_time": "2026-02-23T07:55:49.786486082Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = torch.amp.GradScaler(device_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32532e7c9d6b345",
   "metadata": {},
   "source": [
    "Model settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d529ab02a58a4ac0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T07:55:53.561951218Z",
     "start_time": "2026-02-23T07:55:49.806620144Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using flash attention\n",
      "using flash attention\n",
      "using flash attention\n",
      "using flash attention\n"
     ]
    }
   ],
   "source": [
    "from model import ChatModel\n",
    "from settings import ModelSettings\n",
    "\n",
    "if not minified:\n",
    "    model = ChatModel(\n",
    "        vocabulary_size=ModelSettings.vocabulary_size,\n",
    "        embedding_size=ModelSettings.embedding_size,\n",
    "        max_context_length=block_size,\n",
    "        ff_size_multiplier=ModelSettings.ff_size_multiplier,\n",
    "        transformer_blocks=ModelSettings.transformer_blocks,\n",
    "        attention_heads=ModelSettings.attention_heads,\n",
    "        dropout=0.0,\n",
    "        bias=ModelSettings.bias,\n",
    "        device=device,\n",
    "    )\n",
    "else:\n",
    "    model = ChatModel(\n",
    "        vocabulary_size=ModelSettings.vocabulary_size,\n",
    "        embedding_size=64,\n",
    "        max_context_length=block_size,\n",
    "        ff_size_multiplier=2,\n",
    "        transformer_blocks=4,\n",
    "        attention_heads=4,\n",
    "        dropout=0.0,\n",
    "        bias=ModelSettings.bias,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "if compile:\n",
    "    model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc195a20418e5df9",
   "metadata": {},
   "source": [
    "Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8dc1093f08e8628",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T07:55:53.709920754Z",
     "start_time": "2026-02-23T07:55:53.625671224Z"
    }
   },
   "outputs": [],
   "source": [
    "from optimizer import get_optim_groups\n",
    "\n",
    "optim_groups = get_optim_groups(model)\n",
    "\n",
    "# apply dynamic learning rate to the optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    optim_groups,\n",
    "    lr=3e-4,\n",
    "    betas=(0.9, 0.95),\n",
    "    eps=1e-8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc19bb659d08265",
   "metadata": {},
   "source": [
    "Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa87a85c5d97f75f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T07:55:53.885548724Z",
     "start_time": "2026-02-23T07:55:53.725810712Z"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers.tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"tokenizer.json\")\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(model, start, max_new_tokens=50):\n",
    "    idx = torch.tensor([tokenizer.encode(start,add_special_tokens=False).ids], device=device, dtype=torch.long)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -ModelSettings.max_context_length:]\n",
    "        logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = nn.functional.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "\n",
    "    return tokenizer.decode(idx[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e3028ea893d325",
   "metadata": {},
   "source": [
    "Checkpointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47f3bb6eba858215",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T07:55:53.948628076Z",
     "start_time": "2026-02-23T07:55:53.924045921Z"
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs(info_dir, exist_ok=True)\n",
    "os.makedirs(state_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_checkpoint(\n",
    "        step,\n",
    "        model,\n",
    "        optimizer,\n",
    "        scaler,\n",
    "        train_loss,\n",
    "        val_loss,\n",
    "        learning_rate,\n",
    "        metric_logs\n",
    "):\n",
    "    state = {\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"scaler\": scaler.state_dict() if scaler else None,\n",
    "    }\n",
    "\n",
    "    info = {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"time\": datetime.now().isoformat(),\n",
    "        \"block_size\": block_size,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"eval_interval\": eval_interval,\n",
    "        \"step\": step,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"text\": generate(model, \"Once upon a time\", log_text),\n",
    "        \"metrics\": json.dumps(metric_logs)\n",
    "    }\n",
    "\n",
    "    state_path = f\"{state_dir}/{step:05d}.pt\"\n",
    "    info_path = f\"{info_dir}/{step:05d}.pt\"\n",
    "\n",
    "    torch.save(state, state_path)\n",
    "    torch.save(info, info_path)\n",
    "\n",
    "    return state_path,info_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea15b7d65829578",
   "metadata": {},
   "source": [
    "Load training state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cf313bfd2550641",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T07:55:54.059109468Z",
     "start_time": "2026-02-23T07:55:53.953157441Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_checkpoint(step: int):\n",
    "    state = torch.load(f\"{state_dir}/{step:05d}.pt\")\n",
    "    model.load_state_dict(state[\"model\"])\n",
    "    optimizer.load_state_dict(state[\"optimizer\"])\n",
    "    scaler.load_state_dict(state[\"scaler\"])\n",
    "    print(f\"Loaded checkpoint {step}\")\n",
    "\n",
    "\n",
    "if checkpoint is not None:\n",
    "    load_checkpoint(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704cd5792d35ab75",
   "metadata": {},
   "source": [
    "Learning scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15fc11a7c6b60700",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T07:55:54.109525977Z",
     "start_time": "2026-02-23T07:55:54.088065239Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def get_lr(step):\n",
    "    # 1) linear warmup for warmup_steps steps\n",
    "    if step < warmup_steps:\n",
    "        return learning_rate * (step + 1) / (warmup_steps + 1)\n",
    "    # 2) if it > lr_decay_steps, return min learning rate\n",
    "    if step > lr_decay_steps:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (step - warmup_steps) / (lr_decay_steps - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coefficient = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coefficient ranges 0..1\n",
    "    return min_lr + coefficient * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f9caa1faa59dcd",
   "metadata": {},
   "source": [
    "Clean up old checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39f4318ddec2db9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T07:55:54.138535585Z",
     "start_time": "2026-02-23T07:55:54.115249446Z"
    }
   },
   "outputs": [],
   "source": [
    "from checkpoint_cleaner import CheckpointCleaner\n",
    "\n",
    "checkpoint_cleaner=CheckpointCleaner(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2775c2e3339206",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "198d433f585502b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T08:01:36.884239960Z",
     "start_time": "2026-02-23T07:55:54.190224041Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 10.0936, val loss 10.0847\n",
      "step 10: train loss 9.6919, val loss 9.7082\n",
      "step 20: train loss 8.6244, val loss 8.7564\n",
      "step 30: train loss 7.5334, val loss 7.5111\n",
      "Removed checkpoint pre_checkpoints/state/00000.pt\n",
      "step 40: train loss 7.5454, val loss 7.3536\n",
      "Removed checkpoint pre_checkpoints/state/00010.pt\n",
      "step 50: train loss 7.0324, val loss 7.2933\n",
      "Removed checkpoint pre_checkpoints/state/00020.pt\n",
      "step 60: train loss 7.2897, val loss 7.0998\n",
      "Removed checkpoint pre_checkpoints/state/00030.pt\n",
      "step 70: train loss 6.9825, val loss 7.0782\n",
      "Removed checkpoint pre_checkpoints/state/00040.pt\n",
      "step 80: train loss 7.0303, val loss 7.1989\n",
      "Removed checkpoint pre_checkpoints/state/00050.pt\n",
      "step 90: train loss 6.9475, val loss 7.0621\n",
      "Removed checkpoint pre_checkpoints/state/00060.pt\n",
      "step 100: train loss 7.1708, val loss 6.9144\n",
      "Removed checkpoint pre_checkpoints/state/00070.pt\n",
      "step 110: train loss 6.8460, val loss 6.9343\n",
      "Removed checkpoint pre_checkpoints/state/00080.pt\n",
      "step 120: train loss 6.8119, val loss 6.9966\n",
      "Removed checkpoint pre_checkpoints/state/00090.pt\n",
      "step 130: train loss 7.0052, val loss 6.9543\n",
      "Removed checkpoint pre_checkpoints/state/00100.pt\n",
      "step 140: train loss 6.9234, val loss 6.8070\n",
      "Removed checkpoint pre_checkpoints/state/00110.pt\n",
      "step 150: train loss 6.6615, val loss 6.8490\n",
      "Removed checkpoint pre_checkpoints/state/00120.pt\n",
      "step 160: train loss 6.5929, val loss 6.6323\n",
      "Removed checkpoint pre_checkpoints/state/00130.pt\n",
      "step 170: train loss 6.7193, val loss 6.6833\n",
      "Removed checkpoint pre_checkpoints/state/00140.pt\n",
      "step 180: train loss 6.6289, val loss 6.5393\n",
      "Removed checkpoint pre_checkpoints/state/00150.pt\n",
      "step 190: train loss 6.4505, val loss 6.4280\n",
      "Removed checkpoint pre_checkpoints/state/00160.pt\n",
      "step 200: train loss 6.2942, val loss 6.3642\n",
      "Removed checkpoint pre_checkpoints/state/00170.pt\n",
      "step 210: train loss 6.2657, val loss 6.6315\n",
      "Removed checkpoint pre_checkpoints/state/00180.pt\n",
      "step 220: train loss 6.5534, val loss 6.2186\n",
      "Removed checkpoint pre_checkpoints/state/00190.pt\n",
      "step 230: train loss 6.6006, val loss 6.1591\n",
      "Removed checkpoint pre_checkpoints/state/00200.pt\n",
      "step 240: train loss 6.4160, val loss 6.3366\n",
      "Removed checkpoint pre_checkpoints/state/00210.pt\n",
      "step 250: train loss 6.2889, val loss 6.6368\n",
      "Removed checkpoint pre_checkpoints/state/00220.pt\n",
      "step 260: train loss 6.1943, val loss 6.1674\n",
      "Removed checkpoint pre_checkpoints/state/00230.pt\n",
      "step 270: train loss 6.1215, val loss 6.0904\n",
      "Removed checkpoint pre_checkpoints/state/00240.pt\n",
      "step 280: train loss 6.2143, val loss 5.9875\n",
      "Removed checkpoint pre_checkpoints/state/00250.pt\n",
      "step 290: train loss 6.1674, val loss 6.3094\n",
      "Removed checkpoint pre_checkpoints/state/00260.pt\n",
      "step 300: train loss 6.1285, val loss 6.1139\n",
      "Removed checkpoint pre_checkpoints/state/00270.pt\n",
      "step 310: train loss 5.9856, val loss 5.9629\n",
      "Removed checkpoint pre_checkpoints/state/00280.pt\n",
      "step 320: train loss 5.8491, val loss 6.1188\n",
      "Removed checkpoint pre_checkpoints/state/00290.pt\n",
      "step 330: train loss 5.6870, val loss 5.6950\n",
      "Removed checkpoint pre_checkpoints/state/00300.pt\n",
      "step 340: train loss 5.8116, val loss 5.8150\n",
      "Removed checkpoint pre_checkpoints/state/00310.pt\n",
      "step 350: train loss 5.8509, val loss 5.7099\n",
      "Removed checkpoint pre_checkpoints/state/00320.pt\n",
      "step 360: train loss 6.0016, val loss 5.6803\n",
      "Removed checkpoint pre_checkpoints/state/00330.pt\n",
      "step 370: train loss 6.0508, val loss 5.9215\n",
      "Removed checkpoint pre_checkpoints/state/00340.pt\n",
      "step 380: train loss 5.7911, val loss 5.6263\n",
      "Removed checkpoint pre_checkpoints/state/00350.pt\n",
      "step 390: train loss 5.6155, val loss 5.6467\n",
      "Removed checkpoint pre_checkpoints/state/00360.pt\n",
      "step 400: train loss 5.5978, val loss 5.5374\n",
      "Removed checkpoint pre_checkpoints/state/00370.pt\n",
      "step 410: train loss 5.9310, val loss 5.6428\n",
      "Removed checkpoint pre_checkpoints/state/00380.pt\n",
      "step 420: train loss 5.7208, val loss 5.6269\n",
      "Removed checkpoint pre_checkpoints/state/00390.pt\n",
      "step 430: train loss 5.5800, val loss 5.5140\n",
      "Removed checkpoint pre_checkpoints/state/00400.pt\n",
      "step 440: train loss 5.5583, val loss 5.7474\n",
      "Removed checkpoint pre_checkpoints/state/00410.pt\n",
      "step 450: train loss 5.6091, val loss 5.4104\n",
      "Removed checkpoint pre_checkpoints/state/00420.pt\n",
      "step 460: train loss 5.8287, val loss 5.3196\n",
      "Removed checkpoint pre_checkpoints/state/00430.pt\n",
      "step 470: train loss 5.3787, val loss 5.2411\n",
      "Removed checkpoint pre_checkpoints/state/00440.pt\n",
      "step 480: train loss 5.4335, val loss 5.6625\n",
      "Removed checkpoint pre_checkpoints/state/00450.pt\n",
      "step 490: train loss 5.2988, val loss 5.4616\n",
      "Removed checkpoint pre_checkpoints/state/00460.pt\n",
      "step 500: train loss 5.3551, val loss 5.3159\n",
      "Removed checkpoint pre_checkpoints/state/00470.pt\n",
      "step 510: train loss 5.4863, val loss 5.5878\n",
      "Removed checkpoint pre_checkpoints/state/00480.pt\n",
      "step 520: train loss 5.2328, val loss 5.6186\n",
      "Removed checkpoint pre_checkpoints/state/00490.pt\n",
      "step 530: train loss 5.2877, val loss 5.2202\n",
      "Removed checkpoint pre_checkpoints/state/00500.pt\n",
      "step 540: train loss 5.2196, val loss 5.0702\n",
      "Removed checkpoint pre_checkpoints/state/00510.pt\n",
      "step 550: train loss 5.2967, val loss 5.4062\n",
      "Removed checkpoint pre_checkpoints/state/00520.pt\n",
      "step 560: train loss 5.1399, val loss 5.1693\n",
      "Removed checkpoint pre_checkpoints/state/00530.pt\n",
      "step 570: train loss 5.2685, val loss 4.8461\n",
      "Removed checkpoint pre_checkpoints/state/00540.pt\n",
      "step 580: train loss 5.1514, val loss 5.3506\n",
      "Removed checkpoint pre_checkpoints/state/00550.pt\n",
      "step 590: train loss 5.3902, val loss 4.9661\n",
      "Removed checkpoint pre_checkpoints/state/00560.pt\n",
      "step 600: train loss 5.3469, val loss 5.0535\n",
      "Removed checkpoint pre_checkpoints/state/00570.pt\n",
      "step 610: train loss 5.1504, val loss 4.8364\n",
      "Removed checkpoint pre_checkpoints/state/00580.pt\n",
      "step 620: train loss 5.3315, val loss 5.1375\n",
      "Removed checkpoint pre_checkpoints/state/00590.pt\n",
      "step 630: train loss 5.0280, val loss 5.0768\n",
      "Removed checkpoint pre_checkpoints/state/00600.pt\n",
      "step 640: train loss 4.9578, val loss 4.7867\n",
      "Removed checkpoint pre_checkpoints/state/00610.pt\n",
      "step 650: train loss 5.1681, val loss 5.1226\n",
      "Removed checkpoint pre_checkpoints/state/00620.pt\n",
      "step 660: train loss 4.7977, val loss 4.8167\n",
      "Removed checkpoint pre_checkpoints/state/00630.pt\n",
      "step 670: train loss 4.8484, val loss 5.1127\n",
      "Removed checkpoint pre_checkpoints/state/00640.pt\n",
      "step 680: train loss 5.1431, val loss 4.9757\n",
      "Removed checkpoint pre_checkpoints/state/00650.pt\n",
      "step 690: train loss 4.9905, val loss 4.9056\n",
      "Removed checkpoint pre_checkpoints/state/00660.pt\n",
      "step 700: train loss 5.0163, val loss 5.0071\n",
      "Removed checkpoint pre_checkpoints/state/00670.pt\n",
      "step 710: train loss 5.1546, val loss 4.7118\n",
      "Removed checkpoint pre_checkpoints/state/00680.pt\n",
      "step 720: train loss 4.8857, val loss 5.2286\n",
      "Removed checkpoint pre_checkpoints/state/00690.pt\n",
      "step 730: train loss 4.9641, val loss 5.0726\n",
      "Removed checkpoint pre_checkpoints/state/00700.pt\n",
      "step 740: train loss 4.8153, val loss 4.9452\n",
      "Removed checkpoint pre_checkpoints/state/00710.pt\n",
      "step 750: train loss 4.6794, val loss 4.9314\n",
      "Removed checkpoint pre_checkpoints/state/00720.pt\n",
      "step 760: train loss 4.4705, val loss 4.4927\n",
      "Removed checkpoint pre_checkpoints/state/00730.pt\n",
      "step 770: train loss 4.8188, val loss 4.6183\n",
      "Removed checkpoint pre_checkpoints/state/00740.pt\n",
      "step 780: train loss 4.6297, val loss 4.8454\n",
      "Removed checkpoint pre_checkpoints/state/00750.pt\n",
      "step 790: train loss 4.9442, val loss 5.3425\n",
      "Removed checkpoint pre_checkpoints/state/00760.pt\n",
      "step 800: train loss 5.0456, val loss 4.8273\n",
      "Removed checkpoint pre_checkpoints/state/00770.pt\n",
      "step 810: train loss 4.7502, val loss 4.8991\n",
      "Removed checkpoint pre_checkpoints/state/00780.pt\n",
      "step 820: train loss 4.5807, val loss 4.6559\n",
      "Removed checkpoint pre_checkpoints/state/00790.pt\n",
      "step 830: train loss 4.6939, val loss 4.5324\n",
      "Removed checkpoint pre_checkpoints/state/00800.pt\n",
      "step 840: train loss 4.9357, val loss 5.0889\n",
      "Removed checkpoint pre_checkpoints/state/00810.pt\n",
      "step 850: train loss 4.7492, val loss 4.8887\n",
      "Removed checkpoint pre_checkpoints/state/00820.pt\n",
      "step 860: train loss 4.8345, val loss 4.7089\n",
      "Removed checkpoint pre_checkpoints/state/00830.pt\n",
      "step 870: train loss 4.7303, val loss 4.7818\n",
      "Removed checkpoint pre_checkpoints/state/00840.pt\n",
      "step 880: train loss 4.8337, val loss 4.6943\n",
      "Removed checkpoint pre_checkpoints/state/00850.pt\n",
      "step 890: train loss 4.8096, val loss 4.2977\n",
      "Removed checkpoint pre_checkpoints/state/00860.pt\n",
      "step 900: train loss 4.7754, val loss 4.6852\n",
      "Removed checkpoint pre_checkpoints/state/00870.pt\n",
      "step 910: train loss 4.5013, val loss 4.7688\n",
      "Removed checkpoint pre_checkpoints/state/00880.pt\n",
      "step 920: train loss 4.5337, val loss 4.6814\n",
      "Removed checkpoint pre_checkpoints/state/00890.pt\n",
      "step 930: train loss 4.8053, val loss 4.4702\n",
      "Removed checkpoint pre_checkpoints/state/00900.pt\n",
      "step 940: train loss 4.4954, val loss 4.6373\n",
      "Removed checkpoint pre_checkpoints/state/00910.pt\n",
      "step 950: train loss 4.6389, val loss 4.5959\n",
      "Removed checkpoint pre_checkpoints/state/00920.pt\n",
      "step 960: train loss 4.8952, val loss 4.7735\n",
      "Removed checkpoint pre_checkpoints/state/00930.pt\n",
      "step 970: train loss 4.5332, val loss 4.7307\n",
      "Removed checkpoint pre_checkpoints/state/00940.pt\n",
      "step 980: train loss 4.4291, val loss 4.5307\n",
      "Removed checkpoint pre_checkpoints/state/00950.pt\n",
      "step 990: train loss 4.3871, val loss 4.2774\n",
      "Removed checkpoint pre_checkpoints/state/00960.pt\n",
      "step 999: train loss 4.6294, val loss 4.8226\n",
      "Removed checkpoint pre_checkpoints/state/00970.pt\n"
     ]
    }
   ],
   "source": [
    "from system_metrics import get_system_metrics\n",
    "\n",
    "metric_logs = []\n",
    "\n",
    "for step in range(checkpoint or 0, max_iters):\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    if autocast_enabled:\n",
    "        with torch.amp.autocast(dtype=torch.float16, device_type=device_type):\n",
    "            logits, loss = model(xb, yb)\n",
    "    else:\n",
    "        logits, loss = model(xb, yb)\n",
    "\n",
    "    # exit if the loss is invalid\n",
    "    if not torch.isfinite(loss):\n",
    "        raise Exception(\"Non-finite loss detected.\")\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.unscale_(optimizer)\n",
    "\n",
    "    if step % log_metrics_interval == 0:\n",
    "        total_norm, max_grad = get_grad_metrics(model)\n",
    "        max_weight, total_weight_norm = get_weight_metrics(model)\n",
    "        metric_logs.append({\n",
    "            \"gradient\": {\n",
    "                \"total_norm\": total_norm,\n",
    "                \"max_grad\": max_grad,\n",
    "            },\n",
    "            \"weight\": {\n",
    "                \"max_weight\": max_weight,\n",
    "                \"total_weight_norm\": total_weight_norm,\n",
    "            },\n",
    "            \"system\": get_system_metrics(),\n",
    "            \"current_loss\": loss.item()\n",
    "        })\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    if step % eval_interval == 0 or step == max_iters-1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        state_path,info_path= save_checkpoint(\n",
    "            step=step,\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            scaler=scaler,\n",
    "            train_loss=losses[\"train\"],\n",
    "            val_loss=losses[\"val\"],\n",
    "            learning_rate=lr,\n",
    "            metric_logs=metric_logs\n",
    "        )\n",
    "        checkpoint_cleaner.step(state_path)\n",
    "        metric_logs = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1173b07565d315a4",
   "metadata": {},
   "source": [
    "Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b89a73b965f9742",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T08:01:38.923756287Z",
     "start_time": "2026-02-23T08:01:36.969025361Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Located and shelf into the casting, his screensine it came into bid to staticics in the crowd. Choose an hide her Harvey bell- born usinginy character. InWhile she has recifly at Im supply for an t exercised jar for an battle\n"
     ]
    }
   ],
   "source": [
    "start_token_id = get_batch(\"test\")[0][0][0].item()\n",
    "start_text = tokenizer.decode([start_token_id])\n",
    "print(generate(model, start_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonproject1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
