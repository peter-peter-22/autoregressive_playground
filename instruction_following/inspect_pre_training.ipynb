{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-16T15:52:20.530165387Z",
     "start_time": "2026-02-16T15:52:19.251767069Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from settings import ModelSettings\n",
    "\n",
    "document = np.arange(1100)\n",
    "context_length = ModelSettings.max_context_length\n",
    "print(document)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    1    2 ... 1097 1098 1099]\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T15:52:20.549522365Z",
     "start_time": "2026-02-16T15:52:20.532859515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example\n",
    "example_context_length = 10\n",
    "x = document[:example_context_length]\n",
    "y = document[1:example_context_length + 1]\n",
    "print(x)\n",
    "print(y)"
   ],
   "id": "75e25eeab4b59ee2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[ 1  2  3  4  5  6  7  8  9 10]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T15:52:20.559083416Z",
     "start_time": "2026-02-16T15:52:20.550269537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "block_size = 128  # smaller context size\n",
    "batch_size = 16\n",
    "device = \"cpu\"\n",
    "data_dir = \"tokenized_data\"\n",
    "\n",
    "\n",
    "def get_mini_batch(split):\n",
    "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
    "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
    "    if split == 'train':\n",
    "        data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "    else:\n",
    "        data = np.memmap(os.path.join(data_dir, 'test.bin'), dtype=np.uint16, mode='r')\n",
    "    ix = torch.randint(min(len(data), 1000) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i + block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i + 1:i + 1 + block_size]).astype(np.int64)) for i in ix])\n",
    "    if device == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ],
   "id": "39f49c5833de57aa",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T15:52:20.582467669Z",
     "start_time": "2026-02-16T15:52:20.560293110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x, y = get_mini_batch(\"test\")\n",
    "print(x.shape, y.shape)"
   ],
   "id": "6c0a7117bacc4488",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 128]) torch.Size([16, 128])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T15:52:20.677014094Z",
     "start_time": "2026-02-16T15:52:20.583585668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from model import ChatModel\n",
    "from settings import ModelSettings\n",
    "\n",
    "model = ChatModel(\n",
    "    vocabulary_size=ModelSettings.vocabulary_size,\n",
    "    embedding_size=256,\n",
    "    embedding_dropout=0.0,\n",
    "    attention_dropout=0.0,\n",
    "    max_context_length=block_size,\n",
    "    ff_size_multiplier=4,\n",
    "    ff_dropout=0.0,\n",
    "    transformer_blocks=6,\n",
    "    attention_heads=8\n",
    ")"
   ],
   "id": "d529ab02a58a4ac0",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T15:52:21.417882051Z",
     "start_time": "2026-02-16T15:52:20.678611287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from optimizer import get_optim_groups\n",
    "\n",
    "optim_groups = get_optim_groups(model)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    optim_groups,\n",
    "    lr=3e-4,\n",
    "    betas=(0.9, 0.95),\n",
    "    # b1: how fast the model adapts to the average cale of the gradients, higher=slower\n",
    "    # b2: how fast the weights change, higher=slower\n",
    "    eps=1e-8\n",
    ")\n"
   ],
   "id": "f8dc1093f08e8628",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T15:52:21.425362860Z",
     "start_time": "2026-02-16T15:52:21.418945235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_path = \"mini_model_training.pth\"\n",
    "if os.path.exists(model_path):\n",
    "    model.load_state_dict(\n",
    "        torch.load(\n",
    "            model_path,\n",
    "            weights_only=True,\n",
    "            map_location=torch.device('cpu') if device == \"cpu\" else None\n",
    "        )\n",
    "    )\n",
    "    print(\"Mode weights loaded\")"
   ],
   "id": "62b59ba874f53ba4",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T15:55:55.801045995Z",
     "start_time": "2026-02-16T15:52:21.426061708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from math import exp\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "patience = 5  # number of evaluations to wait\n",
    "min_delta = 0.02  # minimum improvement\n",
    "patience_counter = 0\n",
    "grad_clip = 1.0\n",
    "\n",
    "for step in range(50000):\n",
    "    xb, yb = get_mini_batch(\"test\")\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # Prevent too large gradients\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        loss_num = loss.item()\n",
    "        perplexity = exp(loss_num)\n",
    "        print(f\"step {step}, loss {loss_num:.4f}, perplexity {perplexity:.2f}\")\n",
    "        if best_loss - loss_num > min_delta:\n",
    "            best_loss = loss_num\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break"
   ],
   "id": "198d433f585502b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss 10.2418, perplexity 28051.13\n",
      "step 10, loss 8.4149, perplexity 4513.61\n",
      "step 20, loss 7.2205, perplexity 1367.17\n",
      "step 30, loss 5.7842, perplexity 325.12\n",
      "step 40, loss 4.6758, perplexity 107.32\n",
      "step 50, loss 3.9124, perplexity 50.02\n",
      "step 60, loss 3.0793, perplexity 21.74\n",
      "step 70, loss 2.8498, perplexity 17.29\n",
      "step 80, loss 2.2055, perplexity 9.07\n",
      "step 90, loss 1.8019, perplexity 6.06\n",
      "step 100, loss 1.5371, perplexity 4.65\n",
      "step 110, loss 1.2941, perplexity 3.65\n",
      "step 120, loss 1.0966, perplexity 2.99\n",
      "step 130, loss 0.9492, perplexity 2.58\n",
      "step 140, loss 0.7389, perplexity 2.09\n",
      "step 150, loss 0.6114, perplexity 1.84\n",
      "step 160, loss 0.4281, perplexity 1.53\n",
      "step 170, loss 0.3836, perplexity 1.47\n",
      "step 180, loss 0.2520, perplexity 1.29\n",
      "step 190, loss 0.1920, perplexity 1.21\n",
      "step 200, loss 0.1624, perplexity 1.18\n",
      "step 210, loss 0.1111, perplexity 1.12\n",
      "step 220, loss 0.1042, perplexity 1.11\n",
      "step 230, loss 0.0821, perplexity 1.09\n",
      "step 240, loss 0.0679, perplexity 1.07\n",
      "step 250, loss 0.0662, perplexity 1.07\n",
      "step 260, loss 0.0670, perplexity 1.07\n",
      "step 270, loss 0.0372, perplexity 1.04\n",
      "step 280, loss 0.0408, perplexity 1.04\n",
      "step 290, loss 0.0336, perplexity 1.03\n",
      "step 300, loss 0.0411, perplexity 1.04\n",
      "step 310, loss 0.0406, perplexity 1.04\n",
      "step 320, loss 0.0281, perplexity 1.03\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T15:55:55.970145500Z",
     "start_time": "2026-02-16T15:55:55.803266477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tokenizers.tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"tokenizer.json\")"
   ],
   "id": "777e9c8dac99b4a2",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T15:55:55.982125558Z",
     "start_time": "2026-02-16T15:55:55.972842673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, start, max_new_tokens=50):\n",
    "    model.eval()\n",
    "    idx = torch.tensor([tokenizer.encode(start).ids], device=device, dtype=torch.long)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -ModelSettings.max_context_length:]\n",
    "        logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = nn.functional.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "\n",
    "    return tokenizer.decode(idx[0].tolist())"
   ],
   "id": "a0088e6da74c7802",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T15:55:56.284660370Z",
     "start_time": "2026-02-16T15:55:55.982938299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start_token_id = get_mini_batch(\"test\")[0][0][0].item()\n",
    "start_text = tokenizer.decode([start_token_id])\n",
    "print(generate(model, start_text))"
   ],
   "id": "bc962e94cd200c64",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are 5 things I can do when it's high-end retail sector named The Village. In addition to a wide selection of food and shopping, there are multiple activity options including miniature golf, this East London market hosts various community events including\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
