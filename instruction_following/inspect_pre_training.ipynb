{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-16T22:12:47.352233605Z",
     "start_time": "2026-02-16T22:12:46.377005884Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from settings import ModelSettings\n",
    "\n",
    "document = np.arange(1100)\n",
    "context_length = ModelSettings.max_context_length\n",
    "print(document)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    1    2 ... 1097 1098 1099]\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T22:12:47.381913434Z",
     "start_time": "2026-02-16T22:12:47.354017713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example\n",
    "example_context_length = 10\n",
    "x = document[:example_context_length]\n",
    "y = document[1:example_context_length + 1]\n",
    "print(x)\n",
    "print(y)"
   ],
   "id": "75e25eeab4b59ee2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[ 1  2  3  4  5  6  7  8  9 10]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T22:12:47.391882787Z",
     "start_time": "2026-02-16T22:12:47.384123695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "block_size = 128  # smaller context size\n",
    "batch_size = 16\n",
    "device = \"cpu\"\n",
    "data_dir = \"tokenized_data\"\n",
    "\n",
    "\n",
    "def get_mini_batch(split):\n",
    "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
    "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
    "    if split == 'train':\n",
    "        data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "    else:\n",
    "        data = np.memmap(os.path.join(data_dir, 'test.bin'), dtype=np.uint16, mode='r')\n",
    "    ix = torch.randint(min(len(data), 1000) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i + block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i + 1:i + 1 + block_size]).astype(np.int64)) for i in ix])\n",
    "    if device == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ],
   "id": "39f49c5833de57aa",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T22:12:47.412476298Z",
     "start_time": "2026-02-16T22:12:47.392381228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x, y = get_mini_batch(\"test\")\n",
    "print(x.shape, y.shape)"
   ],
   "id": "6c0a7117bacc4488",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 128]) torch.Size([16, 128])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T22:12:47.523982100Z",
     "start_time": "2026-02-16T22:12:47.413140182Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from model import ChatModel\n",
    "from settings import ModelSettings\n",
    "\n",
    "model = ChatModel(\n",
    "    vocabulary_size=ModelSettings.vocabulary_size,\n",
    "    embedding_size=256,\n",
    "    embedding_dropout=0.0,\n",
    "    max_context_length=block_size,\n",
    "    ff_size_multiplier=4,\n",
    "    transformer_blocks=6,\n",
    "    attention_heads=8,\n",
    "    dropout=0.0,\n",
    "    bias=True,\n",
    "    residual_scaling=True,\n",
    "    weight_tying=True\n",
    ")"
   ],
   "id": "d529ab02a58a4ac0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding Embedding(24000, 256)\n",
      "head Linear(in_features=256, out_features=24000, bias=False)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T22:12:48.227302906Z",
     "start_time": "2026-02-16T22:12:47.524559222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from optimizer import get_optim_groups\n",
    "\n",
    "optim_groups = get_optim_groups(model)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    optim_groups,\n",
    "    lr=3e-4,\n",
    "    betas=(0.9, 0.95),\n",
    "    # b1: how fast the model adapts to the average cale of the gradients, higher=slower\n",
    "    # b2: how fast the weights change, higher=slower\n",
    "    eps=1e-8\n",
    ")\n"
   ],
   "id": "f8dc1093f08e8628",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T22:12:48.233461623Z",
     "start_time": "2026-02-16T22:12:48.227907842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_path = \"mini_model_training.pth\"\n",
    "if os.path.exists(model_path):\n",
    "    model.load_state_dict(\n",
    "        torch.load(\n",
    "            model_path,\n",
    "            weights_only=True,\n",
    "            map_location=torch.device('cpu') if device == \"cpu\" else None\n",
    "        )\n",
    "    )\n",
    "    print(\"Mode weights loaded\")"
   ],
   "id": "62b59ba874f53ba4",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T22:26:38.988342129Z",
     "start_time": "2026-02-16T22:12:48.233999990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from math import exp\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "patience = 5  # number of evaluations to wait\n",
    "min_delta = 0.02  # minimum improvement\n",
    "patience_counter = 0\n",
    "grad_clip = 1.0\n",
    "\n",
    "for step in range(50000):\n",
    "    xb, yb = get_mini_batch(\"test\")\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # Prevent too large gradients\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        loss_num = loss.item()\n",
    "        perplexity = exp(loss_num)\n",
    "        print(f\"step {step}, loss {loss_num:.4f}, perplexity {perplexity:.2f}\")\n",
    "        if best_loss - loss_num > min_delta:\n",
    "            best_loss = loss_num\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break"
   ],
   "id": "198d433f585502b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss 179.4449, perplexity 854889962457899233325099199343136949423274489191228066758577654105173896724480.00\n",
      "step 10, loss 52.4771, perplexity 61731500646598545768448.00\n",
      "step 20, loss 35.9082, perplexity 3933245362400387.00\n",
      "step 30, loss 30.2546, perplexity 13784898576575.82\n",
      "step 40, loss 26.6158, perplexity 362328145095.38\n",
      "step 50, loss 23.7607, perplexity 20851796569.37\n",
      "step 60, loss 21.6412, perplexity 2503995105.65\n",
      "step 70, loss 20.4506, perplexity 761377404.13\n",
      "step 80, loss 18.4783, perplexity 105929754.94\n",
      "step 90, loss 17.3165, perplexity 33148919.87\n",
      "step 100, loss 14.3294, perplexity 1671744.30\n",
      "step 110, loss 12.3518, perplexity 231387.61\n",
      "step 120, loss 9.6641, perplexity 15742.57\n",
      "step 130, loss 8.2472, perplexity 3816.93\n",
      "step 140, loss 5.0302, perplexity 152.96\n",
      "step 150, loss 5.1927, perplexity 179.95\n",
      "step 160, loss 3.1774, perplexity 23.99\n",
      "step 170, loss 2.0806, perplexity 8.01\n",
      "step 180, loss 2.4550, perplexity 11.65\n",
      "step 190, loss 1.5176, perplexity 4.56\n",
      "step 200, loss 1.5587, perplexity 4.75\n",
      "step 210, loss 1.1403, perplexity 3.13\n",
      "step 220, loss 1.4039, perplexity 4.07\n",
      "step 230, loss 0.7948, perplexity 2.21\n",
      "step 240, loss 0.7577, perplexity 2.13\n",
      "step 250, loss 0.5808, perplexity 1.79\n",
      "step 260, loss 0.5180, perplexity 1.68\n",
      "step 270, loss 0.5302, perplexity 1.70\n",
      "step 280, loss 0.5286, perplexity 1.70\n",
      "step 290, loss 0.3861, perplexity 1.47\n",
      "step 300, loss 0.3052, perplexity 1.36\n",
      "step 310, loss 0.2626, perplexity 1.30\n",
      "step 320, loss 0.2965, perplexity 1.35\n",
      "step 330, loss 0.2832, perplexity 1.33\n",
      "step 340, loss 0.2708, perplexity 1.31\n",
      "step 350, loss 0.2426, perplexity 1.27\n",
      "step 360, loss 0.1912, perplexity 1.21\n",
      "step 370, loss 0.2257, perplexity 1.25\n",
      "step 380, loss 0.1205, perplexity 1.13\n",
      "step 390, loss 0.1438, perplexity 1.15\n",
      "step 400, loss 0.1472, perplexity 1.16\n",
      "step 410, loss 0.1101, perplexity 1.12\n",
      "step 420, loss 0.1201, perplexity 1.13\n",
      "step 430, loss 0.1340, perplexity 1.14\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T22:26:39.031743275Z",
     "start_time": "2026-02-16T22:26:38.989172905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tokenizers.tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"tokenizer.json\")"
   ],
   "id": "777e9c8dac99b4a2",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T22:26:39.038914233Z",
     "start_time": "2026-02-16T22:26:39.032364210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, start, max_new_tokens=50):\n",
    "    model.eval()\n",
    "    idx = torch.tensor([tokenizer.encode(start).ids], device=device, dtype=torch.long)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -ModelSettings.max_context_length:]\n",
    "        logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = nn.functional.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "\n",
    "    return tokenizer.decode(idx[0].tolist())"
   ],
   "id": "a0088e6da74c7802",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T22:26:39.342120902Z",
     "start_time": "2026-02-16T22:26:39.039520410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start_token_id = get_mini_batch(\"test\")[0][0][0].item()\n",
    "start_text = tokenizer.decode([start_token_id])\n",
    "print(generate(model, start_text))"
   ],
   "id": "bc962e94cd200c64",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " foodWhat areNP.tenegtenegtenegtenegtenegtenegtenegtenegtenegtenegtenegtenegtenegtenegtenegtenegtenegtenegtenegtenegtenegtenegtenegteneg of activities shel sheltered by the power station including miniature gold, pong, and a cinema.\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
