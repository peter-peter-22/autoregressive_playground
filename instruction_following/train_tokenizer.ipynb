{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Train the BPE tokenizer on a small sample of the datasets, add special tokens.\n",
    "\n",
    "IMPROVEMENT: prevent special token injection"
   ],
   "id": "31d082c2b626219f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:36:36.176458044Z",
     "start_time": "2026-02-13T21:36:21.636182811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tokenizer_datasets import *\n",
    "\n",
    "lite_dataset = False\n",
    "dataset_iter, dataset_length = tokenizer_lite_dataset() if lite_dataset else tokenizer_real_dataset()\n",
    "\n",
    "print(dataset_length)\n",
    "example_message = next(dataset_iter())\n",
    "print(example_message)"
   ],
   "id": "eeb6b55c15ae509f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3798585513\n",
      "Aster is a chatbot who answers questions with rhymes.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:36:36.214760518Z",
     "start_time": "2026-02-13T21:36:36.183756276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "\n",
    "tokenizer = Tokenizer(models.BPE(unk_token=\"<|unk|>\"))"
   ],
   "id": "b945d6a1885d0859",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:36:36.246810937Z",
     "start_time": "2026-02-13T21:36:36.216638476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tokenizers.normalizers import NFD, StripAccents, Lowercase, Sequence\n",
    "\n",
    "tokenizer.normalizer = Sequence([\n",
    "    NFD(),\n",
    "    StripAccents(),\n",
    "    Lowercase()\n",
    "])\n",
    "tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\")"
   ],
   "id": "21234873cfa63d4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello how are u?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:36:36.277075114Z",
     "start_time": "2026-02-13T21:36:36.249120101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(example_message)"
   ],
   "id": "93d668078757e946",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Aster', (0, 5)),\n",
       " ('Ġis', (5, 8)),\n",
       " ('Ġa', (8, 10)),\n",
       " ('Ġchatbot', (10, 18)),\n",
       " ('Ġwho', (18, 22)),\n",
       " ('Ġanswers', (22, 30)),\n",
       " ('Ġquestions', (30, 40)),\n",
       " ('Ġwith', (40, 45)),\n",
       " ('Ġrhymes', (45, 52)),\n",
       " ('.', (52, 53))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:36:36.296894907Z",
     "start_time": "2026-02-13T21:36:36.279170533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from special_tokens import special_tokens\n",
    "\n",
    "special_token_list = list(special_tokens.values())\n",
    "tokenizer.add_tokens(special_token_list)\n",
    "tokenizer.get_vocab()"
   ],
   "id": "e672bf14aedce181",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<|bos|>': 2,\n",
       " '<|pad|>': 7,\n",
       " '<|eos|>': 3,\n",
       " '<|assistant|>': 5,\n",
       " '<|endofturn|>': 1,\n",
       " '<|system|>': 6,\n",
       " '<|user|>': 4,\n",
       " '<|unk|>': 8,\n",
       " '<|endoftext|>': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:36:36.305058809Z",
     "start_time": "2026-02-13T21:36:36.298898984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=8000,\n",
    "    special_tokens=special_token_list,\n",
    "    show_progress=True,\n",
    "    min_frequency=2,\n",
    ")"
   ],
   "id": "bb54d1e6a69bb983",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:37:58.495357640Z",
     "start_time": "2026-02-13T21:36:36.306053292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer.train_from_iterator(dataset_iter(), trainer, length=dataset_length)\n",
    "file_name = \"./tokenizer.json\""
   ],
   "id": "3ae5328152083e2a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no robots completed\n",
      "wiki completed\n",
      "tiny stories completed\n",
      "tiny textbooks completed\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:37:58.516740841Z",
     "start_time": "2026-02-13T21:37:58.506232954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tokenizers import decoders, processors\n",
    "\n",
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
    "tokenizer.decoder = decoders.ByteLevel()"
   ],
   "id": "75f2d1c5e7f3c0fb",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:37:58.529942120Z",
     "start_time": "2026-02-13T21:37:58.518391252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"<|bos|> $A <|eos|>\",  # adds BOS before & EOS after each sequence\n",
    "    pair=\"<|bos|> $A <|eos|> <|bos|> $B <|eos|>\",  # for pairs (less common)\n",
    "    special_tokens=[\n",
    "        (\"<|bos|>\", tokenizer.token_to_id(\"<|bos|>\")),\n",
    "        (\"<|eos|>\", tokenizer.token_to_id(\"<|eos|>\")),\n",
    "    ],\n",
    ")"
   ],
   "id": "7478a5f2bcc078ca",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:37:58.544329408Z",
     "start_time": "2026-02-13T21:37:58.531630948Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.save(file_name)",
   "id": "bd39b6da528eb576",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:37:58.573743056Z",
     "start_time": "2026-02-13T21:37:58.546116085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"<|user|>\" in tokenizer.get_vocab())\n",
    "print(\"<|\" in tokenizer.get_vocab())"
   ],
   "id": "3d84110ddae5b69",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:37:58.607684472Z",
     "start_time": "2026-02-13T21:37:58.575087368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = Tokenizer.from_file(file_name)\n",
    "\n",
    "encoding = tokenizer.encode(example_message)\n",
    "print(encoding.tokens)\n",
    "decoding = tokenizer.decode(encoding.ids)\n",
    "print(decoding)"
   ],
   "id": "1f9598fc08c814dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|bos|>', 'aster', 'Ġis', 'Ġa', 'Ġch', 'at', 'b', 'ot', 'Ġwho', 'Ġanswers', 'Ġquestions', 'Ġwith', 'Ġrhy', 'mes', '.', '<|eos|>']\n",
      "aster is a chatbot who answers questions with rhymes.\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
