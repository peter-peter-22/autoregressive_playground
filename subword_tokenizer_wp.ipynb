{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T17:11:21.031394446Z",
     "start_time": "2026-02-10T17:11:20.950326210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers\n",
    "from tokenizers.normalizers import Replace\n",
    "from tokenizers.pre_tokenizers import Split\n",
    "\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n"
   ],
   "id": "55e3488fe548a93c",
   "outputs": [],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T17:11:21.095678632Z",
     "start_time": "2026-02-10T17:11:21.059164809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents(),Replace(\"\\n\",\"Ċ\")]\n",
    ")\n",
    "tokenizer.normalizer.normalize_str(\"Héllò hôw\\nare ü?\")"
   ],
   "id": "bcfbc5f1f09fdc82",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello howĊare u?'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T17:11:21.151603939Z",
     "start_time": "2026-02-10T17:11:21.100306798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pre_tokenizer = pre_tokenizers.Sequence(\n",
    "    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation(),Split(\"Ċ\",\"isolated\")]\n",
    ")\n",
    "n=tokenizer.normalizer.normalize_str(\"To be, or not to be:\\n\\nThat is \\nthe question.\" )\n",
    "pre_tokenizer.pre_tokenize_str(n)"
   ],
   "id": "ca85ee99e0687be1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', (0, 2)),\n",
       " ('be', (3, 5)),\n",
       " (',', (5, 6)),\n",
       " ('or', (7, 9)),\n",
       " ('not', (10, 13)),\n",
       " ('to', (14, 16)),\n",
       " ('be', (17, 19)),\n",
       " (':', (19, 20)),\n",
       " ('Ċ', (20, 21)),\n",
       " ('Ċ', (21, 22)),\n",
       " ('that', (22, 26)),\n",
       " ('is', (27, 29)),\n",
       " ('Ċ', (30, 31)),\n",
       " ('the', (31, 34)),\n",
       " ('question', (35, 43)),\n",
       " ('.', (43, 44))]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 100
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T17:11:21.248314018Z",
     "start_time": "2026-02-10T17:11:21.169495215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "special_tokens = [\"[UNK]\"]\n",
    "trainer = trainers.WordPieceTrainer(\n",
    "    vocab_size=8000,\n",
    "    special_tokens=special_tokens,\n",
    "    show_progress=False\n",
    ")"
   ],
   "id": "324fe4e2bb855663",
   "outputs": [],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T17:11:21.323303006Z",
     "start_time": "2026-02-10T17:11:21.249217377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cls_token_id = tokenizer.token_to_id(\"[UNK]\")\n",
    "print(cls_token_id)"
   ],
   "id": "b397e892b084b381",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None 1\n"
     ]
    }
   ],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T17:11:23.433962199Z",
     "start_time": "2026-02-10T17:11:21.331015940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "files = [\"input.txt\"]\n",
    "tokenizer.train(files, trainer)\n",
    "file_name=\"wp_tokenizer.json\"\n",
    "tokenizer.save(file_name)"
   ],
   "id": "3ae5328152083e2a",
   "outputs": [],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T17:11:23.563430936Z",
     "start_time": "2026-02-10T17:11:23.484247781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load your trained tokenizer\n",
    "tokenizer = Tokenizer.from_file(file_name)\n",
    "\n",
    "text = \"To be, or not to be:\\n\\nThat is \\nthe question.\"\n",
    "\n",
    "# Encode\n",
    "encoding = tokenizer.encode(text)\n",
    "print(encoding.tokens)"
   ],
   "id": "9a85f6c2abbb04ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['to ', '##be, ', '##or no', '##t to ', '##be', '##:Ċ', '##Ċ', '##that is ', '##Ċ', '##the ', '##question', '##.']\n"
     ]
    }
   ],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T17:11:23.626796153Z",
     "start_time": "2026-02-10T17:11:23.564954558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tokenizers import decoders\n",
    "\n",
    "tokenizer.decoder = decoders.Sequence([decoders.Replace(\"Ċ\",\"\\n\"), decoders.WordPiece(prefix=\"##\")])\n",
    "tokenizer.decode(encoding.ids)"
   ],
   "id": "75f2d1c5e7f3c0fb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'to be, or not to be:\\n\\nthat is \\nthe question.'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 105
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
