{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T12:01:55.511231354Z",
     "start_time": "2026-02-10T12:01:55.421325143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"sp.model\")\n",
    "\n",
    "vocab_size=sp.get_piece_size()\n",
    "print(vocab_size)"
   ],
   "id": "b835c4d4e7fac36e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T12:01:56.792873618Z",
     "start_time": "2026-02-10T12:01:55.519653413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load a text file (any book / text)\n",
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "tokens = sp.encode(text, out_type=int)\n",
    "print(len(tokens))\n",
    "data = torch.tensor(tokens, dtype=torch.long)"
   ],
   "id": "2955db7d7594f9fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279427\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T12:01:56.927441402Z",
     "start_time": "2026-02-10T12:01:56.871701847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cpu_only=False\n",
    "device = \"cpu\" if cpu_only or not torch.cuda.is_available() else \"cuda\"\n",
    "print(\"device:\",device)"
   ],
   "id": "8bbd08d0ca555ebb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T12:01:56.985697259Z",
     "start_time": "2026-02-10T12:01:56.928563237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ],
   "id": "e6105679376345b1",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T12:01:57.038074334Z",
     "start_time": "2026-02-10T12:01:56.987955091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "block_size = 128   # context length\n",
    "batch_size = 32\n",
    "\n",
    "def get_batch(split):\n",
    "    data_src = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data_src) - block_size -1, (batch_size,))\n",
    "\n",
    "    # Input tokens\n",
    "    x = torch.stack([data_src[i:i+block_size] for i in ix])\n",
    "    # Target = next character\n",
    "    y = torch.stack([data_src[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "    return x.to(device), y.to(device)"
   ],
   "id": "b4a0493aae7a9a61",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T12:01:57.113899383Z",
     "start_time": "2026-02-10T12:01:57.040336918Z"
    }
   },
   "cell_type": "code",
   "source": "get_batch(\"train\")",
   "id": "4b891bb62d6437ff",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[7979,  269,  518,  ...,   45,  429,  141],\n",
       "         [  35, 3894,   91,  ..., 7951,   13, 2553],\n",
       "         [ 389,   19,  224,  ..., 7959,  387,  277],\n",
       "         ...,\n",
       "         [ 276, 7965, 7942,  ...,  762, 1505,  281],\n",
       "         [ 493, 7959,  111,  ..., 6974,   10,  354],\n",
       "         [  79,   41,  143,  ...,  255, 7979,   13]]),\n",
       " tensor([[ 269,  518,  765,  ...,  429,  141, 2337],\n",
       "         [3894,   91, 7951,  ...,   13, 2553, 4979],\n",
       "         [  19,  224, 7959,  ...,  387,  277, 7965],\n",
       "         ...,\n",
       "         [7965, 7942,    5,  ..., 1505,  281,    5],\n",
       "         [7959,  111,  790,  ...,   10,  354, 5227],\n",
       "         [  41,  143,  318,  ..., 7979,   13, 6341]]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T12:01:57.171230628Z",
     "start_time": "2026-02-10T12:01:57.121084911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CharEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, block_size):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(block_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        tok = self.token_emb(x)              # (B, T, d_model)\n",
    "        pos = self.pos_emb(torch.arange(T, device=device))  # (T, d_model)\n",
    "        return tok + pos\n"
   ],
   "id": "64c34a5e104c4cb4",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T12:01:57.223925968Z",
     "start_time": "2026-02-10T12:01:57.173071449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, block_size):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "\n",
    "        self.key   = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.query = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.value = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.tril(torch.ones(block_size, block_size)).bool()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Project once\n",
    "        K = self.key(x)    # (B, T, C)\n",
    "        Q = self.query(x)\n",
    "        V = self.value(x)\n",
    "\n",
    "        # Split into heads\n",
    "        K = K.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        Q = Q.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        V = V.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        # Shapes: (B, n_heads, T, d_head)\n",
    "\n",
    "        # Attention scores\n",
    "        att = (Q @ K.transpose(-2, -1)) / (self.d_head ** 0.5)\n",
    "        # (B, n_heads, T, T)\n",
    "\n",
    "        att = att.masked_fill(~self.mask[:T, :T], float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "\n",
    "        # Weighted sum\n",
    "        out = att @ V  # (B, n_heads, T, d_head)\n",
    "\n",
    "        # Recombine heads\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        return self.proj(out)"
   ],
   "id": "5de563287749cb8c",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T12:01:57.293635242Z",
     "start_time": "2026-02-10T12:01:57.225891682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, block_size, head_n):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiHeadSelfAttention(d_model, head_n, block_size)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * d_model, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Attention with residual\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        # Feed-forward with residual\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x"
   ],
   "id": "870b845de359f563",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T12:01:57.331402823Z",
     "start_time": "2026-02-10T12:01:57.315195762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SubwordLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, block_size,head_n):\n",
    "        super().__init__()\n",
    "        self.embed = CharEmbedding(vocab_size, d_model, block_size)\n",
    "        self.block = TransformerBlock(d_model,block_size, head_n)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        x = self.embed(x)        # (B, T, d_model)\n",
    "        x = self.block(x)\n",
    "        x = self.ln(x)\n",
    "        logits = self.head(x)   # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            return logits\n",
    "\n",
    "        B, T, V = logits.shape\n",
    "        probs=logits.view(B*T, V) # probabilities: B * T, V\n",
    "        ids = targets.view(B*T) # ids: B * T\n",
    "        loss = F.cross_entropy(\n",
    "            probs,\n",
    "            ids\n",
    "        )\n",
    "        return logits, loss"
   ],
   "id": "9fb7b6d0b5b1845d",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T12:01:57.413458299Z",
     "start_time": "2026-02-10T12:01:57.370998804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = SubwordLM(vocab_size, d_model=128, block_size=block_size, head_n=4).to(device)\n",
    "\n",
    "def train():\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "    for step in range(5000):\n",
    "        xb, yb = get_batch(\"train\")\n",
    "\n",
    "        logits, loss = model(xb, yb)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 500 == 0:\n",
    "            print(f\"step {step}, loss {loss.item():.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), 'subword.pth')\n",
    "\n",
    "model_path=\"subword.pth\"\n",
    "if os.path.exists(model_path):\n",
    "    model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "else:\n",
    "    train()"
   ],
   "id": "5ebbd77497d160e6",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T12:01:57.469609879Z",
     "start_time": "2026-02-10T12:01:57.415170968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, start, max_new_tokens=200):\n",
    "    model.eval()\n",
    "    idx = torch.tensor([sp.encode(start)], device=device, dtype=torch.long)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -block_size:]\n",
    "        logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "\n",
    "    return sp.decode(idx[0].tolist())\n"
   ],
   "id": "728e73fcccf50c20",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T12:02:06.390215359Z",
     "start_time": "2026-02-10T12:02:05.298644953Z"
    }
   },
   "cell_type": "code",
   "source": "print(generate(model, \"CORIOLANUS:\"))",
   "id": "3cb0896b6527575a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORIOLANUS: hands blush daughter Montague betteriod remembimble coll friar good proper extremity hold years France furnishims zeal M Nor by durst battle wrong Edward seated Calais gladlyubs dugIV forbid una gro jotgs Ha true company saw hideBRO Thank Bound wrong AEd Whatia Ban western gone disin glass crown Had ag die chee Being Pardon extrem doubt lendsct bodes tilllong whistle Wiltshire dischargedud chap greaterGE sirs noseTwas brainsumbling Min gent lod shinesout poison rose counteruselve Mont as nail religellow woes Pit timesGHBYhich Whereof object beha quoth toooon chee woningers drag BIONDELLOocate him trothrs way Right ves sle achieve lives stumble sleicer Many conditions drunkard sway Vienna ca gatecience pleasuretheonst many device song NORTHUMBERLAND landia amen gown waken doesotted dearer helps wakenelve Un yieldratchimo Lartius roundly CyER Welcome exped Hie Unb,anuth MONTAGUE admiriny fig PARIS chaste selfsamehear tree dwell under lower Jupnithen Cob pri served MercyENES Many Napharinainces eight rights degrees hood graft letterfranch hands something departuregrown\n"
     ]
    }
   ],
   "execution_count": 74
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
